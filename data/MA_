2006 - MA001 
RIEMANN'S ZETA FUNCTION: THE ZEROS OFF THE CRITICAL LINE
Dmitri V Skjorshammer
South Fork High School, Stuart, Florida, USA

This investigation is based on my important result about the non-trivial zeta zeros off the critical line. My initial purpose was to generate an expression that would create limits for the Riemann Zeta Function but as my research grew, I realized that a function of asymptotic behavior could play a bigger role than just controlling the Riemann Zeta Function. It then became my purpose to relate the zeta zeros off the critical line to the asymptotic function.<br><br> I used 'Mathematica 5.2 For Students' by Wolfram to analyze complex calculations and graphs.<br><br> One of the first main procedures that I did was to modify the Riemann Zeta Function to control it. I used that result to calculate an order term of the Modified Riemann Zeta Function; the asymptotic function. This was an important step because I used that relation to get the area between the Modified Riemann Zeta Function and the order term.<br><br> From that, I concluded that the probability of finding non-trivial zeta zeros off the critical line is infinitesimal.<br><br> The conclusion of this investigation is that there is an infinitesimal number of non-trivial zeta zeros off the critical line. 
________________________________________
2009 - MA001 
ANALYZING PROPOSITION FIVE
Joyce Lin
Lawton Chiles High School, Tallahassee, FL

This project in its present form is to see whether Archimedes' Proposition Five can be proven to be true. The initial idea was to attempt to prove that Proposition Five is a true statement, and if not, why it can not be true. This is important as in the Book of Lemmas, Archimedes presented Proposition Five as a true statement, but we can not take it to be true unless it can be proven. <br><br>Archimedes' Proposition One was taken and proven to be true using geometric postulates, axioms, and properties of triangles; the proof was recorded. This was repeated once more. An arbelos was then drawn with the Archimedean Twin Circles. Using Proposition One, properties of triangles, properties of circles, properties of parallel and perpendicular lines, and various theorems, postulates, and axioms, Proposition Five was proven to be true. The proof was recorded, and this was repeated once more. The final proof was looked over in the case of any mistakes.<br><br>While and after performing the above experiment, it was noted that the properties of similar triangles were used numerous times. Proving that certain points were on the same line was also instrumental in proving Proposition Five.<br><br>In the end, it was concluded that Proposition Five is indeed correct. This shows that Archimedes was correct in his claim. The original purpose was to attempt to prove Proposition Five, and that is what was done. 
________________________________________
2004 - MA001 
AN INVESTIGATION OF IRREDUCIBLE POLYNOMIALS OVER ZP USING ABSTRACT ALGEBRA
Brianna Rachel Satinoff
Palm Harbor University High School, Palm Harbor, Fl. U.S.A.

Traditional notions of factorization are altered in finite fields such as Zp, equivalent to the integers with addition and multiplication defined mod p for any prime p. This project investigates how the properties of finite fields can be used to determine the number of, and find a classification for, the irreducible polynomials of degree n with coefficients in Zp.<br><br> Kronecker's Theorem states that, for any irreducible polynomial over a field F, there exists a field extension of F in which the polynomial factors completely. A result of this theorem implies that all finite fields are extensions of Zp for some p, and relates their members to the roots of certain polynomials.<br><br> This result is examined to calculate the number of irreducible polynomials of degree n in Zp. A numerical approach is first presented using only basic algebra for the first few values of n, but is then shown to grow in complexity too quickly. A formula is then developed based on Kronecker's theorem and the various field extensions of Zp, first for specific degrees and then for a general formula using combinatorics.<br><br> As a related question, this project examines a grouping of the irreducible polynomials over Zp based on whether p(x) = p(x+1). Several properties of polynomials for which this is true are suggested and proven; the classification is then related to the roots of the polynomials over field extensions of Zp. The investigation ends with a suggested way to count the number of irreducible polynomials for which p(x) = p(x+1). 

Awards won at the 2004 ISEF
Third Award of $250 - American Mathematical Society
________________________________________
2007 - MA001 
QUANTITATIVE EVALUATION OF CANCER CELL COMPLEXITY: A STUDY OF FRACTAL MORPHOMETRY
Samuel Irvin Kornicks
Vero Beach High School, Vero Beach, FL, USA

Cancer detection today is a subjective study of cell shapes and abnormalities. Can cancer be diagnosed more efficiently using a mathematical model?<br><br> If cancer cells are dense, complex structures, then quantifying the morphometry of these cells will show significant differences between the N929 cancer cell line and the normal, MEF cell line. <br><br> Two emerging fields of research, fractals and digital image analysis are used in this study to quantify normal and cancer cells.<br><br> Quantitative evaluation of the surface fractal dimension is one way to measure cellular growth patterns. Determining the lacunarity, or texture of the cell structure, analyzing the density, patterns of binary images, the cells contour, the size and geometry of the cells at different resolutions, and the surface area were all quantified. <br><br> The tumor cell line, N929 and the normal cell line, MEF were grown in culture for 144 hours. The sample size included 6 petri dishes of 4 different dilutions each for a total of 48. Every 24 hours all samples were examined under an inverted microscope. Over 200 digital images were taken of both the cancer cells and the normal cells. These images were analyzed using two different digital image software programs and mathematical calculations completed. The mean of each measurement was determined, and a t-test completed to determine statistical significance. <br><br> Cancer is called the crab due to its irregular growth and chaotic behavior. Finding patterns in cancer growth may enable scientists to diagnose cancer before it becomes a tumor. <br><br> 

Awards won at the 2007 ISEF
Award of $1,000 - Mu Alpha Theta, National High School and Two-Year College Mathematics Honor Society
________________________________________
2004 - MA001 
AN INVESTIGATION OF IRREDUCIBLE POLYNOMIALS OVER ZP USING ABSTRACT ALGEBRA
Brianna Rachel Satinoff
Palm Harbor University High School, Palm Harbor, Fl. U.S.A.

Traditional notions of factorization are altered in finite fields such as Zp, equivalent to the integers with addition and multiplication defined mod p for any prime p. This project investigates how the properties of finite fields can be used to determine the number of, and find a classification for, the irreducible polynomials of degree n with coefficients in Zp.<br><br> Kronecker's Theorem states that, for any irreducible polynomial over a field F, there exists a field extension of F in which the polynomial factors completely. A result of this theorem implies that all finite fields are extensions of Zp for some p, and relates their members to the roots of certain polynomials.<br><br> This result is examined to calculate the number of irreducible polynomials of degree n in Zp. A numerical approach is first presented using only basic algebra for the first few values of n, but is then shown to grow in complexity too quickly. A formula is then developed based on Kronecker's theorem and the various field extensions of Zp, first for specific degrees and then for a general formula using combinatorics.<br><br> As a related question, this project examines a grouping of the irreducible polynomials over Zp based on whether p(x) = p(x+1). Several properties of polynomials for which this is true are suggested and proven; the classification is then related to the roots of the polynomials over field extensions of Zp. The investigation ends with a suggested way to count the number of irreducible polynomials for which p(x) = p(x+1). 

Awards won at the 2004 ISEF
All expense-paid trip to attend the U.S. Space Camp in Huntsville, Alabama and a certificate - National Aeronautics and Space Administration
________________________________________
2008 - MA002 
FINDING ROOTLESS MATRICES USING JORDAN NORMAL FORM
Anne Zijing Ye
Little Rock Central High School, Little Rock, AR

In this paper, we used the Jordan normal forms of nilpotent matrices to find those nilpotent matrices that are rootless. A matrix W is defined as rootless if there exists no other matrix S and no integer r greater than 1 such that W can be expressed as S raised to the r power. In Proposition 1, we proved that an n x n matrix whose Jordan normal form has two Jordan blocks, neither of which has dimensions of a x a, where a is the smallest integer greater than or equal to n/2, is rootless. In Corollary 1 to Theorem 2, we generalized this find to n x n matrices whose Jordan normal forms consist of three or more Jordan blocks, proving our results specifically for the case where one of the blocks has dimensions greater than 3 x 3 and the remaining blocks are 0’s. Although in this paper we described some rootless nilpotent matrices, there might be other nilpotent matrices that are rootless, and there are certainly rootless matrices that are not nilpotent. In the future, we intend to use our results to further explore rootless matrices. 
________________________________________
2009 - MA002 
THE GOLDEN RATIO: AN EXPLORATION OF THE PREVALENCE OF PHI
Bethany Dilda
Branson High School, Branson, CO

This project was designed to determine conclusively and mathematically <br><br>whether or not the presence of the golden ratio is significant in the <br><br>phyllotaxis of South Eastern Colorado. <br><br>I measured five arbitrary proportions on each plant within an arbitrary <br><br>sample of prevalent bushes, flowers, trees, and weeds of South Eastern <br><br>Colorado and analyzed it for the presence of the golden ratio. Using <br><br>these data, I calculated the percentage of the time that the golden <br><br>ratio was found for each species and for the entire sample. I then <br><br>calculated the confidence interval.<br><br>From my data and calculations I can conclude with 95% confidence that <br><br>between 70.5% and 80.5% of the plant species that I measured will <br><br>contain the golden ratio. <br><br>This work conclusively proves the myth of the presence of a “Golden <br><br>Ratio” in the plants of South Eastern Colorado. 
________________________________________
2004 - MA002 
EDUCATION VS. MEDICATION: A STATISTICAL ANALYSIS OF THE EFFICACY OF PATIENT SELF DISEASE MANAGEMENT PRACTICES VERSUS THE USE OF MEDICATION IN LOWERING HBA1C.
Justin Andrew Johns
Evangelical Christian School, Fort Myers, FL, United States of America

The researcher’s hypothesis was that HbA1C levels of Type II diabetic patients receiving only self-disease management education on ways to improve their diet and exercise practices would decrease more dramatically at a statistically significant level compared to those patients receiving only medicine.<br><br> The researcher began by asking the staff of Family Health Centers of Southwest Florida, Inc. to extract specific pieces of data (e.g.-hemoglobin (HbA1C) readings, ethnicity, gender and age) from the charts of 50 randomly selected diabetic patients who received educational training on diet and exercise alone, and 50 randomly selected diabetic patients who were placed on a prescription medicine regimen alone. <br><br> The average HbA1C readings for the patients in the education study group dropped by 8.4% compared to an average overall increase for patients in the medication study group of 2.0%. Based upon the two study groups’ HbA1C data sets, computed means, variances, standard deviations, histogram plots, and the inferential statistical functions performed (t-test/z-test for two samples and for paired samples), it can be concluded with near certainty that education was a significant factor in lowering patients' HbA1C levels. However, nothing can be said with any certainty about the medication group. Also, it can be concluded that the patients chosen are representative samples of Family Health Centers' other 45,000 patients. Furthermore, the scatter plots of the data sets show that a direct and positive linear correlation exists between the education and medication study groups. These statistical findings support the researcher's original hypothesis. 
________________________________________
2010 - MA002 
CONTRASTING THE WALTZ OF PI, MUSIC OF PI AND MUSIC OF THE GOLDEN NUMBER PHI
Danae Jean Pick
Olivet Private School, Port Saint Lucie, FL

The goal of this project was to contrast 6 original compositions based on the irrational numbers pi and Phi and to determine which composition (or number) was more audibly pleasing to the listener. The project further investigated the complex relationship between mathematics and music. The hypothesis tested was that the Music of the Golden Number Phi would be more audibly pleasing than either the Waltz of Pi or the Music of Pi, because the number Phi represents the golden ratio which the eye perceives as beautiful. The researcher believed this beauty would transfer to the musical score. <br><br> <br><br>The numbers pi and Phi were converted to 3 musical scores, Waltz of Pi, Music of Pi, and Music of the Golden Number Phi, using an algorithm that assigned digits to note names and rhythm values. After analyzing the scores, dissonance was discovered in 5 out of 9 of the notes/numbers from the algorithm. The percentage of dissonance calculated in Waltz of Pi, Music of Pi, and Music of the Golden Number Phi was 62.1%, 51.1 %, and 45.4%, respectively. <br><br> <br><br>To eliminate dissonance, algorithm 1 was modified, making the bass clef an octave and a 3rd lower than the treble (imperfect consonance). Algorithm 2 was used to create the Music of Pi Algorithm 2, Waltz of Pi Algorithm 2 and Music of the Golden Number Phi Algorithm 2. <br><br> <br><br>A comparison sheet was created contrasting all 7 compositions (Waltz of Pi 2009 included). The Music of the Golden Number Phi Algorithm 2 was most audibly pleasing. 
________________________________________
2003 - MA002 
"MATHEMATICS IS NOT YET READY FOR SUCH PROBLEMS": COLLATZ CONJECTURE RATIONALIZED
Robert Shea Bracco
Dupont Manual, Louisville, KY, USA

Collatz Function is a classic recursive function that is famous in the field of number theory. Despite many attempts, it has remained unsolved for over 75 years. It's appealing to mathematicians because of its simple appearance, yet obvious intractability to solve. Like Fermat's Last Theorem, it is also widely known for its many ties to other branches of math. <br><br> I found several new results concerning the problem, which I expect to lead to a formal proof of the conjecture.<br><br> These new results include formulas that predict the types of iterations that occur as the output of different linear sets. Using these formulas I showed that Collatz function has a rational predictable structure. I also tested output values against starting values and used the resulting patterns to set up an incomplete proof that has great potential of turning in to a full proof.<br><br> I also found a rational probability function for modulo properties of all natural numbers that are iterated through the function. 

Awards won at the 2003 ISEF
Honorable Mention Awards - American Mathematical Society
________________________________________
2006 - MA003 
INTEGRATION BY QUOTIENTS: AN ALTERNATIVE SOLUTION TO THE GAMMA FUNCTION
William Richard Doenlen
Pensacola High School, Pensacola, FL, United States of America

The experiment tested the application of the formula (equation 1) in order to evaluate the Gamma function, defined as (equation 2) in the set of complex numbers where the real part is greater than zero. In the experiment, however, only real numbers were used, and in the case of negative real numbers (with the exception of the negative integers and zero), the identity (equation 3) was used. The formula was first applied to the indefinite integral (equation 4), yielding (equation 5). Then, the limits of integration of the Gamma function were applied, allowing the Gamma function to be rewritten as (equation 6). As a result of this definition, several other statements could be proved, namely that (equation 7), (equation 8), (equation 9), and that (equation 10).<br><br><br> EQUATIONS HANDWRITTEN IN 
________________________________________
2004 - MA003 
DECRYPTING THE MYSTERY OF VOWEL SOUNDS
Krystelle Andree Denis
Celebration High School, Celebration, Florida, USA

The purpose of this project is to determine if the English vowel sounds show a mathematical symmetry. This experiment was performed to find out if the English vowel sounds are symmetrically distinct from one another, allowing the human ear to distinguish different vowels in an optimal way, making it easier for humans to understand.<br><br>In the making of this project, three samples of English vowel sounds were recorded and analyzed. An FFT software was used to generate a wave spectrum for each vowel. By analyzing each spectrum, the main simple waves were extracted. The sum of the three main simple waves is a fairly accurate version of the original wave. The amplitude and the frequency of each simple wave were noted down, and an Excel spreadsheet software was used to record them. The following mathematical formula was defined for each vowel: [Amplitude-square] x [Frequency]. This formula was applied for each simple vowel. The characteristic results were plotted on a graph to search for symmetry.<br><br>The results showed that each vowel represented on a bar graph, with a logarithmic scale, had a geometrical symmetry relative to a median line. Each vowel bar either crossed this median line at its center point, or was geometrically symmetrical to another vowel bar.<br><br>The results supported the hypothesis. English vowel sounds have a mathematical symmetry that can be seen in a graph. The results may indicate that the English vowels have evolved over time, to maximize the symmetry of sounds. <br><br><br><br><br> 
________________________________________
2005 - MA003 
PROGRAMMABLE QUANTUM COMPUTING: A NEW FRAMEWORK WITH VON NEUMANN TYPE ARCHITECTURE
Nimish P. Ramanlal
Seminole High School, Sanford, Florida, United States

This study develops a framework for programmable quantum computing that is unprecedented in the scientific literature. It is shown how instruction may be stored in memory in a quantum computer and drawn upon for a range of quantum algorithms. In contrast, the current architecture for quantum computing prevalent in the literature requires a quantum computer to be designed and constructed to execute a specific set of instructions. These instructions are hard-wired into the computer and the computer cannot be reprogrammed to execute a different set of instructions. This represented the state of classical computing prior to 1945 when von Neumann first proposed the architecture for programmable computing that is now an integral component of all classical computers.<br><br> Quantum computers use the properties of superposition and entanglement to achieve unprecedented levels of parallel processing. Deutsch, Shor, Grover, and Bernstein and Vazirani have demonstrated how these properties may be used to execute instructions on quantum computers exponentially faster than any classical computer. <br><br> This study retains the exponential improvement but adds programmability. In particular, it is recognized that all mathematical instructions can be reduced to logical operations, and all logical operations can be represented by the set F of Boolean functions f: {0,1}^n -> {0,1}. There are 2^(2^n) elements in this set. A systematic and scalable method is developed to program the entire set F into a quantum computer. Further, it is shown how a quantum computer may be programmed to simultaneously and tunably execute any subset of F over the entire domain in a single operation. It is also shown how this may be used to significantly expand the set of search algorithms developed to date and to solve important classes of NP problems. Applications to iterative maps, biological self-replication, and virtually unbreakable cryptographic codes are discussed.<br><br> 

Awards won at the 2005 ISEF
Second Award of $1,500 - Mathematics - Presented by Science News
________________________________________
2008 - MA003 
NUMBER GRID POTENTIAL
Clara Livingston Bailin
Little Rock Central High School, Little Rock, AR

The goals of this study were to apply algebraic group theory to number grids and to identify connections between defined variables. An additional goal was to isolate the variables affecting order characteristics from lurking variables. These objectives were designed ultimately to contribute to the goal of discerning the extent to which two methodologies for performing a series of transformations differ. The variables affecting the transformations are "cell order" and "symbol order". There variables are so broad that they must be divisions of more focused variables such as "position", "row", "column", and "sub-grid". Relationships between the variables are complex; however, enough is known to conclude that the two coordinate systems are not independent of each other. Group theory has substantial applications to the fields of symmetrical analysis, particle physics, and chemistry. 

Awards won at the 2008 ISEF
Fourth Award of $500 - Mathematical Sciences - Presented by
________________________________________
2005 - MA003 
PROGRAMMABLE QUANTUM COMPUTING: A NEW FRAMEWORK WITH VON NEUMANN TYPE ARCHITECTURE
Nimish P. Ramanlal
Seminole High School, Sanford, Florida, United States

This study develops a framework for programmable quantum computing that is unprecedented in the scientific literature. It is shown how instruction may be stored in memory in a quantum computer and drawn upon for a range of quantum algorithms. In contrast, the current architecture for quantum computing prevalent in the literature requires a quantum computer to be designed and constructed to execute a specific set of instructions. These instructions are hard-wired into the computer and the computer cannot be reprogrammed to execute a different set of instructions. This represented the state of classical computing prior to 1945 when von Neumann first proposed the architecture for programmable computing that is now an integral component of all classical computers.<br><br> Quantum computers use the properties of superposition and entanglement to achieve unprecedented levels of parallel processing. Deutsch, Shor, Grover, and Bernstein and Vazirani have demonstrated how these properties may be used to execute instructions on quantum computers exponentially faster than any classical computer. <br><br> This study retains the exponential improvement but adds programmability. In particular, it is recognized that all mathematical instructions can be reduced to logical operations, and all logical operations can be represented by the set F of Boolean functions f: {0,1}^n -> {0,1}. There are 2^(2^n) elements in this set. A systematic and scalable method is developed to program the entire set F into a quantum computer. Further, it is shown how a quantum computer may be programmed to simultaneously and tunably execute any subset of F over the entire domain in a single operation. It is also shown how this may be used to significantly expand the set of search algorithms developed to date and to solve important classes of NP problems. Applications to iterative maps, biological self-replication, and virtually unbreakable cryptographic codes are discussed.<br><br> 

Awards won at the 2005 ISEF
Honorable Mention Award Certificates for International students and students under the age of 16 - National Aeronautics and Space Administration
First Award of $3,000 - U.S. Air Force
Tuition Scholarship Award in the amount of $8,000 - Office of Naval Research on behalf of the U.S. Navy and Marine Corps.
________________________________________
2009 - MA004 
AN INVESTIGATION INTO THE MULTI-PEG LINEAR TWIN TOWERS OF HANOI
David Zihua Ye
Little Rock Central High School, Little Rock, AR

In this investigation, a variant of the Linear Twin Towers of Hanoi was proposed, and an algorithm to solve it was introduced. An additional peg was added to the Linear Twin Towers of Hanoi problem to produce the variant,"The Multi-Peg Linear Twin Towers of Hanoi". However the same setup and stipulations applied:<br><br>1. Two towers, one black and one white, were set up on the two end pegs.<br><br>2. The goal was to switch the positions of the two towers.<br><br>3. Only one disk could be moved at a time, and larger disks could not go on top of smaller disks.<br><br>4. Disks could only move to adjacent pegs. <br><br>A recursive algorithm, consisting of four main subalgorithms, was then introduced to solve the variant for any k disks, and the number of moves needed was analyzed and approximated for each of the aforementioned subalgorithms. The result is useful, as the Tower of Hanoi problem is a standard exercise for recursion in computer science, and the Multi-Peg Linear Twin Towers of Hanoi problem could be used as a significantly more difficult challenge. 
________________________________________
2005 - MA005 
PHASE II: HOW HIGH WILL IT GO?
Mitchell Andrew Kaundart
Northside High School, Fort Smith Arkansas, United States of America

Anyone looking at a model rocket usually asks: "How high will it go?" In the past, most rocketeers could not answer this, because many complicated equations needed solving to predict rocket apogee, or peak altitude. In the 1960s, these equations were programmed into computers, resulting in the writing of two technical reports, Centuri TIR-100 and Estes TR-10. Both reports contain simple equations and graphs for apogee prediction.<br><br> This research tested which report most accurately predicts apogee. In Phase I, a mid-sized rocket, 182 mm long, was launched and tracked using two theodolites. The apogee was determined ultimately using trigonometry. TR-10 was more accurate, so this became Phase II's hypothesis. Since this research was not to determine which report has the closest apogee prediction for one particular rocket, Phase II's mid-sized rocket was 440 mm long. It had five launches, carrying an electronic altimeter payload. TIR-100 yielded an apogee prediction of 131.064 m; TR-10's prediction was 140.208 m. The resulting average apogee was 100.828 m. TIR-100 produced a closer prediction for this rocket, not supporting the hypothesis. <br><br> Most likely, both predictions were over 30 m too high because 0.75, a variable in both reports generally accepted as any mid-sized rocket's pressure drag coefficient, was used to calculate the predictions. Because determining a rocket's actual coefficient requires complex equations, computers, and/or wind tunnel testing, this approximation was used. The rocket's true coefficient is probably larger, making its apogee lower than predicted.<br><br> 
________________________________________
2007 - MA005 
PHASE IV: HOW HIGH WILL IT GO?
Mitchell Andrew Kaundart
Northside High School, Fort Smith, AR, United States

Model rocketeers often ask: “How high will it go?” This question was difficult to answer until the 1960s when calculus and Newton’s second law were used to create an equations set, called “the Fehskins-Malewicki Solution,” for model rocket apogee prediction. Then, two apogee prediction reports were published: Centuri TIR-100 and Estes TR-10. Another equations set, “the Caporaso-Bengen Solution,” was later developed. Eventually, computer prediction programs, including Winroc 4.5, were produced. This research compared these methods to determine which best answered the question: “How high will it go?” <br><br> This phase used the same rocket data from Phase III but focused on two newly studied apogee prediction methods: the Caporaso-Bengen Solution and Winroc 4.5. The Caporaso-Bengen Solution was hypothesized to be the most accurate prediction method of those studied in Phases III and IV. Yielding the prediction with the Caporaso-Bengen Solution involved two steps. The calculations were solved by hand, assuming a 0.75 drag coefficient, to insure the correct unit cancellations. Then, a spreadsheet was created to prevent rounding errors. The Caporaso-Bengen Solution yielded a prediction of 368.462 feet. Then, the rocket’s apogee was predicted using Winroc 4.5, again assuming a drag coefficient of 0.75. This process was simpler and only required entering the rocket data into a computer. Winroc 4.5 yielded a prediction of 390.2 feet.<br><br> When compared to the Phase III data, the new data supported the hypothesis. The Caporaso-Bengen Solution more accurately predicted the rocket’s apogee than Winroc 4.5, Estes TR-10, Centuri TIR-100, and the Fehskins-Malewicki Solution.<br><br> 
________________________________________
2005 - MA005 
PHASE II: HOW HIGH WILL IT GO?
Mitchell Andrew Kaundart
Northside High School, Fort Smith Arkansas, United States of America

Anyone looking at a model rocket usually asks: "How high will it go?" In the past, most rocketeers could not answer this, because many complicated equations needed solving to predict rocket apogee, or peak altitude. In the 1960s, these equations were programmed into computers, resulting in the writing of two technical reports, Centuri TIR-100 and Estes TR-10. Both reports contain simple equations and graphs for apogee prediction.<br><br> This research tested which report most accurately predicts apogee. In Phase I, a mid-sized rocket, 182 mm long, was launched and tracked using two theodolites. The apogee was determined ultimately using trigonometry. TR-10 was more accurate, so this became Phase II's hypothesis. Since this research was not to determine which report has the closest apogee prediction for one particular rocket, Phase II's mid-sized rocket was 440 mm long. It had five launches, carrying an electronic altimeter payload. TIR-100 yielded an apogee prediction of 131.064 m; TR-10's prediction was 140.208 m. The resulting average apogee was 100.828 m. TIR-100 produced a closer prediction for this rocket, not supporting the hypothesis. <br><br> Most likely, both predictions were over 30 m too high because 0.75, a variable in both reports generally accepted as any mid-sized rocket's pressure drag coefficient, was used to calculate the predictions. Because determining a rocket's actual coefficient requires complex equations, computers, and/or wind tunnel testing, this approximation was used. The rocket's true coefficient is probably larger, making its apogee lower than predicted.<br><br> 
________________________________________
2009 - MA005 
UNIVERSAL LAW FOR THE DISTRIBUTION OF ODD PERIODIC CYCLES WITHIN CHAOS IN NONLINEAR DYNAMICAL SYSTEMS: AN ANALYSIS OF RIGID BIFURCATION (YEAR II)
Almas Abdulla
West Shore Junior/Senior High School, Melbourne, FL

This project continues the analysis of the mathematical nature of the distribution of the odd periodic windows within chaos in nonlinear logistic-type dynamical systems. Past years’ projects have revealed the following universal transition mechanism from chaos to odd periodicity according to the non-monotonic +4 -2 law: <br><br>Chaos =>… =>19 =>15 =>17 =>13 =>15 =>11 =>13 =>9 =>11 =>7 =>9 =>5 =>7 =>3 =>Chaos <br><br>The principal result of this project is the discovery of a fine classification of odd periodic cycles based on their cyclic permutations and oriented graphs of transition, rather than the traditional coarse classification based on the cycles’ period. It is observed that each of the two 2k+1-cycles has its own unique cyclic permutation which is universal in all models. By employing the method of symbolic dynamics, we rigorously prove that the observed cyclic permutations of all the second 2k+1-cycles is indeed universal for all maps. This universal cyclic permutation is the only one which prevents the existence of any other odd periodic cycles of lower periods. This fully explains the absence of lower period odd cycles after the second 2k+1 cycle. Having similar observations for the first 2k+1-cycle, we conjecture that they too have a unique cyclic permutation whose oriented graph of transitions is a combination of two subgraphs which are characteristic for the second 2k-1-cycles. This universal transition graph of the first 2k+1 is the only one which enforces the presence of the 2k-1 cycle while preventing the existence of all other lower period cycles. This fully explains the non-monotonic nature of distribution of odd periodic windows. We address the proof of this conjecture in next year’s project. 

Awards won at the 2009 ISEF
Third Award of $250 - American Mathematical Society
Second Award of $1,500 - Mathematical Sciences - Presented by Intel
Award of three $1,000 U.S. Savings Bonds, a certificate of achievement and a gold medallion. - United States Army
________________________________________
2010 - MA006 
THE MATH OF GETTING LUCKY
Casey Ann Gibson
Nettleton Public Schools, Jonesboro, AR

Many people gamble their money away, not realizing how much they are going to lose. Therefore, the purpose of this project was to illustrate how much money, on average, is lost playing a few casino games. The hypothesis was: when the expected return (expected value) of each dollar spent on three games of chance is calculated, the expected values will be negative, but some games will be fairer (expected value of zero; no money lost or gained) than others. <br><br>First, library research was done on the games (keno, craps, and roulette) to determine all the different versions and ways to play each game. For every different version/way to play each game, an expected value table was created. These tables had columns for x (net profit), P(x) (probability of x), and xP(x) (previous columns multiplied together). At the bottom of each table was the formula used to find expected value: mu equals sigma times x times P(x). To find the expected value for each table, the numbers in the final column (xP(x)) were added together. This final sum was the amount of money lost on average per one dollar spent.<br><br>The hypothesis was supported because all of the expected values were negative, additionally some games were determined to be fairer than others. The fairest game was craps (-1.4 cents per dollar), then roulette (-2.7 to -5.3 cents per dollar), and lastly keno (-25 to -43.7 cents per dollar). In conclusion, none of them should be played with the intent of making a profit. 
________________________________________
2008 - MA006 
UNIVERSAL LAW FOR THE DISTRIBUTION OF ODD PERIODIC CYCLES WITHIN CHAOS IN NONLINEAR DYNAMICAL SYSTEMS: A RIGOROUS APPROACH THROUGH SYMBOLIC DYNAMICS
Almas U. Abdulla
West Shore Junior/Senior High School, Melbourne, FL

This project is a first attempt to shed light into the mathematical theory of a newly revealed universal law for the distribution of odd periodic cycles within Chaos in nonlinear logistic kind discrete equations. Besides their importance in many applications, these nonlinear discrete equations arise as a Poincaré map for the nonlinear differential equations such as the Lorenz, Düffing or Rössler systems. In his celebrated paper, Sharkovsky (1964) discovered the law of the coexistence of periodic cycles according to a decreasing monotonic order. This project reveals a universal non-monotonic -4+2 law of the distributions of odd periodic cycles within chaos:<br><br>Chaos =>…=>19=>15=>17=>13=>15=>11=>13=>9=>11=>7=>9=>5=>7=>3=> Chaos<br><br>Two new universal constants 1.33876779446… and 2.11625732325…are found, which qualitatively characterize the rate of change of the distance between the odd cycles. To understand the nature of the non-monotonic distribution of odd cycles, the method of symbolic dynamics is employed. This method replaces each odd periodic cycle with a cyclic permutation and an oriented graph of transitions. It is proved that if the 7-cycle’s transition graph contains a closed path with five elements, then the existence of a 5- cycle is guaranteed. Then it is demonstrated that the transition graph of the first 7-cycle satisfies this requirement, while the one of the second 7-cycle does not. This fully explains the non-monotonicity of the universal law. It is conjectured that all the stable odd cycles have a universal cyclic permutation. This conjecture is confirmed by in four logistic type models. 
________________________________________
2007 - MA006 
COMPUTATION OF HOMOLOGY GROUPS OF TOPOLOGICAL SPACES CREATED FROM A FOUR-DIMENSIONAL SPHERE
Naran Bayanbat
ASMSA, Hot Springs, AR, USA

Topology is a field of mathematics that studies the structure of open sets in space. In algebraic topology, concepts from abstract algebra are extended to analyze topological spaces. One of the fundamental goals of topology is to classify topological spaces. This is done in algebraic topology via the use of algebraic invariants, which include the homology groups. In this project, the topological spaces created from a four-dimensional sphere with the removal of certain number of tori were classified via their homology groups. The hypothesis was that there exists a general formula for obtaining the homology groups of any space created in this manner. The homology groups of a four-dimensional sphere, a solid torus, and a hollow torus were computed using cell complexes. Then, the Mayer-Vietoris Sequence was applied to compute the homology groups of the topological spaces under investigation. The method of chasing the boundary maps was used to calculate the unknown homology groups. The result of this research was that the homology groups of the topological spaces under investigation were, <br><br> H*={Z, Z^k, Z^(k-1), {e},…},<br><br>where the variable k represents the number of tori removed from the sphere. By obtaining the formula for a general case, the hypothesis was also supported. By calculating the homology groups of these spaces, this research contributes to the goal of classifying the topological space. A similar research could potentially be performed using the homotopy groups as an invariant as well. Lastly, a possibility of future research is to compute the homology groups of topological spaces created by re-embedding the tori back into the topological spaces created from a four-dimensional sphere.<br><br> 
________________________________________
2007 - MA007 
GRAVITATIONAL MICROLENSING AND ITS USE IN DETECTING DARK MATTER
Lauren Nicole Fischer
Seabury Hall, Makawao, Hawaii, USA

The trends resulting from two equations designed to determine the angle of light deflection in gravitational microlensing were examined. These equations showed how the angle of light deflection was affected by alterations in the impact parameter and in the mass of the lens object. Varying values for the mass were substituted into the equation:<br><br>alpha = 4Gm/[(c^2)b]<br><br>where alpha = the angle of light deflection, G = Newton’s gravitational constant, m = the mass of the lens, c = the speed of light, and b = the impact parameter. The values for the impact parameter were kept constant. Then, the same procedure was repeated using varying values for the impact parameter while keeping the mass constant. The graph of the resulting data showed that the angle of light deflection varies linearly with an increase in the mass of the lens. The angle of light deflection with respect to the increase in impact parameter varies as a rapid decrease that then levels out. Therefore, an increase in lens mass will cause a linear increase in the angle of light deflection, while an increase in impact parameter will result in a non-linear decrease.<br><br> 
________________________________________
2008 - MA007 
THE SIMPLIFICATION OF THE ROOT TEST FOR CONVERGENCE/DIVERGENCE OF A SERIES
Reguli Elisabeth Granger
Science Center Home School, Saint Petersburg, FL

The purpose of this project was to find a way to simplify the Root Test for the Convergence/Divergence of a Series. By taking the limit of the bnth root of an rather than simply the nth root of an, the process would be simplified, thus reducing the possibility of error as well as cutting down on time needed to solve a problem.<br><br> Using the original Root Test proof and theorem, I was able to derive my own proof by replacing the nth root with the bnth root, and making necessary changes as needed. The proof led to the theorem, which claims that b can be any real number. <br><br> I did problems, encompassing a variety of situations, including series having fractional and negative exponents. I also found that when the coefficient of n of the numerator and denominator are different, it is generally the easiest to use the smaller coefficient as b, but any coefficient can be used, since the proportion between the numerator and denominator will remain the same.<br><br> The contributions of this project relate directly to the root test - it simplifies it considerably. On a larger scale, though, the applications of series relate to fractals and situations where it is extremely difficult, if not impossible, to find a function. 
________________________________________
2007 - MA008 
A CRYPTOGRAPHIC FORAY INTO LATIN SQUARE POTENTIAL
Clara Livingston Bailin
Little Rock Central High School, Little Rock, AR

The goals of this investigation were to identify isotopic number grids using a form of symbol position-plotting which is based on the Latin Square Rule and to determine each formula's limits of application. Over the course of seven months and numerous trials, it was concluded that a grid will, if manipulated through one of these formulas, transform into predictable isotopic puzzles. These transformations exhibit a type of mathematical chaos which is defined as a sensitivity to initial conditions. This means that because there is at least one category of transformation that will apply to each grid, it can be found that one transformation will follow another. 
________________________________________
2006 - MA008 
WHAT DRIVES YOU? A SCIENTIFIC AND MATHEMATIC STUDY OF PERFORMANCE AND FUEL ECONOMY
Christopher Blake Overby
Wetumpkna High School, Wetumpka, AL USA

The purpose of my project is to find a way to boost the performance of an engine and increase its fuel economy. My hypothesis is that a 2.0 liter displacement four cylinder engine with multi-port fuel injection, dual overhead cams, Bucket-and-shim four valve cylinder heads, bore and stroke measurements of 3.15x4.00 with a 9.0:1 compression ratio, a heavy flywheel, and a six-speed low-to-high end transmission will allow the best performance and fuel economy. <br><br>The “strokes” of a motor are what actually drives it. The intake stroke begins when the piston is at the top of the cylinder and begins moving down. Then the intake valve opens and draws in the fuel and air mixture. The intake valve closes and the piston begins an up stoke. This is when the fuel air mixture is compressed and is the compression stroke. The piston reaches the top of the cylinder and the spark plug ignites the fuel air mixture. This sends the piston on another down stroke. This is the combustion or power stroke. When the piston begins another upstroke the exhaust valve opens and the burnt fuel exits through the valve. This is the exhaust stroke. <br><br>My conclusion was my hypothesis was correct, aside from the heads. Bucket and-shim heads provide too much performance on your motor and will end up using more fuel than what is needed to drive the car. Standard heads are sufficient in performance and fuel economy without restricting the amount of power the engine can produce. <br><br> 
________________________________________
2009 - MA008 
DOES THE BRAIN PROCESS SOUNDS MATHEMATICALLY?
Zachary Joseph Branson
duPont Manual Magnet High School, Louisville, KY

The purpose of this project is to devise a unique method of combining mathematics, music, and sounds to formulate a viable theory for how the brain processes sound. Basic music theory was used to show that there is a clear mathematical relationship among the notes of the musical scale and the ratios of combinations of notes. It was argued that the brain processes musical chords mathematically, finding those that are easy to analyze pleasing, and those that are difficult to analyze harsh. The hypothesis is that the brain processes all sounds mathematically and that simple ratios lead to pleasing sounds and complex ratios lead to harsh sounds. The mathematical analysis tool MATLAB was utilized to perform a Discrete Fourier Transform on 50 different sound files (25 “pleasing” and 25 “harsh”) to obtain the separate frequencies that make up each sound. A computer program was written to mathematically translate these frequencies into the equivalent musical notes and to determine the ratios between the bass frequency and the other frequencies. It then analyzed the sounds as musical chords to determine, based on music theory, if they would be pleasing or harsh if played on a musical instrument. Overall, 80% of the trials support the hypothesis that sounds with simple ratios sound pleasing and sounds with more complex ratios sound harsh. It can be concluded that the brain analyzes all sounds the same way that it analyzes music, and it can be inferred that there is a mathematical component to this analysis. 

Awards won at the 2009 ISEF
Genius Scholarships - Sierra Nevada College
________________________________________
2007 - MA009 
STRICTLY PLATONIC
Ralph F. Jennings, III
Southside High School, Atlanta, GA United States

The purpose of this project was to prove that origami can be used as a tool in the study of Mathematics. The researcher hypothesized that a lesson that allowed students to analyze and solve math problems using origami would be more stimulating and have an altering effect on the perception of math learning. This hypothesis was proven to an extent. He also hypothesized that geometric origami could be used with express focus on the Platonic Solids to construct three dimensional geometric shapes. This hypothesis was proven correct. The Platonic Solids were chosen to be the focal point. Origami was then used to construct the five Platonic Solids, three dimensionally, and to study them.<br><br>Studies have shown that the student would ultimately have a more enjoyable time exploring Mathematics if only they had the hands-on opportunity that origami provided. Previous Studies have also shown that students put in an environment that teaches math with a lecture styled presentations, were less focused and more distracted. These studies later go on to show that students who were taught math in a more unschooled, stimulating, and invigorating experience retained the information, and saw the relevance of Mathematics. This exemplifies the importance of my project. Using my method of teaching instead of the lecture style allows students receive all of the benefits of earlier pedagogic teachings like New Math, and Unschooling, without any of the drawbacks. In addition to geometry they are able to branch out to other areas of Mathematics, such as Topology.<br><br> 
________________________________________
2008 - MA009 
FREQUENCY SEQUENCES: STRUCTURE AND PROPERTIES
Swara Satya Kopparty
Terre Haute South Vigo High School, Terre Haute, IN

Let S = {a , a , a …} be a set of natural numbers with a < a < a <…. We define the difference set, D(S), of S to be the multiset given by D(S) = {a – a : j > i}. The frequency sequence of S is defined as = { : k = 1 to }, where is the number of times k appears in D(S). The difference set and frequency sequence are also defined in this manner for finite sets S. For example, if S = {1, 2, 3, 8}, then D(S) = {1, 1, 2, 5, 6, 7}, and = {2, 1, 0, 0, 1, 1, 1}. We consider the problem of characterizing frequency sequences for both finite and infinite sets. We give several necessary conditions for a sequence to be a frequency sequence. For example, from our results it will follow that (i) the infinite sequence {1, 0, 1, 0, 1, 0…} is not a frequency sequence, (ii) the finite sequence {1, 1, 1, 1, 1, 1, 1, 1} is not a frequency sequence, and (iii) the infinite sequence {1, 0, 1, 0, 1, 0, 1, 1, 1, 1….} is a frequency sequence. Combinatorial arguments are used in the proofs. 

Awards won at the 2008 ISEF
Honorable Mention Award - American Mathematical Society
________________________________________
2003 - MA010 
MONTE CARLO TABULATIONS OF VIRTUAL KNOTS
Albert Reid Carter
Murphy High School, Mobile, Alabama, Uninted States of America

In this project a computer program was written to perform the following tasks: randomly generate 35,000 Gauss words in each of 1-50 crossings, use a Monte-Carlo simulation to estimate the probability distribution of their minimum possible genera, and graph the data. The roots of the Jones polynomials of 350 Gauss words per number of crossings (1-20) were then computed and graphed.<br><br>It was hypothesized that the graphs of these genera would form a normal distribution. The number of words present was hypothesized to peak at genus n/4. It was also hypothesized that the Jones polynomials' graphs would be symmetric with respect to the x-axis and otherwise be evenly distributed.<br><br>Data indicate that when n is even, roughly 50% of the curves are of a genus one less than the maximum possible genus. For odd crossing curves, the largest possible genus is (n+1)/2, and the plurality of the curves are two less than this maximum. For even crossing curves, similar phenomena occured.<br><br>As the number of crossings became larger, the probability of finding a genus zero curve decreased. For example, at 12 crossings, less than 1 in 10,000 was genus zero.<br><br>When curves of 1 to 20 crossings' Jones polynomials were graphed, they were found to have symmetry with respect to the y as well as the x-axis. These polynomial's roots were also found to group around the areas y=+1 while two diagonals showed sparsely populated regions.<br><br> 
________________________________________
2010 - MA010 
UNIVERSAL LAW FOR THE DISTRIBUTION OF ODD PERIODIC CYCLES WITHIN CHAOS IN NONLINEAR DYNAMICAL SYSTEMS: A FINE CLASSIFICATION OF ODD CYCLES (YEAR III)
Almas Abdulla
West Shore Junior Senior High School, Melbourne, FL

This project continues the analysis of the mathematical nature of the distribution of the odd periodic windows within chaos in nonlinear logistic-type dynamical system. Past years’ projects have revealed the following universal transition mechanism from chaos to odd periodic according to the non-monotonic <br><br>+4 – 2 law. <br><br>Chaos => … => 13 => 15 => 11 => 13 => 9 => 11 => 7 => 9 => 5 => 7 => 3 => Chaos<br><br>Every superstable odd cycle of a period larger than 7 appears twice in this diagram. To understand the non-monotonic nature of the distribution of odd cycles, this project attempts to present a fine classification of odd cycles by employing symbolic dynamics and representing each odd cycle with a cyclic permutation and an oriented graph of transitions. All the superstable odd cycles have a universal cyclic permutation and oriented graph of transitions.<br><br>Moreover, for any odd integer k > 5, the cyclic permutation and the oriented graph of one of the two superstable k -cycles (the second one from right) in diagram has a first-kind cyclic permutation and transition graph according to the Sharkovsky theory. It is revealed that the other superstable k-cycles have different cyclic permutations and transition graphs, which are called second-kind cyclic permutations and transition graphs. This project demonstrates that the second-kind transition graph of the k-cycle is a superposition of the transition graphs of the two <br><br>k - 2-cycles with first-kind transition graphs. It is conjectured that the second-kind transition graph of the k-cycle is a unique transition graph, (with the exception of the inverse transition graph) which is implied by the fact that the map has odd cycles of periods k and k - 2, but no odd cycles of smaller period. 

Awards won at the 2010 ISEF
Third Award of $250 - American Mathematical Society
________________________________________
2007 - MA010 
UNIVERSAL LAW FOR THE PERIODIC WINDOWS WITHIN CHAOS IN THE LOGISTIC MODEL FOR NONLINEAR PHYSICAL SYSTEMS
Almas U. Abdulla
Palm Bay High School, Melbourne, FL, USA

This project investigates the Chaos phenomena in nonlinear physical systems described by differential equations. A prototypic system is the Duffing oscillator, described by the second order nonlinear, non-autonomous, ordinary differential equation with cubic nonlinearity, which presents a mathematical model of motion performed by the plane pendulum under a periodic external force. By using numerical calculations and phase space analysis, the transition from periodic to chaotic behavior (and vice versa) is analyzed. By changing the damping parameter, the transition to chaos through the bifurcations of limit cycles is demonstrated. It is calculated that the convergence rate of the damping parameter after the bifurcations between the 2- and 16- cycles is approximately equal to Feigenbaum’s universal constant 4.6692….Numerical results show that after four successful bifurcations, the 16-cycle unexpectedly exchanges with a stable 3-cycle, which further bifurcates to 6- and 12-cycles, until the chaotic strange attractor is reached. Further decrease of the damping parameter provides the transition from chaos to odd periodic limit cycles. It is numerically proven that the stable 9-, 7- and 5-periodic limit cycles successfully lead the motion, subsequently, the latter bifurcates to a 10- cycle, and further to a 15-cycle, which again leads to a chaotic strange attractor. Finally, for small values of the damping parameter, a stable 1-limit cycle emerges from chaos. <br><br> 

Awards won at the 2007 ISEF
Honorable Mention Award - American Mathematical Society
________________________________________
2010 - MA010 
UNIVERSAL LAW FOR THE DISTRIBUTION OF ODD PERIODIC CYCLES WITHIN CHAOS IN NONLINEAR DYNAMICAL SYSTEMS: A FINE CLASSIFICATION OF ODD CYCLES (YEAR III)
Almas Abdulla
West Shore Junior Senior High School, Melbourne, FL

This project continues the analysis of the mathematical nature of the distribution of the odd periodic windows within chaos in nonlinear logistic-type dynamical system. Past years’ projects have revealed the following universal transition mechanism from chaos to odd periodic according to the non-monotonic <br><br>+4 – 2 law. <br><br>Chaos => … => 13 => 15 => 11 => 13 => 9 => 11 => 7 => 9 => 5 => 7 => 3 => Chaos<br><br>Every superstable odd cycle of a period larger than 7 appears twice in this diagram. To understand the non-monotonic nature of the distribution of odd cycles, this project attempts to present a fine classification of odd cycles by employing symbolic dynamics and representing each odd cycle with a cyclic permutation and an oriented graph of transitions. All the superstable odd cycles have a universal cyclic permutation and oriented graph of transitions.<br><br>Moreover, for any odd integer k > 5, the cyclic permutation and the oriented graph of one of the two superstable k -cycles (the second one from right) in diagram has a first-kind cyclic permutation and transition graph according to the Sharkovsky theory. It is revealed that the other superstable k-cycles have different cyclic permutations and transition graphs, which are called second-kind cyclic permutations and transition graphs. This project demonstrates that the second-kind transition graph of the k-cycle is a superposition of the transition graphs of the two <br><br>k - 2-cycles with first-kind transition graphs. It is conjectured that the second-kind transition graph of the k-cycle is a unique transition graph, (with the exception of the inverse transition graph) which is implied by the fact that the map has odd cycles of periods k and k - 2, but no odd cycles of smaller period. 

Awards won at the 2010 ISEF
Second Award of $1,500 - Mathematical Sciences - Presented by Intel
________________________________________
2004 - MA011 
MATHEMATICAL ANALYSIS OF INVERSE RETROGRADE CANONS
Ana Marie Peterlin
Monroe Catholic High School, Fairbanks, Alaska, United States of America

This project is a mathematical analysis of Mozart’s "Table Music for Two" and my table music, "Upside Down and Right Side Up!" Both pieces are inverse retrograde canons (the music is laid on a table and one-person starts playing at one end while another person starts at the other).<br><br> "Upside Down and Right Side Up!" is shown to mathematically use the same time signature and key signature as well as scale and range as Mozart’s "Table Music for Two". Both compositions utilize eighth notes, quarter notes, half notes, and quarter rests<br><br> The mathematical differences found in "Upside Down and Right Side Up!" and "Table Music for Two" are sixteenth notes, dotted quarter notes, dotted half notes, eighth notes, half rests, tied eighth plus half note, two tied quarter notes. None of these notes and rests is used in "Table Music for Two".<br><br> Results show that "Table Music for Two" has zero sixteenth notes, 224 eighth notes, 64 quarter notes, zero dotted quartered notes, 30 half notes, zero dotted half notes, zero eighth rests, 14 quarter rests, zero half rests, zero tied eighth and half note, and zero two tied quarter notes. "Upside Down and Right Side Up!" utilizes 40 sixteenth notes, 73 eighth notes, 63 quarter notes, 1 dotted quarter note, 15 half notes, 5 dotted half notes, 17 eighth rests, 21 quarter rests, 9 half rests, one tied eight plus half note, and 1 two-tied quarter notes.<br><br> The two inverse retrograde canons have more mathematical differences than similarities.<br><br> 
________________________________________
2008 - MA011 
QUANTITATIVE EVALUATION OF CANCER COMPLEXITY: A STUDY OF FRACTAL MORPHOMETRY, YEAR TWO
Samuel Irvin Kornicks
Vero Beach Senior High School, Vero Beach, FL

In order to improve the reliability of cancer diagnosis and facilitate an objective judgment that will complement a pathologist, computational tools for automated cancer diagnosis can be developed. <br><br>Digital technology and the study of fractals have created an ability to develop computed aided diagnosis. This is simply done by counting the pixels that make up digital images. A set cutoff point is to be determined; above which is considered to be atypical, below which is considered to be benign.<br><br> Digital images of over 100 human fixed tissues slides were taken under a microscope in a pathology lab. The five cancer types included breast, lung, kidney, colon, sigmoid colon and lymph node. Data and analysis of 80 images were completed using ImageJ software. <br><br>Measurements such as fractal dimension with y - intercept, lacunarity, and density were recorded. Each tissue type was analyzed separately, and together. Slight variations between tissue types were noted. The means and ranges for the most significant factors such as density and fractal dimension were used to develop an algorithm. The algorithm correctly diagnosed 73 out of 80 images to be either cancer or non-cancerous. The positive predictive value was 92.3%; the negative predictive value was 89.4%. Completing a t-test showed a statistically significant difference in the means between the cancer and benign cells. <br><br> The most difficult to diagnose are the samples whose values are at the cutoff point of the algorithm. A total of 55 values fell in the middle range out of 480 values. 

Awards won at the 2008 ISEF
Fourth Award of $500 - Mathematical Sciences - Presented by
First Award of $1,000 - Mu Alpha Theta, National High School and Two-Year College Mathematics Honor Society
________________________________________
2010 - MA011 
RISK MITIGATION STRATEGIES FOR QUANTS: FROM BLACKJACK TO THE STOCK MARKET
Talia Kate Greene
Cocoa Beach Junior/Senior High School, Cocoa Beach, FL

The fundamental logic of blackjack card counting was employed to develop a two-part stock market investing strategy. The first selected stocks with a high likelihood of future growth and the second quantified that likelihood and used it to determine the most advantageous investment for each stock.<br><br> Four blackjack betting strategies were tested for efficacy at exploiting a previously identified player advantage; the strategy identified as most efficient would be used as the second component of the investing strategy. Computer simulation was used for testing – a program was written that played blackjack using each strategy. The simulation demonstrated Strategy D the most effective betting strategy, far outperforming the others, likely because it had a direct relationship between the amount bet and the amount available, minimizing losses and maximizing wins. <br><br> The investing strategy was then developed and tested. Metrics commonly used as gauges of company performance were chosen: operating income growth, revenue growth, P/E ratio, PEG ratio, gross margin growth, ROE, debt-to-equity, and earnings growth. Values were assigned to these metrics that quantified their correlation to future growth. Betting Strategy D was applied to the total value for each company to determine the optimum investment. Simulations using historical financial records were performed to assess the strategy’s profitability relative to random investment in a select group of companies and to an index. The strategy significantly outperformed the controls in the large cap universe where the companies are relatively stable. It also outperformed the controls, by a smaller margin, in the more volatile small cap universe. 

Awards won at the 2010 ISEF
Genius Scholarships - Sierra Nevada College
________________________________________
2004 - MA014 
CLOSE PRIMES AND RSA KEY SELECTION PROCEDURES A THREE YEAR STUDY
Alexandra Vida Marraccini
MAST Academy, Miami, Florida, United States

The purpose of this research is to analyze the risk of selecting close primes as RSA private keys. The RSA cryptosystem is used worldwide, both in economic and military endeavors, making security maintenance a research priority. <br><br> In the first two years of this study, an algebraic representation of probabilistic risk was developed and analyzed via theoretical methods including matrix partitioning and a multivariate calculus-based approach. This year’s study builds on prior research by utilizing the Mathematica software package to provide both numerical and visual representations of trends in probabilistic risk of close key selection. The behavior of the probabilistic risk function for close key selection was examined in both two and three variables.<br><br> Significant patterns in data reflect a risk/key-size correlation, and limited chaotic fluctuation in risk associated with the distribution of primes. Analysis reveals a possible candidate for modeling probabilistic risk of close key selection, as well as trends in periodic risk minima. The likelihood of selecting two close primes as private keys is found to be from approximately 10^-6 to 10^-7, prompting a possible need to revise RSA key selection procedures to explicitly prohibit the selection of close keys.<br><br> 

Awards won at the 2004 ISEF
Award of $3,000 in savings bonds, a certificate of achievement and a gold medallion - U.S. Army
________________________________________
2004 - MA015 
IT'S NOT TOO LATE FOR THE EARLY PRIMES
Madeleine Heather Goldstein
Dr. Michael M Krop Sr. High, Miami, Florida, United States

My research was completed based on my fascination with prime numbers and their properties. I have always been intrigued by numbers so pure that their only divisors are one and itself. While conducting research, I stumbled upon a unique subset of the prime numbers called the Early Primes. An Early Prime is a prime number that is less than the arithmetic mean of the prime before and the prime after. Upon investigation, I sought to determine a relationship among Early Prime numbers based on the set of prime numbers; would it be possible to generate the next Early Prime number given a specific Early Prime?<br><br> The initial data was collected by arithmetic calculation. Subsequently, I created a computer program in which a set of prime numbers were generated, after being tested to see if it fit within the confines of an Early Prime. If so, it was added to the set of Early Primes being generated. I sought patterns and sequences among this extraordinary subset. I observed that every Early Prime was always closer to the lower prime of the arithmetic mean. I also found a pattern among the density of Early Primes existing in each set of 'n' prime numbers. The computer program, based on an algorithm that generates primes and Early Primes, helped me to discover sequences in numbers and the magic associated with them. I concluded my research with a plethora of new ideas and rich patterns that exist in the prime numbers.<br><br> This research has led me to discover other subsets of the prime numbers, including those primes that are equal to their arithmetic mean of the prime before and after. I hope to discern other such patterns that exist within the set of primes and Early Primes as my research expands. 
________________________________________
2010 - MA016 
COMPUTER MODELING REVEALS A NOVEL PROPERTY DESCRIBING THE RELATIONSHIP BETWEEN TRIANGLES AND THEIR INTERNAL HEXAGONS
Nathan Kondamuri
Munster High School, Munster, IN

Geometry is used in every day life in fields as disparate as engineering, automobile manufacturing, or skyscraper construction. In these real world applications, triangles are essential. I hypothesized that there must be some mathematical formula or ratio that relates the area of the triangle to the area of the interior hexagon formed after each side of any triangle is divided X number of times. If each side of any triangle is divided into congruent segments, and if lines are drawn from these points and connected to the opposite vertices of the triangle, a hexagon is formed in the interior of the triangle and the hexagon’s area is related through a ratio to the original triangle's area. After running experiments, initially by making rough drawings of these triangles and later on situating the same triangles into a computer-modeling program, I confirmed that the ratio of the area of the interior hexagon will always change in relation to the number of congruent segments on each triangle’s side. Graphing the polynomial regression of the data found for each triangle, I derived two separate equations (one for the even and one for the odd number of congruent segments) that describe the ratio of the triangle’s area to the hexagon’s area. I later experimented with isosceles, scalene, and equilateral triangles to learn to what extent my property holds true. Understanding and incorporating this geometric property of triangles and their relationship to interior hexagons will contribute to improved engineering, architectural design and nanotechnology. 

Awards won at the 2010 ISEF
First Award of $3,000 - Air Force Research Laboratory on behalf of the United States Air Force
________________________________________
2005 - MA017 
COMPUTATIONAL MODELING OF PHYSICAL INTERACTIONS IN THREE DIMENSIONS
James Halligan Powell
Terre Haute South Vigo High School, Terre Haute IN, USA

The use of computers to model natural phenomena is becoming increasingly frequent in many areas of science and industry. The differential equations arising in these situations are generally much too complicated to be solved exactly, so numerical techniques are used. Numerical methods for solving first order differential equations have been studied for many years, and many algorithms for approximating solutions to these equations have been developed. For higher order equations, solutions are simply transformed into a system of first order equations and solved using first order methods. This project creates new techniques of finding numerical solutions for second order equations, which commonly describe many physical interactions found in nature. Modifications of the Euler, Heun, and Runge-Kutta methods are made specifically to apply to second order systems. Third and fourth order predictor-corrector algorithms are also tested against these algorithms, although modifications of these algorithms are not made. To determine which most accurately approximate the solution, the standard algorithms and modified versions are tested with different scenarios to which exact solutions may be found.<br><br>The modified algorithms were shown to be significantly more accurate than the standard algorithms in some situations and less accurate in others, and no overall best algorithm was found. The nature of the simulation is very important in determining what algorithm should be applied. One unexpected observation is that sometimes the error of a simulation can be incredibly predictable. It may be possible for some functions to create a curve which closely approximates the error of the algorithm. 

Awards won at the 2005 ISEF
$1,500 Third Place Scholarship Award in Mathematics - Robert Luby, Jr. of IBM
________________________________________
2006 - MA022 
HOW THICK IS YOUR ZINC?
Lindsey Dale Tillman
Wildwood High School Wildwood, FL United States of America

My project was designed to use a mathematical formula to determine the thickness of the zinc coating on galvanized metal. There were 5 trials of three pieces of metal each. Trial 1 was 11 gauge metal, Trial 2 was 16 gauge metal, Trial 3 was 20 gauge metal, trial 4 was 24 gauge metal and Trial 5 was 26 gauge. Each piece was weighed before experimentation and then dipped into hydrochloric acid to burn off the zinc coating. Then the mass was weighed again. The data was taken and used in several math formulas to determine the thickness of zinc coating on galvanized metal. <br><br> My conclusion is that yes, by calculating the volume of zinc on the metal and knowing the density of zinc, the thickness of zinc coating on galvanized metal can be determined using a mathematical formula. The information acquired from m project is informative to a wide variety of people. Over 70 million tons of galvanized steel is produced globally each year. This experiment helps consumers comprehend how much protection galvanization provides. Many people rely on galvanized metal to ensure long-lasting products and structures, and by knowing how thick the zinc coating on metal is, these people can better understand just what keeps their steel safe.<br><br> 
________________________________________
2007 - MA022 
NUMERICAL ANALYSIS OF MODERN SONGS USING MUSICAL GRAPH THEORY
Joel Allan Eichelkraut
Catholic High School, Huntsville, AL, USA

Last year, my project was dedicated to finding the total number of sounds that can be made by an electric guitar. That project determined that 52,500 sounds can be made using a standard electric guitar, showing that music has the opportunity to be a lot different. This year I took my project in a different direction by using Musical Set Theory to show the similarity of varying pieces of music. Forty songs from four genres of music were compared, using pitch class sets as a discriminate. A pitch class set is a way of mathematically notating musical chords. For the data to be accurate the normal form, inverted form, and prime form of each pitch class needed to be found. It was expected that varying genres would rely on different pitch class sets. After 3,555 pitch class sets were analyzed it was found that (0,3,7) was the primary pitch class set in 98% of the songs. As predicted Country, Pop, and Rock were very similar, and Classical music was different. 
________________________________________
2005 - MA024 
FACTORING LARGE NUMBERS
Jeffry Marshall Gaston
South Windsor High School, South Windsor, Connecticut, United States

I have devised a method for factoring large numbers. This experiment was conducted to determine the relative speeds of this method and other methods. The algorithm being tested used modular arithmetic and differences of squares. I wrote a C++ computer program with approximately 5,000 lines to test multiple versions of the method. After the experiment, it was concluded that for almost any number, this method is better than brute force or Fermat’s method. Using estimates for speed of the General Number Sieve, which is the best other known method, it was estimated that for numbers with about 20 digits, my method took a little bit more than half as many steps. It was also concluded that for numbers with less than approximately 23 digits, my method would probably be faster than the General Number Field Sieve on average. However, for numbers more than approximately 23 digits, the method will probably be slower. Also, my method did better than Pollard’s Rho method during many tests. 
________________________________________
2003 - MA024 
EXPLORING ALTERNATIVE ROUTING OPTIONS: A THREE YEAR STUDY
Yan Zhong
Vero Beach High School, Vero Beach, FL USA

Route optimization is a necessity in the present-day world, and this research seeks to develop efficient routing for the Press Journal Route #419. The Chinese Postman Problem (CPP) is when each edge of a network must be traversed, but this allows for multiple crossings; it is an expansion of the Euler Path. The CPP has been utilized in the previous two years to formulate better routes compared to its current standard. By compiling the data and employing genetic optimization, it was hypothesized that better routes (measured by time and distance) should be developed because genetic optimization would seek out patterns and test the fitness of remaining codes.<br><br> The procedure evolved through trial-and-error used in genetic optimization. Like genetic optimization, routes from previous years were “crossed over” in order to find patterns from previous experimentation coding. The patterns were taken out systematically and the remaining encryptions for each graph were tested for fitness. Those that had the best fitness were compiled in order to find the best “offspring” of these routes. This procedure was done for two iterations of the route, yielding groups A (one iteration) and B (two iterations).<br><br> After experimentation, results were analyzed to discover possible data patterns. Group A’s average route length was approximately 24.76 km, suggesting a reduction from the original 31.26 km of Route #419 of 20.81%. Group B’s average route length was about 24.60 km, or a reduction of around 21.32%. These results, which generated the lowest averages compared to previous data, supported the hypothesis. <br><br> <br><br> 

Awards won at the 2003 ISEF
Fourth Award of $500 - Mathematics - Presented by Panasonic Consumer Electronics Company
________________________________________
2006 - MA027 
HEXAFLEXAGONS: ARE YOU IN THE GROUP? OR OUT?
Emily Lynne Hartley
Academy of Math and Science, Tucson, Arizona, United States of America

This project focuses on the first four hexaflexagons constructed from straight nets and whether or not the sets of transformations on these objects form groups. A group <G,*> is a set closed under the binary operation * with an associative property and where each element has an identity and an inverse. The elements are the rigid transformations that can be performed on hexaflexagons and * is the composition of these transformations. <br><br> The purpose of this project was to determine if the trihexaflexagon is the only type that forms a group. The charts, tables, and structure diagrams created from the physical model of the flexagon were essential in discovering which ones did or did not form groups. In examining the charts for the hexahexaflexagon, it was determined that the inverses of certain elements of the set of transformations could not be performed from all positional states of the flexagon. Thus, the inverse property of the definition of a group fails. <br><br> The structure diagrams of each flexagon centers around the flexagon that comes before it; so by restricting the elements of the set G to certain positions of the flexagon, I obtained the set of transformations of the prior flexagon. Hence, proving that the hexahexaflexagon is not a group led me to the conclusion that no hexaflexagon larger than the hexahexaflexagon could possibly be a group. <br><br> The next step of this project would be to analyze those hexaflexagons without straight nets to determine whether this conclusion would still hold true.<br><br> 
________________________________________
2005 - MA028 
IS THERE A CORRELATION BETWEEN BODY MASS INDEX AND OXYGEN SATURATION LEVELS OF SLEEP APNEA PATIENTS?
Rachel Elyse Rhodes
Spruce Creek High School, Port Orange, Florida, United States of America

It has often been noted that heavier people tend to have a harder time sleeping than those of smaller caliber. An experiment was designed to show that the greater the body mass index is of an individual, the more prone to lower oxygen saturation levels one would be, thus resulting in a poor night's sleep. A retrospective study was then designed. Fifty unnamed patient charts were then obtained from a sleep disorder center. Height, weight, age, baseline oxygen saturation levels, and minimum oxygen saturation levels were charted. Body Mass Indexes (BMI's) were then calculated from the heights and weights. The difference between baseline oxygen saturation levels and the minimum oxygen saturation levels were charted. Graphs were then made comparing the Body Mass Index with both the minimum oxygen saturation levels as well as the difference between the baseline and the minimum oxygen saturation levels. Chi-square and calculation of the correlation of data were then used to prove the accuracy of the experiment. The results revealed that the greater the Body Mass Index, the lower the minimum oxygen saturation level tended to be. Also the greater the Body Mass Index, the greater the difference between the baseline and the minimum oxygen saturation level was. Sleep apnea and elevated Body Mass Index levels cause many health problems including heart disease, hypertension, decreased mental status, gastroesophageal reflux, and many other health problems. With the knowledge learned from this experiment, these health problems may be averted by a simple sleep apnea test and treatment regimen. 
________________________________________
2003 - MA031 
SCALPEL OR MOUSE? A STATISTICAL COMPARISON OF REAL AND VIRTUAL FROG DISSECTIONS
Victor Edward Cross
Central High School, Phenix City, Alabama, USA

Frog dissections have become increasingly controversial. Many people feel that frog dissections are cruelty to animals, and many students feel that the dissections are gross. In the spring of 2002, students in AP Biology classes were divided randomly to compare the effectiveness of learning (as tested by laboratory practical) when half of the students dissected preserved frogs and the other half of the students dissected using a computer dissection program. Since the Muscogee County School district would only agree to the experiment if the experiment designer did not observe the lab practical or know the individual results by student name, the Lead Science Teacher for the Carver High School Integrated/Math/Science/Technology Magnet Program actually conducted the lab practical and the supervising teacher scored the tests, and presented the data to the experiment designer in chronological order. The null hypothesis was that there would be no significant difference (0.01 level) in the performance of the students regardless of the method of dissection. The alternate hypothesis was that students who dissected organic frogs would perform significantly better on the laboratory practical than students who dissected the frogs.<br><br> A statistical analysis at the 0.01 significance level revealed that students dissecting the preserved frog scored significantly better than students who had performed the dissection on the computer. The null hypothesis was rejected, and the alternate hypothesis was accepted.<br><br> 
________________________________________
2007 - MA031 
THE DISTRIBUTION OF PHI IN THE MUSICAL ERAS
Michael Robert Post
Sarasota High School, Saraota, Florida, USA

What causes appeal? What drives a person to prefer Monet over Rembrandt or Debussy over Mozart? This is a question that if answered, would benefit society for ages. Phi, also known as the Golden Section, is the ratio on a divided line segment in which the smaller section divided by the larger section is equal to the larger section divided by the whole segment. What makes phi “Golden” are its frequent occurrences in aesthetically pleasing works of art, architecture, and nature. The purpose of this research is to determine whether the distribution of songs with phi is uneven throughout the Baroque, Classical, Romantic, and Impressionist eras. The researcher hypothesized that there would be an uneven distribution of songs with phi, and that the Classical era would have the greatest frequency of occurrence. From each era, two representative composers were chosen and five pieces each were randomly selected. The music was analyzed for phi by determining the timing of changes. The total number of measures was recorded for each piece. This total was multiplied by phi, and the derived measure was analyzed for any major changes in the music including key signature, tempo, dynamics, or time signature. Although the classical era did have the most songs with phi, a low chi square contribution value determined the data cannot be found significant and do not support the hypothesis. Because the occurrence of phi is so widespread throughout the world, the implications of this research are nearly endless. Future research could investigate the effect a song with phi has on the brain compared to a song without phi. 
________________________________________
2004 - MA031 
A QUANTUM ALGORITHM FOR THE SIMULTANEOUS EVALUATION OF FUNCTIONS: A COMBINATORICS SOLUTION WITH FRACTAL PROPERTIES
Nimish P. Ramanlal
Seminole High School, Sanford, Florida, United States

Quantum computers, through the properties of superposition and entanglement, are able to achieve parallel processing which allows them to solve problems that would otherwise take indefinite amounts of time on a classical computer.<br><br> <br><br>Unfortunately, no general method has been proposed for devising quantum algorithms. For each mathematical problem or physical simulation, algorithms are developed by ad hoc means. Thus few problems have the potential to be solved on quantum computers.<br><br> <br><br>The purpose of this study is to develop a general method for writing quantum algorithms. Since all logical operations can be expressed as boolean functions f: {0,1}^n -> {0,1}, it is necessary to determine a systematic way by which these functions can be evaluated using quantum algorithms. The method must be scalable for arbitrary n and provide exponential improvement over classical algorithms.<br><br> <br><br>In this study, the set of boolean functions f: {0,1}^n -> {0,1} for arbitrary n are represented by a single truth table. Quantum gates necessary to replicate this truth table are obtained. In a diagramatic representation of these gates, the Sierpinski Triangle takes form. Thus the gates for any value of n can be determined via examination of this pattern.<br><br> <br><br>By effecting a Hadamard transform on all qubits, those that represent the functions and the variables, administration of the gates result in the simultaneous evalution of all functions. The total number of functions, 2^2^n, are evalued by just 3^n gates representing exponential improvement over classical algorithms. 

Awards won at the 2004 ISEF
Honorable Mention Award - American Mathematical Society
First Award of $1,000 U.S. Savings Bond - Ashtavadhani Vidwan Ambati Subbaraya Chetty (AVASC) Foundation
First Award of $700 - IEEE Computer Society
Second Award of $1,500 - Mathematics - Presented by Panasonic Consumer Electronics Company
________________________________________
2009 - MA031 
EFFICIENT TRUE RANDOM NUMBER GENERATION
Dylan Freedman
Carmel High School, Carmel, CA

In today's world, random number generation has become a necessity to cryptography, statistical analysis, mathematical simulation, and gambling. There are many established methods of random number generation, few of which are effective in both quantity and cryptographic security.<br><br>My project investigated methods that efficiently generated true high quality random numbers meeting these criteria.<br><br>To start experimentation, I created a basic framework to implement methods in Java. I used a public online webcam focused on Times Square, New York as a source of entropy and wrote a simple class to process these pixel values. I then implemented five control methods based on preexisting algorithms or data sources. I constructed twelve of my own methods, six of which applied pseudorandom algorithms to my true source of entropy. For each of my methods, I computed the average processing time taken in bits per millisecond and the average data produced in bits per image. These quantities were measured with the same sample sizes.<br><br>The final method I created was by far the most efficient. It strategically applied true random numbers to reseed the famous Mersenne Twister algorithm. To further obfuscate the data, it used an xor operation on the results of one iteration and the seeding values of the previous iteration. This method quickly and efficiently produced a high quantity of high quality, cryptographically secure, true random numbers. Compared to the random number generating algorithms I have researched, this method appeared to be the most effective in quantity, quality, and cryptographic security. 

Awards won at the 2009 ISEF
Second Award of $1,500 - Air Force Research Laboratory on behalf of the United States Air Force
________________________________________
2005 - MA032 
A MATHEMATICAL ANALYSIS OF GENDER DIFFERENCES IN COLOR PERCEPTION
Katherine Louise Bouman
West Lafayette Junior-Senior High School, West Lafayette, IN

The data for the opponent channel curves of 24 subjects (12 female and 12 male) was extracted by the use of a program created on Matlab. First, the computer monitor that all subjects were tested on was calibrated. Subjects were then taken into a dark room and were tested on the same computer monitor. The subjects chose the red-green opponent channel curve first. After 30 seconds of exposing them to an equal energy white screen, the subject saw a colored square with a simulated color of 700 nm light. The subject added more or less green or red until they saw a color that they believed had no red or green left in it. This was done for every 20 nm of color ranging from 700 nm to 400 nm. The same procedure was performed for the blue-yellow curve. After finding the standard deviation of the difference in the means, the summed statistical significance was found. The red-green curve resulted in 37.09% statistical significance, while the blue-yellow curve had a 0.3% statistical significance, meaning there is a three in a 1000 chance the results gained were just by coincidence for the blue-yellow curve. The wavelength with the most difference in perceived color was 520 nm, or approximately pure green. Subjects were then compared to each other by the use of a derived similarity measurement. This also showed a difference in of what males and females perceive as pure green and yellow. A 14.43% statistical significance was gained from this. 
________________________________________
2010 - MA033 
BETA ADJUSTED RATIO QUANTIFYING VOLATILITY OF MUTUAL FUNDS
Matthew John Richardson
Basha High School, Chandler, AZ

Variability and risk for individuals investing in the market today can be somewhat daunting. The experimentation undertook during these processes offers a new ratio for risk adjustment and performance evaluation. By broadening the length of the guaranteed investment bill and accounting for more variability within the model, the result should produce a superior statistical ratio compared to the accepted Sharpe ratio. This new ratio can give investors more confidence in their performance appraisals. When applied, the new statistical measurement will give a risk adjusted assessment different from that of the conventional Sharpe ratio. The volatility of the market’s influence is integrated within the formula to produce a performance evaluation accounting for the possibility of confounding variables affecting the Sharpe. The results will likely impact the estimates of returns in a mutual fund. Thus, this ratio can make a substantial difference in the estimates of returns present in a mutual fund. <br><br>The actions undertaken through the course of experimentation developed this new statistical model. Primarily, using a systematic random sample of mutual funds from varying categories, data was gathered including entities such as mean return values, standard deviations, betas, alphas, and Sharpe ratios. Subsequent to collection of data for not only these managers, but those for Treasury Bills as well, the implication of these values in the newly derived formula yield a valuable test statistic capable of quantifying risk adjusted returns of any fund across any category. 
________________________________________
2010 - MA034 
AN ANALYSIS OF THE PRIMITIVE CYCLES EXISTENCE CONJECTURE
Dhiraj Rawat Holden
University High School, Fresno, CA

The Primitive Cycles Existence conjecture, a generalization of the 3x+1 problem to 3x+d, where d is relatively prime to 6, as proposed by Lagarias, deals with the 3x+d function, T(x) = (3x+d)/2 if x is odd or x/2 if x is even. It states that for every d satisfying the above conditions, there is at least one x relatively prime to d such that for one k, the kth iteration of T(x) = x, or that a primitive cycle exists for d. The work presented establishes conditions for which a primitive cycle is possible by proving two theorems relating to the Primitive Cycles Existence Conjecture. The first theorem details conditions for a number divisible by a number of a certain form that is necessary for it to be a primitive cycle, and the second theorem builds on the first theorem to determine under what conditions a possible cycle can exist. These cycles are a subset of all cycles for all d. The number of iterations k takes before the kth iteration is equal to the k+nth iteration for any n for any k, i.e. total stopping times of the 3x+d function, were analyzed using a Java program to find stopping times for 1 to 9999 for d = 1,5,7,11,13,17 and plotted. The resulting graph demonstrated a logarithmic relationship between the number and the stopping time. An application of the 3x+d function for use in cryptography is demonstrated. 

Awards won at the 2010 ISEF
Renewable Scholarships to the IIT College of Science and Letters - Illinois Institute of Technology
________________________________________
2010 - MA035 
CRANK 0 PARTITIONS AND THE PARITY OF THE PARTITION FUNCTION
Kaavya Niveda Jayram
Sravani Academy, Morgan Hill, CA

The formal properties of integer partitions have been investigated for over 200 years by some of the brightest minds in mathematics such as Euler, Hardy, and Ramanujan, with surprising applications to modern physics and computer science. The partition function p(n) denotes the number of ways in which an integer n can be written as an (unordered) sum of other integers. Motivated by Ramanujan's investigations into the modular properties of p(n), this project aims to make progress on the parity problem of p(n) by means of deriving generating functions for cranks and ranks.<br><br>Berkovich and Garvan (2002) showed that there is always a bijection between the crank k and crank -k partitions of n for every k>0. Consequently, the parity problem for p(n) reduces to studying crank 0 partitions. <br><br>I obtained the following results:<br><br>(1) I derived a generating function for crank 0 partitions of n, which is similar to a generating function for p(n). I also obtained a general form for the crank k generating function.<br><br>(2) I described an involution on crank 0 partitions of n, whose fixed points are called invariant partitions. I then derived a generating function for crank 0 invariant partitions.<br><br>(3) Finally, I derived a generating function for rank 0 self-conjugate partitions.<br><br>The proof techniques are based on identifying and manipulating the key combinatorial objects underlying cranks and ranks, and avoid the analytic techniques inherent in previous methods. 

Awards won at the 2010 ISEF
Fourth Award of $500 - Mathematical Sciences - Presented by Intel
________________________________________
2004 - MA036 
A COMPARISON OF KNOT INVARIANTS
Abhinav Kapur
duPont Manual High School, Louisville KY, United States

The purpose of this project was to determine the strengths of the Alexander polynomial, Jones polynomial, A2 invariant, and the type 2 and type 3 Vassiliev invariants in distinguishing different knots from each other. For each knot of ten or less crossings (492 knots total), each of these invariants was calculated, using the KnotTheory Mathematica package developed by Dr. Bar-Natan at the University of Toronto. <br><br>Then, the number of times an invariant returned a non-unique value was counted. This indicated that an invariant returned the same value for two different knots, and was noted as a failure. The stronger an invariant, the fewer failures it had. A separate count was kept of failures just between mirror image pairs.<br><br>The Alexander polynomial failed 70 times out of a possible 492 between non-mirror image pairs, while it failed every single time between mirror image pairs. The 2nd and 3rd order Vassiliev pairs failed 375 times out of a possible 492, plus 80 times between mirror image pairs. The Jones polynomial was much stronger, failing only 31 times, plus 26 times between mirror image pairs. The A2 invariant was the strongest of all, failing only 12 times between non-mirror image knots, while failing 32 times between mirror image pairs. <br><br>The results show that the Alexander and Vassiliev invariants are unreliable, because if they return the same value for two knots, there is a high probability the knots are actually different. The Jones and A2 polynomials are quite reliable. If they return the same values for two knots, then the knots are likely to be equivalent.<br><br> 
________________________________________
2006 - MA036 
THE SOLUTION OF THE DIRICHLET PROBLEM WITH RATIONAL BOUNDARY DATA
Michael Anthony Viscardi
Josan Academy, San Diego, CA, USA

I study the Dirichlet problem for the Laplace operator in any simply connected bounded domain in the plane with boundary data that are rational holomorphic functions. I completely characterize, both geometrically and algebraically, those domains for which all solutions are rational in terms of a Riemann map and in terms of the Bergman kernel of the domain. In particular, I formulate and prove several theorems, and combine them into a single theorem which implies the following:<br><br>1. The solution to every Dirichlet problem with rational holomorphic data is rational if and only if a Riemann map is rational.<br><br>2. The solution to every Dirichlet problem with rational holomorphic data is rational if and only if the Bergman kernel K(z,w) is rational (as a function of z and w).<br><br>3. The solutions to the above Dirichlet problems are rational if and only if the solution is rational for a single, relatively ‘simple’ data function, namely, the function 1/(z-a), where a is a point in the domain.<br><br>4. The Bergman kernel K(z,w) is rational (as a function of z and w) if and only if K(z,a1) and K(z,a2) are rational functions of z, where a1 and a2 are any two distinct points in the domain.<br><br>Furthermore, I state and prove the first explicit formula for the solution of any such Dirichlet problem. My formula is written using only two functions: the data function and a Riemann map. Potential applications of my research include aerodynamics, airplane wing design, and modeling of solar prominences. 

Awards won at the 2006 ISEF
First Award of $1,000 - American Mathematical Society
Intel ISEF Best of Category Award of $5,000 for Top First Place Winner - Mathematics - Presented by Lucent Technologies
Award of $1,000 - Mu Alpha Theta, National High School and Two-Year College Mathematics Honor Society
Mathematica software package for all Intel Grand award first place winners. - Wolfram Research, Inc.
________________________________________
2006 - MA038 
UNDERSTANDING THE RELATIONSHIP BETWEEN PROTEIN ESSENTIALITY AND PROTEIN INTERACTION NETWORKS USING MATHEMATICS
Emily Zhao
Terre Haute South Vigo High School, Terre Haute, IN, USA

The rapid development of biotechnology generated enormous amounts of biological data at the system level. Understanding the relationship between different data sources is important and challenging in the current genomic era. The focus in this project was the relationship between protein interaction networks and protein essentiality using relatively simple mathematical and computational tools. Essential proteins are those when mutated or knocked out render the cell inviable. It was first shown, in 2001, that essential proteins were closely related to the network property and the essentiality of their neighbors, which made up the protein interaction network. Based on this literature, a new hypothesis was developed testing the fraction of essential protein neighbors. First the databases were sorted using a C program. The sorted data proved the new hypothesis which led to the next step. Four models were then studied, three of them developed based on the available databases, to determine the probability of a protein's essentiality with respect to the number of its neighbors and the number of essential neighbors. Finally, using statistical analysis, it was shown that the four models performed similarly and outperformed a majority rule that is commonly used in biological studies. The methods developed in this paper can be potentially useful for other studies related to networks. 
________________________________________
2003 - MA038 
AN INVESTIGATION OF FACTORS THAT INFLUENCE FLORIDA DROUGHT CONDITIONS-A TWO- YEAR STUDY
Charles M. Addicott
Welllington Community High School, Wellington , Fl, USA

This researcher undertook an investigation to determine whether or not a drought predictive model could be developed that would allow mitigating actions to be taken in advance of a drought. The year 2000 drought was used as a case study and the factors considered included evaporation, transpiration, consumption, discharge, rainfall and Lake Okeechobee level. <br><br> Monthly data from the years 1927 through 2002 were utilized to develop a regression model. Once lag effects were incorporated the regression had an r2 of 0.821, which is outstanding for such a complex physical model. The model can be utilized as either a predictive or analytical tool by running any number of “what if” scenarios. Examples include the use of the model to predict that the current consumption growth will result in lake levels similar to the year 2000 drought in approximately 50 years. This is not a long time to change a system of this size. In a second application, the 2000 drought was analyzed utilizing historical discharges as opposed to the extensive discharge actually experienced. The result was that the lake would have been approximately three feet higher during the year of the drought. This condition could have much improved the situation. Analytical work such as the examples described could lead to a significant improvement in the management of this critical water resource.<br><br> In addition to the above effort, this researcher utilized the business methodology of “Process Management” to determine if particular problem areas could be targeted for improvement. The results clearly demonstrated that the discharge process was not in statistical control. Since this is the single largest controllable parameter impacting lake level, improvement in this process would be very beneficial. 

Awards won at the 2003 ISEF
Fourth Award of $500 - Mathematics - Presented by Panasonic Consumer Electronics Company
________________________________________
2004 - MA039 
THE SNAKE LEMMA AND ITS APPLICATIONS TO GRAPH THEORY
Allison Paige Berke
Mira Loma High School, Sacramento CA, USA

The Snake Lemma is the primary tool used to relate homology groups of surfaces. These surfaces can also be represented graphically, which translates cycles on surfaces to cycles on graphs. Cycles on vertices, important in graph theory, are represented on manifold spaces within chain complexes, which in turn make up homology groups. The purpose of this project is to use the Snake Lemma to prove the Hamiltonicity of graphs. By proving the Snake Lemma to three degrees of accuracy, between first the traditional homology groups, and then to chain complexes and individual cycles making up these chains, the Snake Lemma was used to relate cycles on the graphs of manifold surfaces. An exact relating function was developed and defined, and the Snake Lemma was used to prove the existence of a Hamiltonian cycle on a graph whose representative manifold maps to a second manifold whose graph contains a Hamiltonian cycle. By proving exactness between the homology groups of two manifold spaces, a function was derived that proves exactness further between chain complexes, groups of cycles, and individual cycles. By using a simple manifold’s graph, containing a Hamiltonian cycle, the Hamiltonicity of at least one cycle on a graph representing a second manifold was proven using the previously defined function. The relation of graph cycles is verified by the relation, through the Snake Lemma, of homology groups of the representative manifolds. 

Awards won at the 2004 ISEF
Honorable Mention Award - American Mathematical Society
Scholarship award of $20,000 - Department of Homeland Security
Award of $5,000 - Intel Foundation Achievement Awards
Third Award of $1,000 - Mathematics - Presented by Panasonic Consumer Electronics Company
________________________________________
2005 - MA040 
TACTIC SABERMETRICS: HANDEDNESS IN BASEBALL
Gideon Edward Rosenberg Klionsky
Lincoln Park High School, Chicago, Illinois, United States of America

This experiment was performed in order to test the claim that batters have higher batting averages than the mean when facing pitchers of opposite handedness and lower batting averages than the mean when facing pitchers of the same handedness. Required also was testing whether switch-hitters are representative of the entire population of hitters when facing a given handedness of pitcher.<br><br>Data on the 2002, 2003, and 2004 Major League Baseball seasons were collected from ESPN.com and organized according to pitcher-handedness and batter-handedness for a total of six groups (left, right, and switch batters against left and right pitchers). To analyze the data, two-proportion Z-tests of significance were calculated for each of the combined sets of data, and p-values were found corresponding to likelihood of difference in data sets due to chance.<br><br>Switch-hitters' averages were shown to be insignificantly different than the combined averages of handed hitters, meaning that switch-hitter groups could be used as controls. Both matchups involving same-handed matchups (right-right and left-left) were found to be significantly lower (both p=0.0001) than the mean. Additionally, the left-handed batter vs. right-handed pitcher category was found to be significantly higher (p=0.0004) than the mean.<br><br>The fourth category, right-handed batters vs. left-handed pitchers, showed a statistically insignificant (p=0.17) difference from the mean. 
________________________________________
2003 - MA040 
NEED FOR SPEED: CAN AN ALGORITHM BE FORMULATED TO ILLUSTRATE THE EFFECTS OF ANGULAR ASCENT AND DESCENT ON MOMENTUM?
Garrett C. Kabinoff
Boynton Beach High School, Boynton Beach, Fl, USA

This year’s study was to create an algorithm to accurately measure the effect of angular ascent and descent on a car’s momentum as it travels around a banked oval racetrack, and enable the researcher to incorporate the theory of LLARS and illustrate conditional convergence through real world application. The outcomes will be forwarded to road design an engineer firms.<br><br> The researcher went to seven racetracks throughout the United States. Each track measured a mile and a half in length with an asphalt surface. The track and tire temperatures were carefully measured with minimal variance. The test cars were similar in all relevant dimensions and specifications. The controlled variables were the cars’ speeds and banking of each track. The car entered the curve at each racetrack at four different speeds. This was repeated six times for each speed and the results was recorded and graphed. <br><br> The researcher analyzed the data. As a result a single algorithm was generated to measure the speed from driving through the banked curve of an oval racetrack. This algorithm was a result of incorporating the theory of LLARS (longitude and latitude angle rate of slippage), which illustrates angular ascent and descent, with traditional rate formulas. <br><br> The researcher was then concluded by the insignificant margin of error (less than .03%), that the algorithm was accurate. In addition, the graph of the results clearly illustrates conditional convergence at 280 of banking. These findings will be instrumental in the planning of safety features for roads, highways and racetracks!<br><br> 
________________________________________
2006 - MA040 
MORPHING MENDEL'S GENETICS
Paige Michael Poole
Jefferson County International Baccalaureate School, Irondale, Alabama, USA

“Morphing Mendel’s Genetics” was an experiment aimed at comparing theoretical predictions to the actual outcomes of offspring produced from a monohybrid cross between a Corn Snake homozygous recessive (mm) for a certain gene and a Corn Snake heterozygous (+m) for the same gene. The theoretical outcomes were calculated using Mendel’s punnett square method, and actual outcomes were collected utilizing data collected from Corn Snake breeders.<br><br> In total, data from thirty-six clutches (one clutch is one laying of eggs by a snake) was analyzed in “Morphing Mendel’s Genetics.” The theoretical outcome for each clutch was 50% homozygous recessive (mm) and 50% heterozygous (+m). However, the mean percentage of homozygous recessive Corn Snake in a single clutch was calculated to be 55.8369%. Using the Chi Square Goodness of Fit test, it was determined that the null hypothesis should be accepted and the alternate hypothesis rejected, because my sample statistic did not fall within my critical region. This does not mean, however, that the alternate hypothesis can never be proven, only that with the data collected in this experiment it cannot be proven.<br><br> Therefore, there was not enough statistical difference between the actual and theoretical values to reject the validity of predicting genetic outcomes through the use of Punnett Squares. The data further came to illustrate Mendel’s idea that the more data an individual collects and the more tests he/she performs the closer he/she will get to his/her theoretical value.<br><br> 
________________________________________
2008 - MA041 
THE EFFICIENCY OF PRIME-TESTING ALGORITHMS
Casey Luo Fu
Torrey Pines High School, Encinitas, CA

The purpose of this project is to determine the most efficient prime-testing algorithm from four algorithms that I developed and a widely used algorithm. The algorithms include:<br><br>1. dividing by odd numbers less than or equal to the square root of the number being tested (this algorithm is widely used and is used here as the control) <br><br>2. applying divisibility rules<br><br>3. dividing by numbers less than or equal to the square root of the number tested that are not multiples of 5<br><br>4. dividing by odd numbers less than or equal to the square root of the number tested that are not multiples of 3 <br><br>5. dividing by odd numbers less than or equal to the square root of the number tested that are not multiples of 3 or 5<br><br> I hypothesized that Algorithm #2 would be the most efficient because the number being tested is decreased by a factor of ten.<br><br> The primality was tested using a Java program with the above five algorithms. A number between twelve and nineteen digits was entered, and the next five primes were printed with the time taken in milliseconds. Four different numbers were entered for each number length. <br><br> The results showed that Algorithm #5 was about 1.9 times as fast as the control, Algorithm #4 was about 1.5 times as fast, Algorithm #3 was about 1.2 times as fast, and Algorithm #2 was about 1.1 times as fast. My hypothesis wasn’t supported because Algorithm #5 was the most efficient. 
________________________________________
2006 - MA041 
PREDICTING IMPROPER FRACTIONAL BASE INTEGER LENGTH
John Wilson Dorminy
Sola Fide Homeschool, McDonough, GA, USA

Combinatorics and algebra have been used to find equations for the smallest integer with a certain length in an integral base. However, improper fractional bases have not been explored in much depth since their discovery in the 1930s. In this study, the researcher discovered an original formula for the smallest integer with a specific digit length in an improper fractional base.<br><br> The researcher wrote an original computer program to convert integers from base 10 to any improper fractional base. He used this program to find 100 combinations of length, improper fractional base, and the smallest integer with that length in that fractional base. The researcher used graphing, combinatorics, and difference equations to attempt to find a method to predict the smallest integer with a specific length in an improper fractional base.<br><br> The researcher then used number theory to evaluate the divisibility requirements of the numbers, and discovered a recursive formula for the smallest integer with a specific length in a given improper fractional base. He used this formula to find an equation for the number of integers in an improper fractional base with a certain length. The formula may also be useful in encryption with improper fractional bases. 
________________________________________
2003 - MA041 
UNIQUE COMBINATION OF LINEAR ALGEBRA, DIFFERENTIATION, & INTEGRATION TECHNIQUES TO ELUCIDATE THE IMPLICATIONS OF KIDNEY STONE CHARACTERISTICS
Delbert Andre' Green II
The Louisiana School for Math, Science, and the Art, Natchitoches, LA, USA 

The objective of this project was to find an adequate way to scrutinize the results found in the first phase of research in order to maximize the efficiency of the next phase of research. A novel sustem of combining techniques from linear algebra and differential and integral calculus was utilized. Linear algebra was used to form differentiable and integrable functions of the data that was collected in phase I. Differentiation and integration were used to analyze the functions and gest such information as incidence intensity, maximum/minimum occurrence values, exact average values, etc. From this analysis extremely pertinent information was obtained that will allow the optimization of the third phase of research. This project achieves the following: allows one to "look into" biological systems. 

Awards won at the 2003 ISEF
DuPont's Center for Collaborative Research and Education, Office of Education recognizes individual and team winners in the categories that best exemplify DuPont's business-related interests: Biology, Chemistry, Engineering/Physics, Earth/Environmental Science and/or Mathematics. Each project is recognized with a Primary Award of $1,000. - National Aeronautics and Space Administration
________________________________________
2010 - MA042 
THE SQUARE-ROOT-OF-TIME RULE AND THE US STOCK MARKET: UTILIZING THE JARQUE-BERA AND HURST ANALYSES TO ASSESS THE VALIDITY OF TIME-SCALING IN VOLATILITY
Shiqi Shan
Caddo Parish Magnet High School, Shreveport, LA

The square-root-of-time rule is used in finance to annualize historical volatilities (HV) and thus plays an integral role in risk analysis. The basis for this rule is Random Walk Theory, which says that a market’s returns follow normally distributed Geometric Brownian Motion (GBM). Research in other countries has resulted in evidence that Random Walk Theory may not apply to their markets and can result in skewed volatility calculations. The project’s aim was to utilize two descriptive analyses, the Jarque-Bera Normality Test (JB) and Hurst Analysis, to test whether the US market follows the tenets of Random Walk. The research is a continuation. Phase I involved calculating HV using the square-root-of-time rule to find a relationship between HV and optimal trailing stop loss. Thus, the continuation involves studying the premise of Phase I’s project and the basis on which the finance world assesses volatility. The hypothesis was that the JB Test would most often reject normal distribution of returns and the Hurst Analysis would most often produce an exponent greater than 0.5, indicating positive autocorrelation rather than GBM. The methods included computing periodic logarithmic returns of blue-chip stocks on Excel over a 10-year period and performing the tests with analysis software. The JB test showed a rejection of the null hypothesis for each stock, showing a lack of normal distribution over the time horizon. The Hurst Analysis demonstrated 27 periods of GBM (exponent=0.5), 16 periods of positive autocorrelation (>0.5), and 2 periods of negative autocorrelation (<0.5), indicating a strong likelihood of the return series to display behaviors outside of Random Walk. In conclusion, the research shows that it is possible that Random Walk does not apply to the US Market. 
________________________________________
2007 - MA042 
A WRINKLE IN PRIME: AN INVESTIGATION OF PRIME PROPERTIES IN A FOUR-DIMENSIONAL SIEVE OF ERATOSTHENES
Chelsea Nicole Oden
Monte Vista High School, Monte Vista, Colorado, United States

The Sieve of Eratosthenes is commonly used to generate prime numbers by eliminating composites within a given range of numbers. When completed, the method generates a two-dimensional graph of the distribution of primes. In last year's study, I generated cubic Sieves in various bases and discovered some new properties of primes influenced by the third dimension. This investigation extended the three-dimensional study of the Sieve of Eratosthenes by applying the Sieve to a fourth dimension. Upon entering the fourth dimension, a cube becomes a subcomponent of a hypercube, just as a square entering the third dimension becomes the face of a cube. Understanding the complexity of this four-dimensional geometric figure was a challenge, but I finally arrived at a suitable method for depicting a four-dimensional Sieve. A cube may be "unfolded" into a two-dimensional geometric figure comprised of six two-dimensional components; likewise, a hypercube may be broken down into a three-dimensional figure comprised of eight three-dimensional components. I adapted this "unfolded" hypercube as a three-dimensional depiction of a four-dimensional figure and used Microsoft Excel to create coordinates for hypercubic Sieves of various bases. After I had successfully generated the hypercubes, I used JAVA to write a computer program to analyze the prime densities of cubic and hypercubic Sieves and to calculate the prime densities of a hypercube's eight subcubes. Using systems of matrices and trigonometric functions, I also calculated angles of axis-plane intersection for "prime" planar ranges within the hypercubes and determined the equations of selected planes. 

Awards won at the 2007 ISEF
Award of three $1,000 U.S. Savings Bonds, a certificate of achievement and a gold medallion. - United States Army
Second Award of $1,500 - United States Air Force
________________________________________
2007 - MA043 
ERADICATING MALARIA: A SELFISH APPROACH
Jessica Tsu-Yun Su
Suncoast Community High School, Riviera Beach, Florida, USA

The maternal-effect selfish genetic element Medea is engineered such that if a mother has it and her child does not, the child dies. This property causes the selfish gene to be evolutionarily favored over the wildtype gene, even if it carries a significant fitness cost. Thus attaching a gene of interest (an anti-malarial gene, for instance) to such an element will cause the selfish element to drive the gene of interest into the population, provided the fitness cost associated with it is not unusually large. The central issue considered here was how long it takes for Medea to fixate under various conditions. By finding recurrence relations it has been shown that given an infinite population with random mating and an anti-malarial gene with 5% fitness cost, and provided half of the initial male ppoulation is homozygous for the selfish element, the selfish element will go to fixation in 11 mosquito generations; at 0 fitness cost it takes 10 generations. In practical terms, this means no mosquitoes will be capable of carrying malaria in five to six months (barring mutation). Similar results are obtained by using Monte Carlo simulations to analyze finite populations, and both results are in close agreement with experimental evidence. Also considered were the effects of mutation, migration between several populations, multiple selfish element, number of males homozygous for the selfish element initially seeded into the population, and to a brief extent, non-uniform seeding of flies into populations. 
________________________________________
2008 - MA043 
CLASSIFICATION OF POLYNOMIALS BASED ON THEIR MONODROMY ACTION
Daniel Craig Thompson
Arkansas School for Mathematics, Sciences, and the Arts, Hot Springs, AR

Algebraic Topology is an important field of modern mathematics, concerned with studying objects called topological spaces by imposing algebraic structures. The project looked at complex mappings from the Riemann sphere to itself in the form of polynomials and rational functions. Such mappings are ramified covers and induce a group action over the fiber over each point. The purpose of this study was to investigate the relationship between the polynomial or rational function and the monodromy action induced on fundamental group of the domain space.<br><br>Monodromy groups were classified using the branch cycle description. Additionally, the Riemann-Hurwitz Theorem was used to restrict which branch cycle descriptions were possible. The BCD was computed by taking a bouquet of loops around one's ramification points, and finding how each loop, or generator, permutes the fiber.<br><br>Branch cycle descriptions were first computed for individual cases, for specific polynomial degrees. This study concluded that one is able to simply relate the coefficients of the cover to the BCD for polynomials of degree less than five. Above this degree, however, it becomes difficult to even find the ramification points and one cannot find a general case. Similarly, rational functions quickly become too difficult to compute. Further research could prove a 1-1 correspondence between equivalence classes of branch cycle descriptions and equivalent covers. 
________________________________________
2006 - MA043 
COMPUTER-AIDED IDENTIFICATION OF CANCER FROM PHOTOMICROGRAPHS BY ENTROPY ANALYSIS
Lucia Mocz
Mililani High School, Mililani, Hawaii, United States

Computer algorithms were developed for identification of cancer based on the entropy of a pixel’s neighborhood in photomicrographs of clinical tissues. The photomicrographs were obtained from the Cancer Research Center of Hawaii. Breast, colon, and lung cancer were analyzed. Samples originated from tumor, adjacent, and normal tissues. A total of 576 specimens were used with and without immunohistochemical reactions. The information content of the corresponding photomicrographs was determined by entropy analysis. Entropy analysis appeared to be most successful in the identification of lung cancer with an overall accuracy of 71%. The accuracy for breast and colon cancer was 59% and 38%, respectively. The differences in accuracy could be attributed to the different micromorphology of the three cancer types. Immunohistochemical staining had little or no effect on the results and was not required for identification. To generate a visually rich representation of the photomicrographs to aid pathologists in the identification of cancer, the images were converted to entrograms using pseudo-color transformation via sinusoidal function, phase modulation, and frequency modulation. Frequency modulated entrograms provided more detail than the original photomicrographs and the other transforms. In conclusion, entropy analysis appeared to be competitive with the currently most widely used molecular-based clinical method for identifying lung cancer. The overall accuracy of the latter is also 71% and its sensitivity and specificity are 74% and 70%, respectively. The sensitivity and specificity of entropy analysis are 71% and 72%. The entropy formalism thus may provide a new set of useful tools for future research and diagnostics. 

Awards won at the 2006 ISEF
Award of $500 - American Association for Artificial Intelligence
Honorable Mention Award - American Statistical Association
________________________________________
2010 - MA044 
MEASURING THE DISTANCE BETWEEN TWO OBJECTS BY USING THE PARALLAX METHOD
Lucca Norton
Bradenton Preparatory Academy, Bradenton, FL

The purpose of the project was to calculate the distance between two objects by using the Parallax Method. The two objects were placed in front of a grid, and were looked at from two different angles. Using the Parallax equation, and the collected data, the distance between any particular objects could be determined within a range of 300 feet (91m). The average error out of a distance of 91m was 3.2%, out of 86m 6.8%, out of 82m 3.6%, out of 77m 5.5%, out of 73m 3.2%, out of 68m 4.8%, out of 64m 4.6%, out of 59m 2.4%, out of 55m 3.2% and out of 50m 2.6%. <br><br>The purpose of the experiment was to find out if the Parallax Method is an effective measurement tool for the Earth or just for astronomical issues. Once the experiment was completed and all the data was in, it was concluded that the hypothesis was correct.<br><br>As shown in the 4.1 percent error, the measurements were fairly accurately. Potential reasons for the percent error could have possibly been due to less than ideal weather conditions or even the telescope, which lacked focus and depth.<br><br>Overall this project showed that a method, which is usually utilized for space measurement, could be used to determine distances between two particular objects or by rearranging the equation even the distance to the objects. 
________________________________________
2007 - MA044 
TREE-REALIZABILITY OF A DISTANCE MATRIX
Arkajit Dey
The Harker School, San Jose, CA, USA

A fast algorithm for both testing the tree-realizability of a distance matrix and constructing the optimal realization is presented. The fastest existing algorithms are only designed to either test tree-realizability or construct a realization. The presented algorithm’s modifications over existing algorithms include a streamlined list of input parameters and maintaining a growing distance matrix. In addition to combining both testing and constructing algorithms, the improvements offer several other advantages: early halting upon detecting a non-tree-realizable distance matrix, a running time that is just as fast as existing construction algorithms on input that is realizable, and faster performance for input that is non-tree-realizable. The algorithm has a worst-case running time that is quadratic in the order of the input distance matrix and attains the subquadratic running times that are possible in existing algorithms that only construct the realization. For non-tree-realizable input, the algorithm needs to process, in expectation, at most three-quarters of the vertices to halt.<br><br> Tree-realizations can make naturally difficult problems such as the traveling salesman problem more easily solvable optimally over a tree-metric. In addition to its implications in the study of graph theoretic algorithms, the proposed algorithm also has applications in many varied disciplines: phylogenetic tree reconstruction in molecular and evolutionary biology, prediction of physical properties of alkanes in organic chemistry, inference of Internet network topology, evaluation of Internet performance, and the analysis of memory and mental association in psychology. 

Awards won at the 2007 ISEF
Second Award of $1,500 - Mathematical Sciences - Presented by Acatel-Lucent
________________________________________
2010 - MA045 
CONTINUED FRACTION CONVERGENTS AND LINEAR FRACTIONAL TRANSFORMATIONS
Evan Michael O'Dorney
Venture School, San Ramon, CA

There are many methods known for constructing rational approximations to the square root of an integer k, including the method of continued fractions and the iteration of the linear fractional transformation f(x) = (dx + k)/(x + d), where d is the integer part of the square root of k. This project finds necessary and sufficient conditions for the two methods to yield infinitely many of the same results.<br><br>The method of continued fractions always yields the closest possible approximations as well as all fractions p/q satisfying the Pell Diophantine equation p^2 - kq^2 = +/-1; the iterative method, while generally faster, often yields less accurate fractions. Earlier research showed that the two methods yield identical sequences of approximations if and only if 2d/(k - d^2) is an integer, leaving open the question of when they yield infinitely many common results.<br><br>Using a new viewpoint, combining elementary number theory with the group-theoretic and linear-algebraic properties of PGL_2(Z), we prove that the two sequences of approximations meet infinitely often if and only if 4d^2/(k - d^2) is an integer, and in this case the overlap includes all solutions to the Pell equation. Remarkably, for all other k, the finite overlap does not contain even a single such solution. An open question is whether this project can be generalized to other linear fractional transformations. 
________________________________________
2010 - MA046 
ORBIFOLDS AND MUSIC
Tulsi Manoj Patel
Arkansas School for Mathematics, Sciences, and the Arts, Hot Springs, AR

This project was conducted to observe the behaviors of non-Western music in orbifolds. Specifically, traditional Persian and Indian music were analyzed, and the focus was finding the efficiency of the voice leading for each piece and looking for any patterns which seemed as though they may be common within that tradition or simply to all pieces analyzed from that tradition. It was hypothesized that Persian music would have a more efficient voice leading than Indian music, as it used more equivalence classes, and that these two traditions would have similar patterns when mapped on an orbifold. Voice leading was measured by numerically representing the music, with equivalent notes represented as the same numbers, and finding the distance for each chord progression. Patterns were observed by finding how many chord progressions in each piece were symmetric or nearly symmetric under transposition or inversion on the orbifold. The orbifold was made by forming the quotient space (R/kZ)n, where k is the number of equivalence classes for the tradition being used. However, this space was then an n-torus which still contained ordered chords. To get an orbifold, all permutations of each point were identified to one representative point. After analysis, the Persian pieces did have more efficient voice leading than the Indian compositions. However, patterns common to both traditions were not found. The analysis of more traditions may help find the expected patterns. On a larger scale, this project may find patterns common to specific traditions in terms of voice leading and transformations. 
________________________________________
2006 - MA046 
CONSTRUCTING SU DOKU PUZZLES
Alexander L. Carter
Murphy High School, Mobile, Alabama, United States 

In this project, I analyzed Latin squares coming from parallel lines over finite fields to see if they held the Su Doku property. A finite field with nine elements is constructed by synthetically attaching a solution to the equation "the square of a variable is negative one" to the three element field. The points on parallel lines over this extension field can be used to male Latin squares. A Latin square is an array of numbers where each number appears exactly once in each row and column. A Su Doku Latin square has the property that in 3 by 3 blocks, every number from 1 to 9 appears only once. I found that for slopes three through eight that the resulting Latin squares had the Su Doku property while slopes zero, one and two did not. Computer animations from 6 dimensional space illustrated that Su Doku puzzles have many patterns that are not apparent from the puzzle point of view. Su Doku puzzles are not as nice looking in higher dimensions as line fields are.<br><br>By denoting the row and column indices in ternary digits, I was able to arrange the puzzle on a 3 by 3 by 3 by 3 hypercubeical array. In this way rows, columns, and Su Doku squares all appeared as 3 by 3 faces in this hypercube. Then relations among rows, columns, and squares could more easily be viewed. In this way some puzzles were less difficult to solve. 
________________________________________
2010 - MA047 
ON THE LOWER CENTRAL SERIES QUOTIENTS OF A GRADED ASSOCIATIVE ALGEBRA
Anirudha Balasubramanian
Saint Albans School, Washington DC, DC

Noncommutative ring theory, essential to quantum physics and noncommutative geometry, is not so well understood as its commutative counterpart. Relating its problems to the commutative realm is often a very useful technique, and the study of the lower central series has been a key component to this approach – both structurally and geometrically. While the lower central series quotients of the free associative algebra have been studied extensively since 2006, as have algebras where Spec(A_ab) is smooth, the degenerate case remains relatively unexplored; this project is novel in its investigation of an algebra associated to a non-smooth variety.<br><br> In “On the Lower Central Series Quotients of a Graded Associative Algebra,” we study the lower central series and its associated graded components for a noncommutative associative algebra (regarded as a Lie algebra) that is the quotient of the free algebra on n generators, A_n, by a single homogeneous relation P. Using combinatorial methods, we find an explicit basis and Hilbert-Poincaré series for the second associated graded component, B_2(A_n/<x^d+y^d>), for the cases n = 2, 3. We generalize these results to generic homogeneous P and build framework for the case of general n using algebraic techniques and previous algebro-geometric results. <br><br> We demonstrate analogy with the results of Etingof in the pseudosmooth case in terms of structure and geometry of abelianization and help build a unified theory of the lower central series for all associative algebras. <br><br> Finally, we conjecture complete results for the case of A=A_4/<P> and present MAGMA computations for other graded spaces. 

Awards won at the 2010 ISEF
Second Award of $500 - American Mathematical Society
________________________________________
2004 - MA047 
PROBABILISTIC RISK ASSESSMENT MODEL (OPTIMUM LAKE OKEECHOBEE LEVEL)
Charles Michael Addicott
Wellington Community High School, Wellington, Florida, United States

This project focused on the development of a Probabilistic Risk Assessment Model to help define the optimum operating level for Lake Okeechobee. The project is complicated by competing demands placed on the “lake operator” – SFWMD (South Florida Water Management District). Some stakeholders want to maintain a high lake level while others want the level to remain low. Further complicating the issue is the large natural variation in lake level.<br><br>A six-sigma process capability analysis indicates that, on average, the level is maintained within the specifications (13.5 ft to 15.5 ft). However, the DPMO (defects per million opportunities) is in excess of 500,000. In other words, about half the time the level is outside the specifications. This manifests itself as a problem for defining the optimum level because there is a known probability that the level will vary from the target a significant portion of the time. Although reducing the variation would make it easier to satisfy competing stakeholder demands, this researcher assumed that the level control process would remain unchanged.<br><br>This researcher modified the Kepner-Tregoe decision analysis tool such that now it can account for the probabilistic nature of level control. This yielded a model that can be used as a consensus-building tool that could allow for defining the optimum range of operating level. To ease the utilization of the model, a Microsoft Excel program was developed to “bury the complexities” inherent in the calculations which will allow stakeholders to focus on the things only they can address. 

Awards won at the 2004 ISEF
Honorable Mention Award - Bureau of Reclamation/U.S. Department of the Interior
________________________________________
2010 - MA047 
ON THE LOWER CENTRAL SERIES QUOTIENTS OF A GRADED ASSOCIATIVE ALGEBRA
Anirudha Balasubramanian
Saint Albans School, Washington DC, DC

Noncommutative ring theory, essential to quantum physics and noncommutative geometry, is not so well understood as its commutative counterpart. Relating its problems to the commutative realm is often a very useful technique, and the study of the lower central series has been a key component to this approach – both structurally and geometrically. While the lower central series quotients of the free associative algebra have been studied extensively since 2006, as have algebras where Spec(A_ab) is smooth, the degenerate case remains relatively unexplored; this project is novel in its investigation of an algebra associated to a non-smooth variety.<br><br> In “On the Lower Central Series Quotients of a Graded Associative Algebra,” we study the lower central series and its associated graded components for a noncommutative associative algebra (regarded as a Lie algebra) that is the quotient of the free algebra on n generators, A_n, by a single homogeneous relation P. Using combinatorial methods, we find an explicit basis and Hilbert-Poincaré series for the second associated graded component, B_2(A_n/<x^d+y^d>), for the cases n = 2, 3. We generalize these results to generic homogeneous P and build framework for the case of general n using algebraic techniques and previous algebro-geometric results. <br><br> We demonstrate analogy with the results of Etingof in the pseudosmooth case in terms of structure and geometry of abelianization and help build a unified theory of the lower central series for all associative algebras. <br><br> Finally, we conjecture complete results for the case of A=A_4/<P> and present MAGMA computations for other graded spaces. 

Awards won at the 2010 ISEF
Second Award of $1,500 - Mathematical Sciences - Presented by Intel
________________________________________
2004 - MA047 
PROBABILISTIC RISK ASSESSMENT MODEL (OPTIMUM LAKE OKEECHOBEE LEVEL)
Charles Michael Addicott
Wellington Community High School, Wellington, Florida, United States

This project focused on the development of a Probabilistic Risk Assessment Model to help define the optimum operating level for Lake Okeechobee. The project is complicated by competing demands placed on the “lake operator” – SFWMD (South Florida Water Management District). Some stakeholders want to maintain a high lake level while others want the level to remain low. Further complicating the issue is the large natural variation in lake level.<br><br>A six-sigma process capability analysis indicates that, on average, the level is maintained within the specifications (13.5 ft to 15.5 ft). However, the DPMO (defects per million opportunities) is in excess of 500,000. In other words, about half the time the level is outside the specifications. This manifests itself as a problem for defining the optimum level because there is a known probability that the level will vary from the target a significant portion of the time. Although reducing the variation would make it easier to satisfy competing stakeholder demands, this researcher assumed that the level control process would remain unchanged.<br><br>This researcher modified the Kepner-Tregoe decision analysis tool such that now it can account for the probabilistic nature of level control. This yielded a model that can be used as a consensus-building tool that could allow for defining the optimum range of operating level. To ease the utilization of the model, a Microsoft Excel program was developed to “bury the complexities” inherent in the calculations which will allow stakeholders to focus on the things only they can address. 

Awards won at the 2004 ISEF
Fourth Award of $500 - U.S. Coast Guard
________________________________________
2006 - MA048 
MY TRAVELING SALESMAN PROBLEM ALGORITHM: IS IT EFFICIENT AND FAST?
Lukas William Roots
El Cerrito High School, El Cerrito CA, USA

The purpose of this project is to develop and test an algorithm that provides reasonable and reliable results to Traveling Salesman Problems (TSPs), yet has a small enough maximum bound of required calculations that the algorithm can be applied to virtual any size TSP.<br><br> The lion's share of the time spent on this project was devoted to developing my algorithm, which seeks to gain more accurate results than existing "greedy" algorithms (such as the Repetitive Nearest Neighbor and Cheapest Link algorithms) by taking into account not the weight of each line, but the effect traveling down that line would have on what other lines can be traveled.<br><br> Once my algorithm was finished, I applied it to three real life TSPs of 11, 12, and 12 randomly selected vertices each, and compared the results with circuits found by the Respective Nearest Neighbor and Cheapest Link algorithms. Results indicate that my algorithm's maximum bound grows at the same rate as the Repetitive Nearest Neighbor algorithm (quadratically) yet is far more accurate than either the Repetitive Nearest Neighbor or Cheapest Link algorithms. In fact, my algorithm provided optimum results for all 3 test problems. All calculations were done by hand.<br><br> 
________________________________________
2010 - MA048 
NEW TRIANGULAR INEQUALITIES IN THE FORM OF S-A, S-B, AND S-C
Kang-Ying Liu
Saint Andrew's Priory School, Honolulu, HI

The title of my project is New Triangular Inequalities in the form of s-a, s-b, and s-c. I was inspired by the Heron formula. I wondered if it’s possible to invent some new triangular inequalities in this particular form. At first I converted different combination of a,b,c and s-a, s-b, s-c into the form of R, r, s. I subtract them to find out which one is greater. Then I put them together into a chain of inequalities. As a result, I created nine triangular inequalities based on this chain of inequality. 
________________________________________
2008 - MA050 
MONOPOLY: WANT TO WIN?
Rustem Bilyalov
Destrehan High School, Destrehan, LA

I did this project because I wanted to see if there is a scientific strategy in the game of Monopoly. First you need to figure out the income with no houses for every group of properties for 100 circles. Then you do that for every possible amount of houses and hotels. Next make a “cost-income” chart using the data. Use the chart to figure out the best property. My data is in the “cost-income” chart that is included in my report. The result of my experiment is this: the best group to buy is the cyan group of properties. That answer comes directly from my “cost-income” chart. The advantage the cyan group has is so miniscule that I understood that there is no real strategy in the game of Monopoly. 
________________________________________
2006 - MA051 
TOPOLOGY, TOPOLOGY
Ralph F. Jennings III
Southside High School, Atlanta, GA, USA

The Purpose of this project is to educate, and second to find and explore the possibility of a relation ship between Origami and Geometry.<br><br>My Hypothesis stated; how can one relate the angles one sees when folding Origami to Geometry or math in general. I realized that this would not be an adequate hypothesis to base my project on, so I made another, it reads; through deductive reasoning and solid proof I plan to find a relationship between origami and geometry. Whether it be in the angles or lines one sees while folding or if there is some type of math one could possibly relate it to. Thus my collective proofs shall be based on the second hypothesis even if I prove my first in doing so.<br><br>The Application of Origami can be taught with three principles. These principles, which one must become familiar with, are Kawasaki’s Theorem, Huzita’s Axioms and the study and application of Topology. This math can be implemented in the classroom to help learners who would otherwise think math was boring. It would also introduce them to new areas of math, which they might not otherwise be aware of. This helps students to broaden their minds. It describes a definite application to mathematics in general. <br><br>My Conclusion shows that there is clear and definite relationship between Mathematics and Origami. In doing this project I have found that Origami could be used as a teaching tool and I’ve introduced a new math that is not widely known.<br><br> 

Awards won at the 2006 ISEF
Scholarship Award of $12,500 per year, renewable annually - Florida Institute of Technology
________________________________________
2010 - MA055 
EFFECTS OF MOTILITY AND CONTACT INHIBITION ON TUMOR VIABILITY: A DISCRETE SIMULATION USING THE CELLULAR POTTS MODEL
Jonathan F. Li
Saint Margaret's Episcopal School, San Juan Capistrano, CA

The effect of cell migration on tumor growth was analyzed using the Cellular Potts Model. Motility, cell-to-cell adhesion, contact inhibition, and cell compressibility are incorporated into the model. We find that increased motility has a direct effect on the growth rate of a tumor. Cell lines with greater motility overcome the attractive forces of cell-to-cell adhesion and have more space to proliferate. In addition, contact inhibition amplifies the effect of motility. Strict contact inhibition penalizes clumped cells by halting their growth, giving motile cells a greater advantage. These results confirm that migration correlates with tumor viability. The model also shows that cells with less response to contact inhibition are more invasive. This raises questions on the effectiveness of some chemotherapy treatments, which may actually select for these more invasive cells. The results can be used to identify specific attributes that are associated with high growth rates and lead drug production to target those characteristics. 

Awards won at the 2010 ISEF
Honorable Mention Award - American Mathematical Society
Scholarship Award of $12,500 per year, renewable annually - Florida Institute of Technology
Renewable Scholarships to the IIT College of Science and Letters - Illinois Institute of Technology
________________________________________
2010 - MA056 
SOLVING DIFFERENTIAL EQUATIONS WITH AN ANALOG COMPUTER
Zach Miller
Cherry Creek High School, Greenwood Village, CO

The processing capability of an analog computer was observed in this study. An analog computer was designed and constructed to specifically model and solve a first order differential equation. The computer was constructed from a resistor and capacitor in series. The circuit modeled the first order differential equation, R(di/dt) + i/C = 0. The solution of this ODE was found to be VR = V0 e-t/RC. A constant dc voltage source was connected to the circuit that provided 5v. After the source was disconnected from the circuit, the discharge was measured from the capacitor in ten second intervals. These measurements were plotted to obtain the solution to the ODE that the circuit represents. A frequency generator was also connected to the circuit in order to observe the system’s response to square, triangle, and sine waveforms. 
________________________________________
2010 - MA058 
INVESTIGATING COMPLEXITY CLASS NP-COMPLETE
Laurence Paul Bordowitz
Macomb High School, Macomb, IL

The hardness of a problem can be determined using math. Very hard problems with specific properties are members of a "Complexity Class" called NP. Easy problems which can be solved quickly are in P. There's a special class called "NP-Complete," whose members can "turn into" every other NP problem. If an NP-Complete problem can be solved quickly, then the NP-Complete problems can turn into every other NP problem and those problems can be solved quickly. The problem asking whether this is possible is called "P=NP"<br><br> A possible solution is taking the problem "Minesweeper," proven to be NP-Complete, and making a "correct" program which solves it. The theoretical algorithm exhibited attempts to do this by exploiting Minesweeper's properties to make it a big system of linear equations whose answers are 1 or 0. The algorithm is proven to be incomplete. Future research will focus on proving P not equal to NP. 
________________________________________
2008 - MA301 
USING TECHNOLOGY TO SAVE SCHOOL'S MONEY
Ajay Shil Gupta, Huxley Rodriguez, 
Celebration High School, Celebration, FL

In the premature stages of our science fair project, we interviewed the Osceola County Print Workshop manager and Celebration High School personnel to gather data and information regarding printing costs. We did this as an endeavor to save school’s money through the use of technology. The information we received was strictly professional and was needed to drastically reduce school’s costs on printing. We gathered a substantial amount of information which inevitably led us to believe that it is possible to reduce the schools current financial expenditure. After compiling information and analyzing data, we were able to configure a system solution that involved using printing software. Through the use of the system solution, we were able to calculate and analyze costs versus current spending. This enables us to save Celebration High School $9540 annually. This also allows the Osceola County School District to save $90,060 annually. However, this is only statistics for the high schools in Osceola County. If we were able to include the middle schools, elementary schools, and charter schools the Osceola County School District would be able to save at least $567,000 annually. The money that the school will save annually can benefit not just the administration and teaching staff, but also the student body in general. The money can be utilized towards athletics, extracurricular activities, and in addition to providing students with the utmost highest quality of education as a form of hiring additional teachers. 
________________________________________
2003 - MA303 
MATHEMATICAL PATTERNS IN HEMINGWAY AND FAULKNER
Jason Paul Krenzelok, Hannah Clemons
Arkansas School for Math and Science, Hot Springs, AR, USA

The purpose of this project was to find mathematical patterns in Ernest Hemingway and William Faulkner’s short stories. From the researched methods of statistical evaluation and the assessment of each authors’ styles, it was hypothesized that a mathematical pattern for the usage of the comma and period could be found in their short stories “The Short, Happy Life of Francis Macomber” by Hemingway, and “Dry September” by Faulkner. <br><br> The data was collected and transferred into Microsoft Excel and Minitab. Dot Plots and statistical analyses were applied to each data set, and compared to smaller sample data sets that were collected from other short stories by each author. The effects of dialogue were also investigated by taking samples from dialogue and non-dialogue extremes. It was found that Faulkner did not have a consistent style, but Hemingway did. This can be attributed to short, choppy dialogue in contrast to his long complex sentences in Faulkner’s works. Hemingway, however, consistently uses short, choppy sentences. Therefore, another random sample from a different author, Kurt Vonnegut, was collected and compared to the Hemingway data. It was shown that their punctuation distributions were very similar, especially with the commas. <br><br> Some future implications for this project include the development of plagiarism detectors using punctuation analysis. Also, with the punctuation distribution patterns, it may be possible to develop a categorization system for different author styles. A clearer picture of an author’s stylistic technique could be obtained through a more in depth study that observes the different parts of speech, such as clauses, utilized in their works.<br><br> 
________________________________________
2007 - MA303 
OPTIMIZING NETWORK EFFICIENCY: ON THE RAMANUJANCY OF HEISENBERG GRAPHS OVER RINGS OF DEGREE 6 AND HIGHER
Vincent Huynh Dang, Yang Ge
Stanton College Preparatory School, Jacksonville, Florida, USA

The purpose of this paper is to explore the conditions for the Ramanujancy of Heisenberg Graphs in order to respresent networks of optimal efficiency. We say that a graph is Ramanujan if it is simple, undirected, and its largest non-trivial eigenvalue, L(f,g), of its adjacency matrix fits the condition that |L(f,g)|<=2(k-1)^(1/2). A We find L(f,g), by using exponential sums and prove a general theorem for the non-Ramanujancy of a graph based on said eigenvalue. By assuming minimality of the last term in the exponential sum, without loss of generality, we isolate and prove the conditions for the occurance of all lower dimensional eigenvalues that exceed the Ramanujan bound. Also, graphical analysis of the spectra of Ramanujan graphs was performed and it was shown that with a fixed number of elements in a symmetric set and as p (where p is a prime number) increases without bound, the distribution of the eigenvalues resemble the Sato-Tate semicircle distribution. Proving conditions for Ramanujancy and non-Ramanujancy is desirable as a Ramanujan graph can represent efficient communication networks in that they minimize cost of wiring and maintenance, but maximize the number of connections from one vertex to another. 

Awards won at the 2007 ISEF
Fourth Award of $500 - Team Projects - Presented by Science News
________________________________________
2007 - MA305 
DETERMINING THE POINT OF ENTROPY: BUILDING A LORENZIAN WATERWHEEL
Andrew Patrick Gumpper Flynn, William Leonard Cocke
Palmer High School, Colorado Springs, Colorado, United States

Edward Lorenz discovered dynamic systems that exhibit chaotic motion, and are extremely sensitive to initial conditions. The Lorenzian Waterwheel is a dynamic system modeled to portray the convection of fluids, meteorology, population growth, predator-prey relationships etc. The waterwheel, a disk angled off the horizon with attached cups, is governed by the complex equations known as the Lorenzian Attractor. <br><br> ~<br><br>The goal of this project was to modify the waterwheel’s angle and determine the importance of the angle on the motion of the wheel; the point of angular entropy. <br><br> ~<br><br>We built a waterwheel using four disks with diameters of .295 meters and .195 meters on a rotating axle. Cups were placed on screws connected to the disks and a hose connected to a pneumatic trough controlled water flow. The angle of the waterwheel was changed by raising or lowering one of the axle’s sides. Data was recorded over ten minute intervals using a Rotary Motion Sensory connected to DataStudio software. The analysis of the data determined that the waterwheel does not have a point of angular entropy: even when orthogonal the wheel displayed chaotic motion.<br><br> ~<br><br>Analysis of the angular position and velocity graphs displayed the chaotic nature of the waterwheel. The angle’s importance on the waterwheel’s motion is limited to the draining of the cups into one another caused by their relative vertical positions. When orthogonal, the cups drained into one another and produced the complex motion of a dynamic system, but the motion did not ostend to the Lorenzian Attractor equations.<br><br> 
________________________________________
2008 - MA306 
PAYCHECK PERFORMANCE
Nicole Renee Reeves, Ethan Merritt, 
Wray High School, Wray, CO

The purpose of this project was to create an equation to show how Major League Baseball should pay players based on performance. This was done by taking statistics in pitching, fielding, and batting from the 2007 season and deciding how much each stat is worth when paying a player. Statistics were chosen based upon the importance in winning that others and made them worth more than others. The formula worked to show that players should be paid based on their performance rather than how they are being paid now. The equation showed that some players are in fact being paid much less money than others when that player has performed better throughout the season. 
________________________________________
2003 - MA306 
FRACTALS, POWER-LAWS, AND THE WEIBULL DISTRIBUTION: MATHEMATICALLY MODELING CRUMPLED PAPER
Andrew Michael Leifer, David Guillaume Pothier, Raymund Chun-Hung To
Fairview High School, Boulder, Colorado, United States of America

We analyzed the distribution of the lengths of ridges or folds in crumpled paper to mathematically model the crumpling process. This model is a first step towards finding the energy absorption in crumpled membranes and other structures. When a single ridge buckles into smaller ridges, the resulting ridge-lengths display fractal behavior and follow a power-law distribution with an exponent of approximately -2.5. We assume that a crumpled ball results from the repetitive process of buckling many ridges. Mathematically this parallels rock fracturing, another process known to be based upon fractals. In the geophysical process, integrating over power-law fracturing processes at multiple scales yields a Weibull distribution of crushed rock sizes. We found that a Weibull distribution fits the measured values of ridge-lengths in a crumpled ball, confirming our assumption. Furthermore, both the total number of ridges and the mean ridge-lengths were found to have power-law relationships to the radius of the crumpled ball. All of these results demonstrate that paper crumpling is a fractal process that can be modeled. 

Awards won at the 2003 ISEF
First Place Award of $1,000 - American Mathematical Society
Third Award of $1,000 - Team Projects - Presented by Science News
________________________________________
2005 - MA311 
FISHY REFLECTION
Duc Thi Nguyen, Thoa Nguyen, Kristin Osaki
Oak Grove High School, San Jose, CA, United States

As a light source hits a series of stringed fish lines, a 3-D curve of light appears. The purpose of this project is to derive an equation for the curve and an explanation as to why this instance happens. <br><br>To derive the equation, data points were measured by placing a person in front of a piece of pexiglass directly attached to the frame stringed with fishing line. Said person, with her head clamped tightly in one position, would then dot light points that hit the fishing line. This process is repeated for several different angle measurements from above and below the eye level perspective. From eye level, the curve is a straight line on the X-axis. As the eye goes above, the curve bends downward and from below, the curve bends upward. Data (X,Y) points are measured by drawing an X and Y axis at the direct point where the light would hit.<br><br>From there, Z, the imaginary 3-D point of the curve, would be calculated by using knowledge of trigonometry, geometry, and algebra. When all Z’s are found for every (X,Y) data points, all Y’s, and Z’s will be inputted into Microsoft Excel where an equation will be formulated. Then using parametric equations from Calculus the equation can be derived.<br><br>The physical explanation of why this curve appears can be explained by disproving the two possibilities of diffraction and refraction with regards to light. Thus, the obvious option is reflection which can be proved by basic laws of reflection.<br><br><br> 
________________________________________
2008 - MA001 
THE DNA INEQUALITY IN NON-CONVEX REGIONS
Eric Kerner Larson
South Eugene High School, Eugene, OR

The DNA Inequality states that the average curvature of a curve inside of a given closed figure exceeds the average curvature of the figure. In the paper by Lagarias and Richardson (1997) that proved it for convex figures, the question arose if it could be possible to prove it for some non-convex region; the authors suggested L-Shaped regions. In this paper, we disprove the conjecture for L-Shapes and show that the DNA inequality holds for (another) non-convex region, in fact for a quadrilateral. 

Awards won at the 2008 ISEF
Third Award of $250 - American Mathematical Society
________________________________________
2010 - MA001 
ON THE LATTICE REPRESENTATIONS AND LINEAR EXTENSIONS OF SERIES-PARALLEL AND (M+N)-FREE POSETS
Martin Ayalde Camacho
Central High School, Saint Paul, MN

I analyzed three statistics of particular families of posets. First, I investigated the sets of comparable and incomparable elements of a poset and their relationship to the number of linear extensions of that poset. I then extended results of Tenner to particular families of posets, and specifically looked at the number of linear extensions of (3+1)-free posets. I also related my study of linear extensions to lattice representations of posets through connecting theorems and I ended with a theorem which proves an equivalence between posets lacking a particular subposet and lattice representations lacking a corresponding sublattice. My work has possible applications in complex event processing, cryptography, computational geometry, and computing theory. 

Awards won at the 2010 ISEF
First Award of $3,000 - Mathematical Sciences - Presented by Intel
________________________________________
2006 - MA002 
NOVEL METHOD OF COMPUTING JACOBI SYMBOLS FOR MERSENNE NUMBERS: ALLOWING FOR GENERATION OF S VALUES FOR THE LUCAS-LEHMER PRIMALITY TEST
Anarghya A Vardhana
Jesuit High School, Portland, OR, United States of America

If p is an odd integer greater than 1, and the sequence S1, S2,… is defined by <br><br>S1 = 4, Si + 1 = Si^2 – 2, then the number 2^p – 1 is a prime number if and only if one has <br><br>Sp – 1 is congruent to 0 (mod 2^p – 1). This is known as the Lucas-Lehmer primality test for Mersenne primes. The starting value 4, can be replaced by 10, 2/3, or in general, any number S for which the Jacobi symbols ((s – 2)/(2^p – 1)) and ((-s – 2)/(2^p – 1)) are equal to 1. The problem of constructing such universal starting values led to the consideration of the subgroup H = {r is an element of Q* : (r/(2^p – 1)) = 1 for almost all prime numbers p} of Q*. In finding these elements of H, the student came across a property of Jacobi symbols that could be used to compute the Jacobi symbol of a pair of Mersenne numbers. In this paper, the student proves this new property of Jacobi symbols for Mersenne numbers. The student further extended the investigation by using the newly found elements of H to propose the generation of new starting values for the Lucas-Lehmer primality test, and connected the study of Mersenne primes to perfect numbers. The student concludes with several open questions regarding H, and further exploration and applications with H for future research. <br><br> 

Awards won at the 2006 ISEF
Third Award of $250 - American Mathematical Society
Second Award of $1,500 - Mathematics - Presented by Lucent Technologies
________________________________________
2003 - MA003 
THE ORDER OF CHAOS
Jay Stewart Anderson
Paint Valley High School Bainbridge OH. USA

The purpose of this project was to evaluate the properties and find the order of chaotic systems. This was done with the use of the Sierpinski triangle, as well as representations of other fractal systems. The hypothesis was that chaotic systems do have a certain order and this order can be found in a graphical representation, a fractal.<br><br> The procedures of the project were as follows. Random integers valued from 1 to 6 were recorded on a triangle with the three vertices labeled with three different sets of two numbers from 1 to 6. For example, random integers from 1 to 6 were generated by (1) throwing a die and (2) using a home-made-device similar to a pinball machine with six collection boxes for a wooden ball. Computer programs were written to generate fractals using a larger number of computer generated random numbers. <br><br> The results showed that chaotic systems can create fractals, self-similar figures that appeared highly ordered. For a given recording method, the same figure was created with different sequences of random numbers. As a result, a conjecture can be drawn that all chaotic systems, regardless of complexity may be represented by fractals.<br><br> 

Awards won at the 2003 ISEF
Scholarship award of $5000 per year for four years - University of Akron
Tuition scholarship of $5,000 per year for 4 years for a total value of $20,000 - Indiana University
________________________________________
2010 - MA003 
GRIDS AND GREENERY: MATHEMATICAL MODELS OF INVASIVE SPECIES
Cathryn Margaret Manduca
Century High School, Rochester, MN

Mathematical models were used to determine the necessary weeding efficiency needed to prevent an invasive species from taking over. Sets of experiments were run in which invasive species had different amounts of competitive advantage over natives, were weeded at different efficiencies, and weeding started at different times after introduction of the invasives. Based on those models, an equation was determined that could find the minimum weeding efficiency needed to control and invasive with a given competitive advantage. These weeding efficiencies are surprisingly low, and could be useful for helping people determine how to use minimum energy in stopping an invasive species in a given area. 

Awards won at the 2010 ISEF
Fourth Award of $500 - Mathematical Sciences - Presented by Intel
________________________________________
2003 - MA003 
THE ORDER OF CHAOS
Jay Stewart Anderson
Paint Valley High School Bainbridge OH. USA

The purpose of this project was to evaluate the properties and find the order of chaotic systems. This was done with the use of the Sierpinski triangle, as well as representations of other fractal systems. The hypothesis was that chaotic systems do have a certain order and this order can be found in a graphical representation, a fractal.<br><br> The procedures of the project were as follows. Random integers valued from 1 to 6 were recorded on a triangle with the three vertices labeled with three different sets of two numbers from 1 to 6. For example, random integers from 1 to 6 were generated by (1) throwing a die and (2) using a home-made-device similar to a pinball machine with six collection boxes for a wooden ball. Computer programs were written to generate fractals using a larger number of computer generated random numbers. <br><br> The results showed that chaotic systems can create fractals, self-similar figures that appeared highly ordered. For a given recording method, the same figure was created with different sequences of random numbers. As a result, a conjecture can be drawn that all chaotic systems, regardless of complexity may be represented by fractals.<br><br> 

Awards won at the 2003 ISEF
First Awards of $3,000 - U.S. Air Force
________________________________________
2003 - MA004 
A THEORETICAL APPROACH TO THE OPTIMAL LENGTH OF A MUSICAL SCALE
Elena Udovina
Hathaway Brown School, Shaker Hts., OH, USA

It is a well-known problem in music theory that the 12-tone scale used in Western music today is not acoustically ideal; that is, the intervals that can be played in it only approximate acoustic consonance. Two natural questions, therefore, arise: why was 12 chosen for the length of the scale, and is it the ideal length? An order of consonance was assigned on the musical intervals, which was shown mathematically sound and consistent with previous studies. Octave equivalence was assumed for the purposes of a purely mathematical investigation. Only well-tempered scales with equal intervals between subsequent notes and a finite number of notes inside one octave were analyzed, the accepted approximation in mathematical studies in the field. Several definitions of a best scale were introduced that involved different techniques of evaluating the cumulative error of each scale relative to its length, and scales of length up to 150 tones were compared using C++ and Microsoft Excel under the definitions established. The first 100 consonant intervals (as defined by the order of consonance established) were used in the computer approximation; while the number is less than infinite, other studies usually consider no more than 10 intervals. The results suggest that in the era of computer-generated sound when large lengths of scales are becoming more viable, the scale of lengh 53 is the best for accompaniment of voice. 
________________________________________
2005 - MA004 
COMBINATIONAL ORDER
Robert Harold Lucy
Dove Science Academy, Tulsa, Oklahoma, USA.

These days, many problems with computer programs are related to ordering groups of numbers. If a computer was asked to find a combination of numbers that would work, it would try different combinations one by one. This "brute force” method could take a very long time if given a big combinational number. When dealing with computer algorithms, a mathematical function for indexing combinational numbers could force the computer to skip numbers that we know will not work. <br><br> The purpose of this project is to find a mathematical function that will index numbers. I have created a function that can index numbers which I have called: "Combinational Order". This method uses what I have called a "pattern number" to find the combinational number.<br><br> To start the mathematical equation I have created we will need to look at the first combinational number of the ordered group. Next, we need to know the order number of the combinational number we are trying to find. We then need to know how much each element changes. We then have to divide the order number by the number of times each element changes starting from the left element to the right element. After we have done this step we have found what I call the pattern number. Using the pattern number we can then figure out the combinational number.<br><br> The method I have created could save a lot of time and make combinational number problems simpler. 
________________________________________
2006 - MA004 
THE 196-ALGORITHM IN VARYING BASES WITH APPLICATIONS TO CRYPTOGRAPHY
Aaron Conner Barnett
Central High School, St. Joseph, MO, USA

Recent advances in algorithmic structure have made it more difficult to stay ahead of people trying to break into data. The reverse-sum, or 196-algorithm is a seemingly simple series that can inspire many questions. This algorithm simply takes a number and reverses the order of its digits and adds the result to the starting number. This process is repeated until a palindrome is created. The number 196 has never been found to create a palindrome, even when the number stretches millions of digits long. Using encryption technology based around this sequence, data could be better and security technology. Instead of relying on traditional base-10 computations, other bases of computation could be employed, leading to different advantages and disadvantages in regards to string value, frequency and length. These operations will be executed in C++ and Mathematica, taking advantage of the object-oriented nature in the storage of patterns and series. By reversing and creating strings of appropriate lengths, the implementation can be easily input into encryption algorithms. Instead of more traditional static constants in the RSA encryption scheme, the 196 algorithm creates a dynamic system of encryption that increases in security as message length increases. While leaving the modulus generation system the same, my modifications complicate the algorithm by requiring computation of the Euler phi function of numbers generated by the 196 algorithm. 
________________________________________
2010 - MA004 
EXPLAINING WIND FARM OUTPUT USING REGRESSION ANALYSIS
Kate Alexandra Geschwind
Mayo High School, Rochester, MN

Wind farms are a common source of energy today, but the amount of energy produced is intermittent and challenging to predict. The project goal was to create a practical mathematical model using readily-available explanatory variables that would accurately represent the amount of energy produced from a 67-turbine wind farm in southeastern Minnesota. Such a model would be useful to electrical grid operators in predicting this wind farm output. The analysis used recorded hourly data from December 2009, including time of day; temperature; dew point; relative humidity; wind speed; wind direction; cloud cover; turbine availability; and the amount of energy produced, measured in kilowatts. The regression capabilities of a standard spreadsheet software program were used to create different mathematical models using various combinations of explanatory variables. The regression models were analyzed using R squared and the residual error values to measure the accuracy of the model and identify the best-performing explanatory model. Using the same December 2009 data, this model was then compared to a simple wind turbine power curve model which might also be used to explain the output of the same wind farm. By comparing the standard deviation of residual error values as well as the sum of the absolute value of residual error values, the final regression model was shown to be more accurate at explaining electrical energy output than the power curve model. This regression model, combined with forecasts of the explanatory variables, would be a useful tool for predicting the hourly output of the wind farm. 

Awards won at the 2010 ISEF
Third Award of $250 - American Mathematical Society
HM - American Statistical Association
Renewable Scholarships to the IIT College of Science and Letters - Illinois Institute of Technology
First Award of $1,000 - Mu Alpha Theta, National High School and Two-Year College Mathematics Honor Society
________________________________________
2004 - MA005 
THE PERFECT DIAMOND FROM EULER'S METHOD
Tom Shi Feng
Mississippi School for Mathematics and Science, Columbus, MS, USA

There are many instances when fans of the visiting baseball team complain of the unfairness due to home field advantage. To provide one resolution to this, I set out to determine the fairness of some current Major League Baseball diamonds and new ones. Fairness is defined to be a state in which no player is favored over another player for a reason other than skill in the game of baseball. It is clear that a perfectly symmetrical field would be fair, but I hypothesize that other dimensions can also be fair. Accounting for air drag due to the geographic location and also field dimensions in my model, I derived a differential equation and solved it using Euler's method. Assuming that each player hits the ball at a certain initial velocity and angle of trajectory, I determined the maximized values to hit a homerun in each section of each field. As I ranked those fields using my own fairness ratings, I found that Edison Field was most fair and Metrodome was most unfair. Though the calculated degree of unfairness is quite small, the new ballparks are much fairer than their pre-existing fields. For example, the Twins will have a much fairer field in 2008 than their current Metrodome which has the lowest ranking among the fields I analyzed. 
________________________________________
2010 - MA005 
CREATING AND DEDUCING STRUCTURE USING DOMINATION NUMBERS IN PERMUTATION GRAPHS
Maxim Ilya Wimberley
Liberal Arts and Science Academy, Austin, TX

The domination number of a graph G, denoted by [gamma](G), is the minimal cardinality of a dominating set in G. For any permutation [alpha] of the vertex set of a graph G, the permutation graph P[alpha](G) with respect to [alpha] is the graph obtained from G and its copy G’ by joining u in V(G) and v in V(G’) if and only if v = [alpha(u). For any graph G and permutation [alpha], the current bounds on [gamma](P[alpha](G)) are [gamma](G) [less than or equal to] [gamma](P[alpha](G)) [less than or equal to] 2[gamma](G) and there exists a disconnected graph G and a permutation [alpha] that achieve [gamma](G) = a and [gamma](P[alpha](G)) = b for any two positive integers a [less than or equal to] b [less than or equal to] 2a. This research further inquires about the bounds on the domination number of a permutation graph. A connected graph G and permutation [alpha] are constructed that achieve [gamma](G) = a and [gamma](P[alpha](G)) = b for any two positive integers a [less than or equal to] b [less than or equal to] 2a. Different sufficient conditions are provided for a graph G to have [gamma](P[alpha](G)) = 2[gamma](G) for all permutations [alpha]. The structures of graphs G with [gamma](Pid(G)) = [gamma](G) are also investigated.<br><br>The results about achieving the lower bound on [gamma](P[alpha](G)) suggest ways to structure a network in which each node affects its immediate neighbors so as to minimize the number of nodes required to affect all nodes. Similarly, the results regarding achieving the upper bound on [gamma](P[alpha](G)) suggest ways to maximize such a quantity. 

Awards won at the 2010 ISEF
Second Award of $1,500 - Mathematical Sciences - Presented by Intel
________________________________________
2004 - MA006 
INVESTING IN STOCKS BY RANDOM, TECHNICAL, AND FUNDAMENTAL SELECTION
Alyssa Anne Grant
Wilsonville High School, Wilsonville, OR, United States of America

Is using random selection a better method than using the fundamental and technical methods when investing in stocks? There are three types of selection methods for stocks: random, using random selection; technical, analyzing past and present data; and fundamental, using current stock reports. The purpose of this experiment is to test and compare the three types of selection methods. Random selection is hypothesized to be the better method than the technical and fundamental methods. A sample size of thirty different stocks (ten from each method) and the population contains of all stocks in the New York Stock Exchange and the NASDAQ. Randomly selected stocks were blindly picked pointing to each of the ten stocks from a hanging poster consisting of the population stocks. Technically selected stocks were from the Value Line Investment Survey where the ten highest rated stocks over the past ten years were chosen. The ten fundamentally selected stocks were picked using a computer program to determine which stocks are currently doing the best. Each experimentation week, all thirty stocks’ closing prices were recorded on Microsoft Excel Spreadsheet. The results are analyzed by the overall percentage increase/decrease in each selection method. The random selection is most successful because it had an overall percentage increase of 16.91% with one negative-gain stock. The fundamental selection is 4.89% with three negative-gain stocks, and the technical selection is 2.41% increase with negative-gain stocks. There is sufficient evidence to support the hypothesis that randomly selecting stocks is the better method. 

Awards won at the 2004 ISEF
Scholarship award of $5,000 per year for 4 years - Lewis & Clark College
________________________________________
2006 - MA007 
EGYPTIAN FRACTIONS
Gardenia Arredondo
Dove Science Academy, Oklahoma City, OK, USA

In this project we studied Egyptian fractions. In other words, how to convert a regular fraction into an Egyptian representation where the fraction is expressed as a sum of unit fractions. We examined the existing methods that had been found in the past and presented one of them in our study: The Greedy method found by Fibonacci. Moreover, we came up with our own method of putting a fraction in Egyptian form and named this method “Gardy”. We classified fractions according to their numerators and tried to figure out a pattern on how the Egyptian representation turns out. When we apply each method for the conversion, we genuinely conjectured that our method churns better Egyptian forms in some certain cases. We also showed that there might be more than one Egyptian form of the same fraction, meaning that the Egyptian fraction cannot be unique for a particular fraction. 
________________________________________
2010 - MA007 
FINDING A PATTERN IN DISEASES
Mitchell Craig Brown
Grants High School, Grants, NM

The purpose of this project was to determine if a biological pattern could be found concerning Tuberculosis (TB) that allowed for an accurate prediction to be made concerning where and when the next outbreak of that particular disease will be within the State of New Mexico. This prediction was made based on three regressions superimposed on a scatter plot of the data: the number of cases of TB from 2000 to 2008. The three regressions used were linear, quadratic, and sinusoidal. Linear was used on both the population and case number graphs to see if the respective variables were increasing or decreasing; quadratic on the case number graph to see where the low point in the data was; and sinusoidal on the case number graph to see if the data fit a sine wave, meaning that the data followed a periodic pattern. It was hypothesized that a pattern would be found that would allow one to know when and where new cases of TB will occur. Results show that a pattern was found that may be able to predict future cases of TB: for New Mexico overall, the number of cases from 2000 to 2008 nearly fit perfectly on a sinusoidal regression, which appears to repeat about once every five years. After receiving the 2009 data, this pattern seems to hold true within a plus or minus four margin of error (the sine wave predicted 52 cases, while in actuality there were 48—an eight percent error). Within each individual county, the margin of error is about plus or minus two cases. When the U.S. data was graphed, there was not a very precise sinusoidal regression, but this could be because there are more people, leading to a longer period between an increase and decrease in the case number, and the data available does not go back far enough to test this. 
________________________________________
2009 - MA007 
SEQUENCES OF REDUCIBLE 0,1 POLYNOMIALS
Martin Augustine Camacho
Central High School, St Paul, MN

I considered a class of naturally occurring sequences of 0,1 polynomials (polynomials whose coefficients are either 0 or 1) and explored the properties of such sequences, including whether any given sequence is finite or infinite, patterns in the terms of sequences, and the lengths of such sequences. The construction is as follows: starting with 1 plus a power of x, each element of the sequence is defined as the previous element plus the minimal power of x such that the result is reducible. I proved results regarding sequences beginning with specific polynomials, the exponents appearing in the terms of these sequences, and the length of these sequences. I also presented applications of the sequences to the irreducibility of small-length polynomials. Finally, I derived an algorithm for determining whether a given sequence is finite or infinite. My proofs utilized tools from cyclotomic theory and the study of reciprocal and non-reciprocal polynomials.<br><br> These sequences construct infinite classes of both reducible and irreducible polynomials. In addition, upon evaluation at a positive integer m, these sequences always yield base-m representations for composite numbers with a presumably small number of large prime factors. These prime factors could be used in number theoretical cryptosystems such as Rivest-Shamir-Adleman.<br><br> Finally, my work could also be extended to a general theorem that would effectively classify an infinite group of polynomials as reducible or irreducible with small computational time. 

Awards won at the 2009 ISEF
Honorable Mention Award - American Mathematical Society
Third Award of $1,000 - Mathematical Sciences - Presented by Intel
________________________________________
2005 - MA008 
A DAY AT THE CARNIVAL
Nathaniel Morck Beaver
Perham High School, Perham, Minnesota, United States

This experiment was done to determine how well games and their expected returns match up with their actual return in a real world environment, and to see which one attracted the most people. The hypothesis is that the results should be within the standard deviation and that the less number of trials, the less accurate the results, and also that people will go to games that they most often won and were the most highly fluctuating. The games were tested and results recorded at a fundraising local carnival. The results, when a large amount had been recorded, agreed closely with the theoretical expectations. People seemed to enjoy moderately complex games in which they felt their choices influenced the outcome, and in which the payout rules were simple. Thus, the hypothesis was mostly correct. 
________________________________________
2003 - MA008 
CHAOS IN ELASTIC AND PASS-THROUGH CASES FOR THE THREE-PARTICLE ONE-DIMENSIONAL SYSTEM
Sayed Ashique Zaman
P.L. Dunbar High School, Fort Worth, Texas, USA

In the research, the chaos demonstrated by the motion of three particles of unequal mass that are allowed to converge based on their gravitational attractions to each other in a one dimensional system was examined. Two cases for this system were particularly studied: the elastic case, in which, upon collision, the particles bounce off of each other, and the pass-through case. The research was done using a computer program that simulated this movement and displayed the chaos of specified systems using Poincare surfaces of section. <br><br> The elastic case was examined using advanced techniques that allowed for the finding of "ranges of stability". Also several new specific mass arrangements were found that lead to the attainment of complete chaos. The pass-through case, which had never been previously studied, was found to exhibit much stability, although counterparts to the elastic case’s ranges “ranges of chaos”, were found.<br><br> 
________________________________________
2005 - MA010 
FINDING VARIETIES IN NONLINEAR SYSTEMS USING ALGEBRAIC-GEOMETRY AND MAPLE
Matthew Ryan Tierney
Westview High School, Portland, Oregon, United States

Systems of nonlinear equations are investigated whose coefficients depend on several parameters. These equations arise in the context of mass action kinetics in chemistry. The goal of my research has been to analytically find equilibrium solutions for large chemical mechanisms. <br><br> To simplify computations without losing the applicability of my results, I have used reaction network analysis in order to reduce the size of complete chemical mechanisms. From the reduced mechanism, I derived the ordinary differential equations formed by chemistry rate law equations. Setting these equations equal to zero gave me the nonlinear system from which I could solve for the equilibrium concentrations of the mechanism’s species. Applying Groebner basis proved the absence of multiple steady states (equilibria) in one particular mechanism. I am also investigating a more complicated mechanism that accounts for methane. The importance of understanding this second mechanism involves global warming. <br><br> I used numerical analysis to uncover various phenomena in both mechanisms; including, chaos and the “chaotic blue sky catastrophe.” More importantly, hysteresis (bistability) has been demonstrated numerically within the methane mechanism. Since methane contributes twenty times more to the greenhouse effect than any other greenhouse gas, a realistic hysteresis region (bistability) for methane has several implications for (atmospheric) government policy. Algebraic-geometry techniques, including Wu’s Method of characteristic sets and various algorithms, are currently being employed to reveal more information about the (affine) variety of this nonlinear system.<br><br> 

Awards won at the 2005 ISEF
Third Award of $250 - American Mathematical Society
________________________________________
2003 - MA011 
THE THEORY OF INFINITY SHIFT AND INTERDEPENDENT SYSTEMS
Brian Matthew Lauber, Chaminade-Julienne Catholic High School
Dayton Ohio, The United States of America

This project was aimed to obtain logical explanations for the observations made in four geometric problems. While it was initially intended to yield answers with respect to Euclidean geometry, it was soon realized solutions could only be found in an expanded mathematical system.<br><br> While each of the four problems involved a different geometric construction, all of the problems shared a single characteristic: they each in some way contained infinite values. The first problem sought to determine whether or not a point of intersection could exist if two lines were an infinite distance apart. The second problem analyzed the relationship between a circle and a constant chord length as the radius of the circle was extended for an infinite distance. The third problem was intended to explain why all Euclidean parallel lines could be proved to be coinciding. Lastly, the fourth problem sought to justify the existence of a point in an infinitely collapsed space. The general hypothesis was that a unified solution could be obtained for all of the aforementioned problems by determining a relationship between the observable forms of infinity.<br><br> Current results suggest that the mathematical system is ultimately composed of two interdependent systems working in unison to form a single, complete system. The first of these systems is the proportional system, which pertains solely to the placement of points or values and their relative relationship to other points of values in an isolated space. The second system is the infinity shift, which establishes a relationship between three observable forms of infinity that ultimately suggests the existence of an infinite number of unobservable forms of infinity. Because either system may be freely modified without influencing the other system, solutions to the four problems can be attained by altering the infinity shift system and observing the proportional system. 
________________________________________
2007 - MA011 
MATHEMATICAL DETERMINISM IN THE CREATIVE PROCESS OF MUSICAL COMPOSITION
Prachi Janardan Pai
School of Science and Technology, Beaverton, Oregon

The purpose of this project was to investigate whether the process of musical composition, which involves aspects of artistic creativity, contains the elements of mathematical determinism. Musical scores from different composers from different time periods were modeled through an Iterative Function System. The correlation dimension values of these scores were then identified. Interestingly, all the real music scores showed identical signatures of mathematical determinism that were related to non-linear statistical fractals. 

Awards won at the 2007 ISEF
Fourth Award of $500 - Mathematical Sciences - Presented by Acatel-Lucent
________________________________________
2007 - MA012 
THE STRING TOPOLOGY BV ALGEBRA, HOCHSCHILD COHOMOLOGY AND THE GOLDMAN BRACKET ON SURFACES
Dmitry Vaintrob
South Eugene High School, Eugene, OR, USA

This project provides an algebraic description of the String Topology Batalin-Vilkovisky algebra for a large class of manifolds. Such a description previously existed only for spheres and projective spaces.<br><br>The homology H(LX) of the space of free loops of a closed oriented smooth manifold X has a rich algebraic structure called string topology, discovered by Chas and Sullivan in 1999. In particular, H(LX) is a Batalin-Vilkovisky (BV) algebra. However this structure is hard to compute in algebraic terms.<br><br>This project studies string topology in the case when the manifold X is aspherical. In this case the Hochschild cohomology Gerstenhaber algebra HH(A) of the group algebra A of the fundamental group of X has a BV structure. My main result is a theorem establishing a natural isomorphism between the Hochschild cohomology BV algebra HH(A) and the string topology BV algebra H(LX). In particular, for a closed oriented surface X of hyperbolic type this gives a complete description of the BV algebra operations on H(LX) and HH(A) terms of the Goldman bracket of loops on X. <br><br>There are several conjectures connecting the string topology BV algebra with algebraic structures on the Hochschild cohomology of algebras related to the manifold X. My theorem is the first such hypothesis that has been proven. The proof is based on a combination of topological and algebraic constructions allowing to compute and compare multiplications and BV operators on both H(LX) and HH(A). 

Awards won at the 2007 ISEF
First Award of $1,000 - American Mathematical Society
Second Award of $500 U.S. Savings Bond - Ashtavadhani Vidwan Ambati Subbaraya Chetty (AVASC) Foundation
A Scholarship of $50,000. - Intel Foundation Young Scientist Award
Intel ISEF Best of Category Award of $5,000 for Top First Place Winner - Mathematical Sciences - Presented by Acatel-Lucent
The SIYSS is a multi-disciplinary seminar highlighting some of the most remarkable achievements by young scientists from around the world. The students have the opportunity to visit scientific institutes, attend the Nobel lectures and press conferences, learn more about Sweden and experience the extravagance of the Nobel festivities. - Seaborg SIYSS Award
________________________________________
2010 - MA012 
MARKOV MATRICES APPLIED: TESTING THE ACCURACY OF THE MARKOV CHAIN USING A POPULAR BOARD GAME
Wyatt Vann Dunham
Grove High School, Grove, OK

This project was to test the accuracy of the Markov Chain using a board game of chance. To test this project, each of the 8 tokens rolled the dice 30 times each night over a four week period. While testing the hypothesis, a tally chart, paper of where each token landed, and a frequency table was used for data every week. <br><br> The Markov Chain correctly predicted 2 out of 5 spaces to be in the top 5. For the top 10, the Markov Chain correctly predicted 6 out of 10 to be in the top 10. The Markov Chain correctly predicted 18 out of 20 spaces to be in the top 20. There were a few spaces that made the top 5, 10, or 20 and were not predicted to be in the top rankings. Community Chest #2 and Marvin Gardens made the top 20 each time. Atlantic Avenue and Pennsylvania Railroad made the top 10 each time. For the top 5, the only space to rank in the top 5 each time was Pennsylvania Railroad, which was the only space to make two separate top rankings.<br><br> It was observed that the Markov Chain is better at predicting larger increments such as color groups in a game of chance rather than smaller increments like spaces in the game of chance. 

Awards won at the 2010 ISEF
Honorable mention - Society of Exploration Geophysicists
________________________________________
2007 - MA012 
THE STRING TOPOLOGY BV ALGEBRA, HOCHSCHILD COHOMOLOGY AND THE GOLDMAN BRACKET ON SURFACES
Dmitry Vaintrob
South Eugene High School, Eugene, OR, USA

This project provides an algebraic description of the String Topology Batalin-Vilkovisky algebra for a large class of manifolds. Such a description previously existed only for spheres and projective spaces.<br><br>The homology H(LX) of the space of free loops of a closed oriented smooth manifold X has a rich algebraic structure called string topology, discovered by Chas and Sullivan in 1999. In particular, H(LX) is a Batalin-Vilkovisky (BV) algebra. However this structure is hard to compute in algebraic terms.<br><br>This project studies string topology in the case when the manifold X is aspherical. In this case the Hochschild cohomology Gerstenhaber algebra HH(A) of the group algebra A of the fundamental group of X has a BV structure. My main result is a theorem establishing a natural isomorphism between the Hochschild cohomology BV algebra HH(A) and the string topology BV algebra H(LX). In particular, for a closed oriented surface X of hyperbolic type this gives a complete description of the BV algebra operations on H(LX) and HH(A) terms of the Goldman bracket of loops on X. <br><br>There are several conjectures connecting the string topology BV algebra with algebraic structures on the Hochschild cohomology of algebras related to the manifold X. My theorem is the first such hypothesis that has been proven. The proof is based on a combination of topological and algebraic constructions allowing to compute and compare multiplications and BV operators on both H(LX) and HH(A). 

Awards won at the 2007 ISEF
Tuition Scholarship Award in the amount of $8,000 - Office of Naval Research on behalf of the United States Navy and Marine Corps.
UTC Stock with an approximate value of $2000. - United Technologies Corporation
________________________________________
2007 - MA013 
THE STOCK MARKET, PHI AND THE FIBONACCI SERIES
Lisa Renee Biegler
Timber Lake High School, Timber Lake, South Dakota, United States

My study was intended to discover whether the Fibonacci series determines the patterns of the stock market. I first gathered the volume, opening, closing, high, and low prices for ten different stocks for a two year period. I calculated ratios of daily values to seek any correlation to phi. I used the PhiMatrix program to calculate the pattern of high and low points in the stock market and their correlation to the phi ratio. From my research, I observed that when ratios were calculated from the daily values, the resulting numbers only existed between 1.04 and .96. When charted, these values all averaged close to one, indicating that the sock market’s daily price does not include phi, but barely changes its value a significant amount. When the PhiMatrix program was applied to charts, it strongly indicated points on major waves. The data generated indicated that turning points in the stock market coincide with phi ratios. If either a high or low price is known crucial buying and selling points may be predicted. 
________________________________________
2003 - MA013 
ACOUSTIC SIGNATURE RECOGNITION THROUGH FRACTAL ANALYSIS
Allison Lindsey Schmidt
Tullahoma High School, Tullahoma, Tennessee, United States 

The purpose of this study was to investigate a relationship between categories of sounds, the fractal dimensions of the sound patterns, and the fractal dimensions of the frequency spectra. First, sounds were recorded as WAVE files. Each sound, recorded from an individual event, fit into one of several categories, such as running water and car engines. The recordings were then processed by MatLab using an m-file that changes a sound to a set of discrete amplitude values and applying a Fourier transform. The acoustic signature and frequency spectrum were plotted for each file. The box-counting technique was utilized with a magnified view of the plot to determine the fractal dimension of each graph. A comparison of the fractal dimensions reveals correlations, among which are the tendency for sounds produced by similar sources to have similar fractal dimensions, a proportionality between the fractal dimension of a sound pattern and the fractal dimension of its corresponding Fourier transform, and a tendency for sounds of lower frequencies to display higher fractal dimensions.<br><br> 

Awards won at the 2003 ISEF
Scholarship award of $8000 for original research in an important Naval-relevant scientific area. - U.S. Navy & Marine Corps
________________________________________
2007 - MA014 
THE MATHEMATICAL MODELING OF THE DEVELOPMENT AND PROGRESSION OF TORNADO VORTICES
Mathew Kyle Tucker
Mankato West High School, Mankato, Mn, United States

In today’s pop media, we tend to put more emphasis on tornadoes, when talking about weather, than we do other weather events. But what is it that drives these amazing things? The intent of this project was to show the mathematics behind tornado vortices. Specifically, the goal of the project was to explore tangent and rotational velocities, the wind velocity vector fields, as well as the pressure applied to and by the tornado and its vortices.<br><br> This was accomplished by finding the velocity in the z-axis, and translating that information into time, and with it solving for the period of one full rotation on the xy plane. In order to do this several videos were collected and with a video analysis program (VideoPoint) the coefficients are able to be determined. From there it is possible to use inductive reasoning to find the function of position with respect to time and radius. Then it is possible to use the partial derivative of the position function, with respect to time or time and radius, to attain the functions for both tangent and rotational velocity.<br><br>In all, this project has shown that yes, it is very possible to equate an equation to the behaviors and structure of a tornado and its vortices. This is a fact of some importance, because it can aid in the on going search for a “better” tornado forecast system.<br><br> 
________________________________________
2003 - MA014 
COMPUTATION OF QUANDLE COCYCLE KNOT INVARIANTS
Anatoly Preygel, Montgomery Blair High School
51 University Blvd. East, Silver Spring, MD 20901

The mathematical knot theory deals with the classification of all possible knots, i.e. closed curves in 3-space.<br><br>This work investigates some aspects of the recently introduced quandle cocycle invariants. An algorithm for calculating colorings of knots, leading to a new invariant, the degree, is proposed and used to compute many such quandle invariants. It is proven that the degree of a knot is equal to the bridge number. Periodicity properties of these invariants are established. 

Awards won at the 2003 ISEF
Third Place Awards of $250 - American Mathematical Society
Third Award of $1,000 - Mathematics - Presented by Panasonic Consumer Electronics Company
________________________________________
2009 - MA015 
PATTERNS IN TRANSITIVE RELATIONS
Dalton James Allan
Saginaw Arts and Sciences Academy, Saginaw, MI

The question of how many transitive relations there are on a set with n elements is currently an open question in the field of mathematics. A transitive relation is a relation on a set such that, if (a, b) and (b, c) are in the relation, then (a, c) is also in the relation. The number of transitive relations on a set grows extremely rapidly as the number of elements increases. The number of transitive relations has only been counted for sets with sixteen or fewer elements. No formula for the number of transitive relations on a set is known, so it is as yet impossible to predict the number of transitive relations on a set. Answering this question is beyond the scope of the present project. However, this project does make some headway in identifying some specific types of transitive relations in an attempt to count some of the transitive relations on a set.<br><br>In this study, patterns in transitive relations on sets with a small number of elements were analyzed and attempts were made to find generalizations from these patterns. These small sets were chosen because they are large enough to insights into the problem, yet small enough to be manageable. In examining transitive relations on these sets, several patterns emerged. Some theorems and conjectures were formulated that provide a starting point for answering the larger question about the number of transitive relations on a set with n elements. It is conjectured that any transitive relation with sufficiently many elements is almost reflexive (has not more than one non-reflexive relationship), where “sufficiently many” is a function of the cardinality of the set the relation is on. If true, this conjecture reduces the number of relations that must be considered when counting the transitive relations on a set. 
________________________________________
2005 - MA015 
ON UNIVERSALITY PROPERTIES OF POSITIVE-DEFINITE INTEGRAL QUADRATIC FORMS
Scott Duke Kominers
Walt Whitman High School, Bethesda MD, USA

We present several results in the representation theory of quadratic forms, the study of the properties of linear transformations relating degree-two polynomials. This field arose as a generalization of Lagrange's Four Squares Theorem, which states that all integers can be represented as a sum of four squares and has led to the concept of representation of forms by forms. Recent research has investigated which forms, called n-universal, represent all forms of rank n. We prove the uniqueness of the 2-universality criterion provided by Kim, Kim, and Oh and find a criterion for 3-universality of positive-definite integral rank-6 quadratic forms. We define the concept of representation of a form in multiple ways and examine the properties of lattices which represent rank-n forms in multiple ways. Finally, we consider an extension of the new Waring's Problem which considers forms which are represented in r ways by a sum of squares.<br><br> 

Awards won at the 2005 ISEF
First Award of $1,000 - American Mathematical Society
________________________________________
2006 - MA015 
EXTENDED FAULT TOLERANCE OF HYPER-STAR GRAPHS
Meelap Vijay Shah
Stoney Creek High School, Rochester Hills, Michigan, USA

Interconnection networks define how the processors in a multi-processor computer are connected. The choice of interconnection network is vital to the performance and reliability of a computer. This project investigated several properties of the recently introduced hyper-star graph. <br><br> During long computations on multi-processor computers, several processors may become faulty. The effect of having 2(kappa-1) faulty processors in a hyper-star (kappa is the connectivity of the graph) was studied in order to determine if the hyper-star has a good extended fault tolerance (EFT) and extended fault diameter (EFD). Positive results would imply that the hyper-star offers a reliable connection scheme that retains high productivity.<br><br> To gain insight, a computer simulation program was written in Java. This simulation generated a hyper-star, denoted HS(n,k), and chose a set F of 2(kappa-1) faults. An adjacency matrix was created for the non-faulty processors. A recursive algorithm used this matrix to grow a forest of spanning trees on the graph. This determined the number of components in HS(n,k)\F as well as the size of each component (EFT). The Floyd-Warshall algorithm was then run on the adjacency matrix to determine the diameter of HS(n,k)\F (EFD). This simulation was run thousands of times on different hyper-stars. Based on the results, a general theorem was conjectured for the EFT of hyper-stars and was proven with mathematical induction. A conjecture was made about the EFD of hyper-stars.<br><br> This project showed that the hyper-star has a good EFT and potentially good EFD, increasing its competitiveness with other interconnection networks.<br><br> 

Awards won at the 2006 ISEF
Honorable Mention Award - American Mathematical Society
________________________________________
2005 - MA015 
ON UNIVERSALITY PROPERTIES OF POSITIVE-DEFINITE INTEGRAL QUADRATIC FORMS
Scott Duke Kominers
Walt Whitman High School, Bethesda MD, USA

We present several results in the representation theory of quadratic forms, the study of the properties of linear transformations relating degree-two polynomials. This field arose as a generalization of Lagrange's Four Squares Theorem, which states that all integers can be represented as a sum of four squares and has led to the concept of representation of forms by forms. Recent research has investigated which forms, called n-universal, represent all forms of rank n. We prove the uniqueness of the 2-universality criterion provided by Kim, Kim, and Oh and find a criterion for 3-universality of positive-definite integral rank-6 quadratic forms. We define the concept of representation of a form in multiple ways and examine the properties of lattices which represent rank-n forms in multiple ways. Finally, we consider an extension of the new Waring's Problem which considers forms which are represented in r ways by a sum of squares.<br><br> 

Awards won at the 2005 ISEF
Second Award of $1,500 - Mathematics - Presented by Science News
________________________________________
2006 - MA015 
EXTENDED FAULT TOLERANCE OF HYPER-STAR GRAPHS
Meelap Vijay Shah
Stoney Creek High School, Rochester Hills, Michigan, USA

Interconnection networks define how the processors in a multi-processor computer are connected. The choice of interconnection network is vital to the performance and reliability of a computer. This project investigated several properties of the recently introduced hyper-star graph. <br><br> During long computations on multi-processor computers, several processors may become faulty. The effect of having 2(kappa-1) faulty processors in a hyper-star (kappa is the connectivity of the graph) was studied in order to determine if the hyper-star has a good extended fault tolerance (EFT) and extended fault diameter (EFD). Positive results would imply that the hyper-star offers a reliable connection scheme that retains high productivity.<br><br> To gain insight, a computer simulation program was written in Java. This simulation generated a hyper-star, denoted HS(n,k), and chose a set F of 2(kappa-1) faults. An adjacency matrix was created for the non-faulty processors. A recursive algorithm used this matrix to grow a forest of spanning trees on the graph. This determined the number of components in HS(n,k)\F as well as the size of each component (EFT). The Floyd-Warshall algorithm was then run on the adjacency matrix to determine the diameter of HS(n,k)\F (EFD). This simulation was run thousands of times on different hyper-stars. Based on the results, a general theorem was conjectured for the EFT of hyper-stars and was proven with mathematical induction. A conjecture was made about the EFD of hyper-stars.<br><br> This project showed that the hyper-star has a good EFT and potentially good EFD, increasing its competitiveness with other interconnection networks.<br><br> 

Awards won at the 2006 ISEF
Third Award of $1,000 - Mathematics - Presented by Lucent Technologies
Second Award of $1,500 - United States Air Force
________________________________________
2005 - MA015 
ON UNIVERSALITY PROPERTIES OF POSITIVE-DEFINITE INTEGRAL QUADRATIC FORMS
Scott Duke Kominers
Walt Whitman High School, Bethesda MD, USA

We present several results in the representation theory of quadratic forms, the study of the properties of linear transformations relating degree-two polynomials. This field arose as a generalization of Lagrange's Four Squares Theorem, which states that all integers can be represented as a sum of four squares and has led to the concept of representation of forms by forms. Recent research has investigated which forms, called n-universal, represent all forms of rank n. We prove the uniqueness of the 2-universality criterion provided by Kim, Kim, and Oh and find a criterion for 3-universality of positive-definite integral rank-6 quadratic forms. We define the concept of representation of a form in multiple ways and examine the properties of lattices which represent rank-n forms in multiple ways. Finally, we consider an extension of the new Waring's Problem which considers forms which are represented in r ways by a sum of squares.<br><br> 

Awards won at the 2005 ISEF
DO NOT ANNOUNCE - First alternate for trip - American Committee for the Weizmann Institute of Science
________________________________________
2009 - MA016 
SOLAR ENERGY: HAVE YOU SEEN THE LIGHT?
Stephanie Niki Tzouanas
Clear Lake High School, Houston, TX

With the rising global temperatures, solar energy has become a sunny solution to many. This project investigated how solar energy could best be utilized to meet the energy needs of a small industrial plant by mathematically modeling solar collectors and determining which control mechanism (proportional, integral, or proportional/integral control) best maintained the outlet temperature of the heat transfer fluid.<br><br>A first-principles, dynamic model of the solar energy collector was developed and used to identify the control structure that best achieved the stated control objective. Flow rate of the heat transfer fluid was used as the manipulated variable, and the integral absolute error criterion was used to tune and evaluate the control performance.<br><br>The proportional/integral controller had the lowest integral absolute error and was found to be the best mechanism to maintain the outlet temperature of the heat transfer fluid. The proportional/integral controller was also found to be the best mechanism to maintain the outlet energy rate of the heat transfer fluid at setpoint. <br><br>Maintaining a constant outlet energy rate is preferred over maintaining a constant outlet temperature because this control objective affords smoother system operation and greater flexibility in terms of applications.<br><br>Suggesting a different control objective (collector outlet energy rate instead of outlet temperature) is a unique contribution of this project, and to the best of my knowledge it has not been used in published literature. Further research could include more rigorous modeling, consideration of additional control mechanisms, and application of the new control objective in an actual plant. 
________________________________________
2004 - MA016 
A NOVEL SET OF REPRESENTATIONS OF THE TWO-COMPONENT LINK GROUP AND CONSEQUENT LINK INVARIANTS
Sam Jay Lewallen
Stuyvesant High School, New York City, New York, United States

We propose a novel set of representations of the group pi(L) of a link L of two components onto the direct product of two dihedral groups. (The “group of a link” actually refers to pi(S -L), the fundamental group of three dimensional Euclidean space S with the link removed.) We analyze the group theoretic properties of these representations and show that they correspond to a coloring invariant of the link. We then demonstrate how this coloring invariant reduces into a mapping of one of the link components into a dihedral group and the other into a cyclic group. We analyze this new, reduced, coloring, which we call n-bicoloring, and explore its applicability to determining link equivalence. We also discuss several possible algorithms for deriving numerical and polynomial invariants from the novel coloring, and conjecture certain properties of these invariants, namely that they are related to the analogous classical invariants by a “twist” with the linking number. We then suggest an interpretation of these new invariants in terms of the covering spaces of the link. Such an interpretation facilitates more powerful generalizing of the novel representations. 

Awards won at the 2004 ISEF
Third Award of $250 - American Mathematical Society
Award of $5,000 - Intel Foundation Achievement Awards
Fourth Award of $500 - Mathematics - Presented by Panasonic Consumer Electronics Company
First Award of $3,000 - U.S. Air Force
Scholarship in the amount of $8,000 - Office of Naval Research on behalf of the U.S. Navy and Marine Corps.
All expense paid trip for five weeks and scholarship to the Bessie Lawrence International Summer Science Institute at the Weizmann Institute of Science in Rehovot, Israel - American Committee for the Weizmann Institute of Science
________________________________________
2008 - MA017 
CHIP-FIRING ANALYSIS OF STABILIZATION BEHAVIORS, HITTING TIMES, AND CANDY-PASSING GAMES
Paul Myer Kominers
Walt Whitman High School, Bethesda, MD

We apply the theory of chip-firing, a deterministic simulation of probabilistic processes on graphs, to the analysis of several combinatorial problems. We give formulae for the number of firings required to stabilize maximized trees and cycles. As applications of these formulae, we compute the hitting time between distinct vertices on cycles. We then examine the candy-passing game introduced by James Tanton. After showing that this game is isomorphic to a particular chip-firing game, we show that eventually every player’s candy pile will become consistent whenever the number of candies is at least 3n-2, where n is the number of students playing the game. We additionally discuss this result’s bearing on the Google PageRank algorithm. 

Awards won at the 2008 ISEF
Third Award of $250 - American Mathematical Society
________________________________________
2006 - MA018 
A FULLY COMBINATORIAL PROOF OF THE CHAN-ROBBINS-YUEN THEOREM
Daniel Abraham Litt
Orange High School, Pepper Pike, OH, USA

The Chan-Robbins-Yuen Theorem was conjectured in 1999 to aid in the study of dissections of the complete flow polytope. Though a proof has been given using complex methods, no proof using elementary methods--no fully combinatorial proof--was known. We provide such a proof, as well as several related results.<br><br> <br><br>Let M be the set of n x n matrices with the following properties:<br><br> <br><br>1) All elements are nonnegative integers.<br><br>2) All elements above the superdiagonal equal zero.<br><br>3) The sum of the elements in the i-th row and the sum of the elements in the i-th column each equal the i-th triangular number.<br><br> <br><br>The Chan-Robbins-Yuen Theorem states that |M| is the product of the first n Catalan numbers and relates to the dissections of the complete flow polytope into simplices. While providing the first known fully combinatorial proof of this theorem, we also derive a useful identity for the product of Catalan numbers, as well as discover an entire class of combinatorial problems with several interesting properties for future study.<br><br> <br><br>Proving the Chan-Robbins-Yuen Theorem by elementary methods is useful both because the work is insightful, elegant, and new, and because some of its results have the potential to drive future research. 

Awards won at the 2006 ISEF
Second Award of $500 - American Mathematical Society
Second Award of $1,500 - Mathematics - Presented by Lucent Technologies
________________________________________
2003 - MA018 
SOLUTIONS TO A SYSTEM OF EQUATIONS
Edward Yi Zhong
Tom C. Clark, San Antonio Texas, USA

This project was designed to prove or disprove Problem 6312 that appears on page 675 of the 1980 volume of "The American Mathematical Monthly". The problem asks to prove or disprove that the set of n equations in n unknowns<br><br>x1^(l1)+x2^(l2)+x3^(l3)+...+xn^(li)=0 (i=1,2,...,n)<br><br>where the li's are relatively prime positive integers, has only the trivial solution xi=0 (i=1,2,...,n), if and only if each m=2,3,...,n divides at least one li. This problem was then solved using three different techniques- a simple brute force method, a method utilizing the ideas of the roots of unity, and an algebraic method using ideas of the complex plane. An alternate route to a solution was also investigated using palindromic polynomials- a relatively unknown branch of polynomial study. This problem was then generalized to different finite fields. It was found that the problem was true for the real number system but false for the complex number system. Furthermore, new techniques are hinted at for further study into this problem. 

Awards won at the 2003 ISEF
Honorable Mention Award Certificates for International students and students under the age of 16 - National Aeronautics and Space Administration
________________________________________
2008 - MA018 
THE NONAGON ANOMALY
Katherine Rose Banks
Stuyvesant High School, New York, NY

In this paper, we give a proof that a 9-sided convex lattice polygon cannot have exactly 8 or 9 interior lattice points using combinatorial methods. This result, known as the Nonagon Anomaly, has been proven before in a heavily geometric manner that works only for the 2D integer lattice. Our proof shows the Nonagon Anomaly true for any set of points S satisfying the Pentagon Property: that a convex pentagon with vertices in S must have at least one point of S in its interior. Though our proof holds in this more general setting, the 2D integer lattice is known to satisfy the Pentagon Property, so we phrase all arguments in terms of convex lattice polygons for notational ease and familiarity. Other sets satisfying the Pentagon Property, as we show, include planar polygons in the n-dimensional integer lattice.<br><br> We first reprove the Fat Hexagon Theorem, known for the 2D integer lattice, which states that a convex lattice hexagon with a lattice point that is not a vertex on its boundary must have at least two interior lattice points. The new proof holds for any set of points satisfying the Pentagon Property. We proceed to prove that a convex lattice pentagon with at least three non-vertex lattice points on its boundary must have at least two interior lattice points, and introduce other lemmas before moving to casework. We hope our methods can be adapted for the general problem of finding numbers of interior lattice points which are unachievable for given n. 

Awards won at the 2008 ISEF
Award of three $1,000 U.S. Savings Bonds, a certificate of achievement and a gold medallion. - United States Army
UTC Stock with an approximate value of $2,000 - United Technologies Corporation
________________________________________
2008 - MA019 
A MATHEMATICAL INVESTIGATION OF THE CONSTRUCTABILITY OF ORIGAMI FOLDS
David Harry Richman
Spring Valley High School, Columbia, SC

While there exists an obvious connection between origami, the art of paper-folding, and the mathematical field of geometry, this connection has remained mostly unexplored until quite recently. Over the past decade, however, a few articles have been published that mathematically analyze various aspects of paper-folding. These include attempts to formally classify the set of possible origami folds. This investigation was focused on studying the group of points and lines than can be made using only origami folds. The term “origami fold” was defined as those described in Robert Lang’s article (2003). More specifically, the coordinate plane is imagined as an infinitely large sheet of “paper,” and the defined origami folds as “operations” that can be applied to previously known points or lines to create a new line, the line of the fold. Then, the article considers the collection of all points that are constructible starting with two arbitrary points, labeled (0, 0) and (1, 0), through the repeated application of origami folds. The set of all possible coordinates of these constructible points are then explored. The results were compared with the group of constructible points using a straightedge and compass, which has already been studied extensively. 
________________________________________
2003 - MA019 
GAME AND GRAPH THEORIES MEET: AN ANALYSIS OF PREDATOR-PREY GAMES
Robert Thomas Cordwell
Manzano High School, Albuquerque, New Mexico, USA

The purpose of my project is to develop a theory for predator-prey type games, which are game theoretical type "games" played on a simple and connected graph. I will then try to determine what use this theory would have in solving other mathematical problems. To the best of my knowledge, this problem is completely original and has never been analyzed before.<br><br> Testing whether the predators can catch the prey assumes that both the predators and the prey play perfectly. However, testing this involves an exhaustive search of all of the possible positions of the predators and the prey. Therefore, I will also write a computer program for rapidly testing many predator-prey type games.<br><br> I have found a method for creating a graph such that given any number, n, of predators, I can find a non-planar graph so that the prey can always escape, no matter what the initial position is. I have also found a planar graph such that a prey can always escape from 2 predators and have proven that there is no planar graph such that, given any initial position of 3 predators and a prey, the prey can always escape. <br><br> Given the increasing role which graph theory has in mathematical models, I believe that this project will have significant applications in the near future. These results may also be used in other areas of mathematics.<br><br> 
________________________________________
2007 - MA019 
A CANON OF CANONICAL FORMS
Christopher Lopez
The Bronx High School of Science, Bronx, New York, United States

The main objective of this project was to algebraically partition the polynomials into equivalence classes. The equivalence relation was defined such that polynomials whose graphs have ‘similar shapes’ are in the same class. <br><br> To perform this partitioning, a general mapping was arrived at in stages. First, the equivalence relation was defined and shown to have certain properties. Then, a general mapping was constructed by first examining specific cases and then generalizing. Graphs of specific polynomials were mostly used to check the consistency of the resulting mappings. <br><br> Partitions for the polynomials were found. Only one partition was needed for the quadratics, and there were three for cubics. Starting with the quartics, however, an infinite number of partitions is needed. It appears that although there are only six basic ‘types’ of quartics, some of the types had subtleties in the graphs which require an infinite number of classes to fully capture. Similar results held for higher degree polynomials.<br><br> A basic application for this project is to carry out the same procedures on the set of vectors, which is isomorphic to the set of polynomials. Another basic application is to carry out the procedures on other types of functions using Taylor series approximations, Finally, if the procedures were to be extended, it could be used for data storage and retrieval in computers.<br><br> 

Awards won at the 2007 ISEF
Third Award of $250 - American Mathematical Society
________________________________________
2003 - MA020 
DEMOCRACY'S DETAILS
Matthew J Rognlie, West Linn High School
West Linn, OR, United States

The goal of this project is to find which ranked voting system best reflects the voters’ preferences.<br><br>It uses a weighted variant of the classic Downsian proximity model of voting to analyze several different ranked voting systems, including Condorcet’s method, the Borda Count, and Instant Runoff Voting. The standard, single-vote “plurality” system is also analyzed because it can be interpreted mathematically as a ranked system. The precise objective is to find the voting system which picks the candidate with the lowest combined deviation from the voters, which is expressed as:<br><br> <br><br>where x refers to each individual voter, m the number of voters, y refers to each individual issue, n to the number of issues, z to each individual candidate, axy to voter x’s opinion on issue y, wxy the importance voter x puts on issue y, and cyz candidate z’s opinion on issue y.<br><br> <br><br>Voters may manipulate their votes to give an extra advantage to a candidate. This potential for “dishonest” voting, not marking true preferences, is included in the analysis. Actual elections with these systems are modeled using a variety of statistical distributions of weights and opinions. The results show that, after the potential for dishonest voting (which is greater in some systems than others) is included, Condorcet’s system elects the candidate with the lowest combined deviation the greatest amount of the time.<br><br> 
________________________________________
2005 - MA020 
CLASSIFICATION OF DETERMINANTAL SEQUENCES
Samuel Mohun Bhagwat
Winston Churchill High School, Livonia, Michigan, USA

Determinantal sequences are doubly infinite sequences {an} of nonzero integers such that the determinant a_n * a_n+3 - a_n+1* a_n+2 is equal to a fixed nonzero d for all n. All such sequences have invariants a = (a_2j + a_2j+2)/(a_2j+1) and b = (a_2j +1 + a_2j+3)/(a_2j+2), which are independent of k. Our aim is to classify all determinantal sequences, a nontrivial Diophantine problem. The key case is when gcd({a_2j}) = gcd({a_2j+1}) = 1, which we call reduced determinantal sequences. Such sequences have the important property that a, b belong to Z and gcd(a_k, a_k+2) = 1 for all k. <br><br>We first examine reduced determinantal sequences in the simple case 0 <= ab <= 4. We show that when ab = 0 or 4, the determinantal sequence is built from two arithmetic progressions, and when ab = 1, 2, or 3 the determinantal sequence is cyclic. We give a simple and explicit procedure to construct all sequences in each case. <br><br>We then proceed to the more difficult case where ab does not belong to {0, 1, 2, 3, 4}. By using arithmetic in the integer ring A of the real quadratic field Q(r), where r^2 - abr + ab = 0, we show that, up to three trivial operations, for each triple (a, b, d) there exist only finitely many reduced d-determinantal sequences with invariants a and b. An explicit example shows that this finite set of possibilities can have more than one element. This finiteness statement is false for 0 <= ab <= 4, and it generalizes earlier uniqueness results of M. Satriano that required |d| = 1. <br><br> 

Awards won at the 2005 ISEF
Second Award of $500 - American Mathematical Society
Fourth Award of $500 - Mathematics - Presented by Science News
________________________________________
2008 - MA020 
TRANSLATION-INVARIANT BINARY REPRESENTATIONS
Akhil Mathew
Madison High School, Madison, NJ

This work deals with a representation of real numbers based on convolutions of binary sequences, or representations, and normalized filters. It was conjectured that no universal filter existed that could decode every rational in the unit interval; I proved a stronger form: to an infinite set of rationals there exists no filter with representations for all of them, using a method of taking differences, weak-star limit points, and a non-elementary theorem of Szego. I also gave elementary proofs of weaker results in special cases using the properties of holomorphic functions and a density result, avoiding Szego's theorem. Then, I connected the representable rationals of one filter with the zeros of its Fourier transform by establishing bounds on representable rationals and their denominators. My proofs relied on results from harmonic analysis, number theory, and abstract algebra. <br><br> I then extended the problem to Banach spaces, or complete normed vector spaces, where I proved that analogous results held. To do that, I proved a non-trivial extension of Szego's theorem to matrices by replacing one summable sequence with another. I developed extensions of techniques used for the elementary bounds for this result. Then, I reduced the extended theorem to the classical version by taking for the summable sequence a multiple of the identity matrix. With this, I showed that one filter on a Banach space can have representations for only a finite number of vectors, and that those vectors lie in the rational convex hull of the set in which the representation takes its values. 

Awards won at the 2008 ISEF
First Award of $1,000 - Mu Alpha Theta, National High School and Two-Year College Mathematics Honor Society
________________________________________
2005 - MA021 
MODELING JOINT BEHAVIOR OF DEFAULTABLE ENTITIES: MULTIVARIATE COPULA METHODOLOGY
Revaz Levan Surguladze
Stuyvesant High School, New York, NY, USA

The multivariate copula framework for modeling the joint behavior of pools of defaultable entities is introduced and different copula functions are applied within this framework. Computational methods and algorithms are developed and implemented within the algebraic programming system Mathematica. As a particular application, a pool of defaultable credits is studied. Several useful statistics are computed and some phenomenological results are discussed. A sampling approach for estimating model uncertainty due to arbitrariness in the choice of copula function is suggested. A measure of deviation of loss distributions generated by a given copula from that generated by the Gaussian copula is defined. 
________________________________________
2006 - MA021 
CLASSIFICATION OF TRIG VALUES BY MEANS OF COMPLEX RADICALS
Jaime Varela
Caprock High School ,Amarillo Tx, United States

The problem was to find a radical that correctly defines a certain value for a trigonometric function. The purpose of this project was to find as many radicals and then see if any identities can be brought out from them.<br><br>The first step was to find other exact values of trig numbers. Afterwards I used these values to set up equivalence equations. Later I used trigonometric identities to set up the equation into a more desirable form. Afterward I used algebraic methods to solve the equation. I was left with a complex radical which I later used to define a wide variety of new values for certain functions. My conclusion is that the imaginary unit is an essential tool for defining trigonometric values.<br><br> 
________________________________________
2005 - MA021 
MODELING JOINT BEHAVIOR OF DEFAULTABLE ENTITIES: MULTIVARIATE COPULA METHODOLOGY
Revaz Levan Surguladze
Stuyvesant High School, New York, NY, USA

The multivariate copula framework for modeling the joint behavior of pools of defaultable entities is introduced and different copula functions are applied within this framework. Computational methods and algorithms are developed and implemented within the algebraic programming system Mathematica. As a particular application, a pool of defaultable credits is studied. Several useful statistics are computed and some phenomenological results are discussed. A sampling approach for estimating model uncertainty due to arbitrariness in the choice of copula function is suggested. A measure of deviation of loss distributions generated by a given copula from that generated by the Gaussian copula is defined. 
________________________________________
2003 - MA021 
A COMBINATORIAL PROOF OF SEYMOUR'S CONJECTURE FOR REGULAR ORIENTED GRAPHS WITH ALMOST REGULAR OUTSETS O'A AND O"A
Lester Wayne Mackey
Half Hollow Hills High School West, Dix Hills, NY, USA

Paul Seymour formulated a graph theory conjecture in 1993 concerning oriented graphs and their squares: Let G = (V,E) be a simple, oriented graph. Then, there exists some vertex v in G with outdegree in G^2 at least twice that in G. Previous advances toward a proof of this conjecture have been restricted largely to the realm of complete oriented graphs. This research project instead focuses on regular graphs and employs a unique combinatorial methodology to confirm Seymour's claim.<br><br> We validate Seymour's Conjecture for regular oriented graphs in which some vertex has an almost regular outset and an almost regular second degree outset using a proof by contradiction. First, we choose an arbitrary graph G and assume that no vertex in G obeys Seymour's Conjecture.<br><br> To establish a contradiction, we consider the second degree outsets of vertices xi in the almost regular outset of some vertex a. We find that when we minimize the sum of all potential second degree outlets, some vertex x1 still must at least double in outdegree in G^2. This is a contradiction.<br><br> In this manner, former results of mathematicians Dean and Latka are strengthened, and the validity of Seymour's Conjecture is proved for all regular oriented graphs with almost regular outset, O'a, and second degree outset, O''a, for a in G. Important implications are held for proof of the Cacceta-Haggkvist Conjecture, analysis of traffic flow, design of electrical circuits, maintenance of the World Wide Web, and study of rigidity in 3-dimensional structures. <br><br> 

Awards won at the 2003 ISEF
Third Place Awards of $250 - American Mathematical Society
________________________________________
2007 - MA021 
RESULTS IN GEOMETRIC INEQUALITIES
Avi William Levy
West Linn High School, West Linn, Oregon, USA

This project is a long term study and development of methods to prove the validity of proposed symmetric inequalities of three variables. Such an inequality, depending on its level of difficulty, may require complicated algebraic factorizations or even computer software to prove correct. As opposed to this unappealing type of solution, geometry can be used to provide intuitive proof of certain inequalities.<br><br>In the progression of this project, both geometric and inequality problems were first researched independently. Next, tools and concepts in geometry were refined to facilitate the application of algebra in a geometric setting. In particular, this stage was concerned with equalities as opposed to inequalities. Two geometric tools researched and applied were inversion and homothecy.<br><br>At first sight unrelated, a vast body of work which relies heavily on algebra in the form of vectors, functions, and systems of equations has been applied to the geometry of a triangle involving special points, lines, measures, and transformations. To some degree, there is great overlap between these two disciplines.<br><br>The project presents several ways of translating an algebraic inequality in three variables into an inequality of lengths in a triangle. In its most aesthetic form, the goal is to rephrase an algebraic inequality into the geometric scene; then, geometric results are applied to arrive at a simple, intuitive, and ingenious proof. 

Awards won at the 2007 ISEF
Honorable Mention Award - American Mathematical Society
________________________________________
2004 - MA021 
2X2 CONTINGENCY TABLES: JACKKNIFE EVALUATIONS OF ASYMPTOTIC AND EXACT TESTS
Michael F. Page
Pingry School, Martinsville, NJ, USA

For 2x2 contingency tables, the statistical test of independence examines whether two variables, each of which has two categories, are independent of one another or related. Researchers typically use the asymptotic chi-square test for large samples and Fisher’s exact test for small to moderate-sized samples. These tests routinely appear in journal articles in medicine, psychology, anthropology, economics, and many other fields. Because of issues with both the chi-square and Fisher’s test, I hypothesized that they should be augmented with jackknife evaluations to avoid erroneous conclusions. A modified jackknife technique, based on leaving out or adding one observation at a time and recalculating the test statistic, was applied to 36 data sets with sample sizes between 100 and about 500. The primary variables of interest were the p-value, the probability that indicates whether the variables are independent or related, and the maximum p-value associated with the modified jackknife. I developed guidelines for combining a jackknife analysis with the chi-square and Fisher’s tests. To simplify the analysis, I determined a mathematical procedure for finding the maximum p-value with limited calculations. The chi-square test led to an incorrect conclusion for 10 of the 36 data sets when judged against Fisher’s test; the jackknife evaluations based on my guidelines were able to identify all 10 of these problems. Other data sets were found to warrant further study with larger samples to ensure stable results. Four of the data sets taken from medical journals, one of which reported an incorrect conclusion, are discussed in detail. 

Awards won at the 2004 ISEF
First Award of $1000 - American Statistical Association
________________________________________
2003 - MA021 
A COMBINATORIAL PROOF OF SEYMOUR'S CONJECTURE FOR REGULAR ORIENTED GRAPHS WITH ALMOST REGULAR OUTSETS O'A AND O"A
Lester Wayne Mackey
Half Hollow Hills High School West, Dix Hills, NY, USA

Paul Seymour formulated a graph theory conjecture in 1993 concerning oriented graphs and their squares: Let G = (V,E) be a simple, oriented graph. Then, there exists some vertex v in G with outdegree in G^2 at least twice that in G. Previous advances toward a proof of this conjecture have been restricted largely to the realm of complete oriented graphs. This research project instead focuses on regular graphs and employs a unique combinatorial methodology to confirm Seymour's claim.<br><br> We validate Seymour's Conjecture for regular oriented graphs in which some vertex has an almost regular outset and an almost regular second degree outset using a proof by contradiction. First, we choose an arbitrary graph G and assume that no vertex in G obeys Seymour's Conjecture.<br><br> To establish a contradiction, we consider the second degree outsets of vertices xi in the almost regular outset of some vertex a. We find that when we minimize the sum of all potential second degree outlets, some vertex x1 still must at least double in outdegree in G^2. This is a contradiction.<br><br> In this manner, former results of mathematicians Dean and Latka are strengthened, and the validity of Seymour's Conjecture is proved for all regular oriented graphs with almost regular outset, O'a, and second degree outset, O''a, for a in G. Important implications are held for proof of the Cacceta-Haggkvist Conjecture, analysis of traffic flow, design of electrical circuits, maintenance of the World Wide Web, and study of rigidity in 3-dimensional structures. <br><br> 

Awards won at the 2003 ISEF
Award of $1,000 U.S. Savings Bond - Ashtavadhani Vidwan Ambati Subbaraya Chetty (AVASC) Foundation
Second Award of $1,500 - Mathematics - Presented by Panasonic Consumer Electronics Company
All expense-paid trip to attend the U.S. Space Camp in Huntsville, Alabama and a certificate - National Aeronautics and Space Administration
________________________________________
2008 - MA021 
BASEBALL AND THE MARKOV CHAIN THEORY
Kevin Eugene Fritz
Home Educated, Hillsborough, NJ

In contrast to an analysis based on simple probabilities of events, a Markov Chain analysis takes into account the influence that the current state of a process has on future states. In this project, I have applied a Markov Chain analysis to model probabilities associated with the game of baseball. For this research an existing algorithm developed by Bukiet et al. was modified and improved. Play-by-play data for an entire season of Major League baseball were acquired and a computer program was written to parse this data to determine the probabilities of baserunners advancing a specific number of bases on various types of hits. These probabilities were used to create an improved probability model for runner advancement which was included in the algorithm. The effect of double plays on runner advancement probabilities was also determined and implemented in the algorithm. A series of computer programs in the C language were written to automate implementation of the model across many players or teams. Finally, the main computer program was modified to add the capability of analyzing partial games to find the effect of certain plays on the probability of winning. The new Markov Chain model was applied to five research questions. The study identified the incremental gain in bases advanced for various types of hits relative to a single. The probability of the pitcher's team winning when a great slugger was intentionally walked was found to be lower than when the slugger was pitched to in most game situations. Regardless of the inning or batting position used, National League teams score more runs when using a pinch hitter and both the inning and batting position had an effect on the number of runs scored. Players were rated according to the number of wins they would add to the average team on the basis of this Markov model and the results were compared to actual winners of MVP and Cy Young awards. Sportswriters selected the highest rated player for the MVP and Cy Young awards only 36% and 50% of the time, respectively. 

Awards won at the 2008 ISEF
Trip to attend the Taiwan International Science Fair. - National Taiwan Science Education Center
________________________________________
2003 - MA021 
A COMBINATORIAL PROOF OF SEYMOUR'S CONJECTURE FOR REGULAR ORIENTED GRAPHS WITH ALMOST REGULAR OUTSETS O'A AND O"A
Lester Wayne Mackey
Half Hollow Hills High School West, Dix Hills, NY, USA

Paul Seymour formulated a graph theory conjecture in 1993 concerning oriented graphs and their squares: Let G = (V,E) be a simple, oriented graph. Then, there exists some vertex v in G with outdegree in G^2 at least twice that in G. Previous advances toward a proof of this conjecture have been restricted largely to the realm of complete oriented graphs. This research project instead focuses on regular graphs and employs a unique combinatorial methodology to confirm Seymour's claim.<br><br> We validate Seymour's Conjecture for regular oriented graphs in which some vertex has an almost regular outset and an almost regular second degree outset using a proof by contradiction. First, we choose an arbitrary graph G and assume that no vertex in G obeys Seymour's Conjecture.<br><br> To establish a contradiction, we consider the second degree outsets of vertices xi in the almost regular outset of some vertex a. We find that when we minimize the sum of all potential second degree outlets, some vertex x1 still must at least double in outdegree in G^2. This is a contradiction.<br><br> In this manner, former results of mathematicians Dean and Latka are strengthened, and the validity of Seymour's Conjecture is proved for all regular oriented graphs with almost regular outset, O'a, and second degree outset, O''a, for a in G. Important implications are held for proof of the Cacceta-Haggkvist Conjecture, analysis of traffic flow, design of electrical circuits, maintenance of the World Wide Web, and study of rigidity in 3-dimensional structures. <br><br> 

Awards won at the 2003 ISEF
All expense paid trip to Operation Cherry Blossom in Tokyo, Japan. Each trip winner will also receive $3,000 in savings bonds, $300 from the Association of the United States Army, a gold medallion and a Certificate of Achievement. - U.S. Army
________________________________________
2005 - MA022 
INVESTIGATING CUBES AND THE "MYSTERY OF SPACE"
Kenneth Beniacar
The Bronx High School of Science, Bronx, New York, U.S.A.

In my experimentation, I took a problem from a NYML Competition and expanded it. This involved finding the length of the longest diagonal of a cube whose sides measured 1. Broadening the topic to include a wide number of adjacent cubes, I focused on cubes whose side lengths form a geometric sequence (2,4,8,16...). I began with two cubes, setting the first cube’s sides to be a constant (1) and varying the second cube’s sides. After analyzing the data for a 1x1x1 cube adjacent to a 2x2x2 cube, I continued increasing the length of the second cube’s side until I found a general formula for the longest diagonal’s length for two cubes. I did the same for three and four cubes. Eventually, I was able to generalize a formula for the longest diagonal of any two cases of any number of adjacent cubes.<br><br> I also examined the “mystery of space” lost when a rectangular prism is broken down into adjacent cubes. Following a similar technique used in the above experiment, I was able to generalize formulas for the longest diagonal of the “mysterious space” for an “x” number of cubes, and show there is only a slight difference between the lengths of the diagonals for “x” number of cubes and their “mysterious space” diagonal. I called this difference the “constant of proportionality”, being the only piece that unified both formulas. I established a general fraction relating the major formulas, which led me to prove an important mathematical theorem. <br><br> 
________________________________________
2003 - MA022 
UNDERSTANDING FIBONACCI PATTERNS IN CACTI OF THE NORTHERN CHIHUAHUAN DESERT PHASE II
Brian S. Christopherson, Carlsbad High School
3000 W. Church Carlsbad, NM 88220 USA

After documenting the fact that Fibonacci numbers 1,1,2,3,5,8,13,21,34,55,89,144, . . .and the Golden Ratio (phi = 1.618033988749894848204586834365638117720309179. . .) are both present in two specific species of cactus last year, two new questions arise for a phase II project. How many different cactus can be added to the collection of cactus that exhibit these number patterns? Can the combination of analyzing the geometry of the growth pattern of cactus along with mathematically analyzing the data reveal a deeper connection to either Fibonacci Numbers or the Golden Ratio?<br><br> The PROCEDURE used to address the first question included collecting data in the field using a blind process to insure valid and reliable data collection. Identifying 10 or more of each species and then looking at spike patterns on each specimen was the task of the data collection team. Subsequent trips were required to confirm that bigger patterns were present.<br><br> DATA ANALYSIS included looking at the geometry of growth patterns of the cactus studied in the field and matching it with already published information about Fibonacci plant growth patterns. While crunching data, a true revelation became clear. There is a limiting process that allows for a more exact approximation of phi with ALL cactus species studied.<br><br> Two CONCLUSIONS were drawn. Numerous species of cactus reveal the mathematical irrational pattern and this approximation becomes more exact as the points and laps counted between vertical points increases. Second, plants (especially cactus) must maximize its efficiency if it is to survive. Having irrational growth patterns guarantee this maximizing effect. 
________________________________________
2008 - MA022 
CONTINUED FRACTIONS AND ORBITS OF A LINEAR FRACTIONAL TRANSFORMATION
Shravani Mikkilineni
Detroit Country Day School, Beverly Hills, MI

If k is a non-square positive integer then the continued fraction of sqrt(k) provides a sequence {p_n/q_n} of excellent rational approximations to sqrt(k), but usually the only way to compute each p_N/q_N is by a process that requires first computing p_i and q_i for all i<N. Induction shows that if d = floor[sqrt(k)] and 2d/(k-d^2) is an integer, then for f_k (x)=(dx+k)/(x+d) we have f^n_k (d)=p_n/q_n for all n greater than or equal to 0. It was conjectured that if 2d/(k-d^2) = m/2 is a half-integer (m odd), then f^i_k (d) = p_n_i/q_n_i for a specific sequence 0 = n_0 < n_1 < n_2 < ... depending on the parity of k. We introduce new ideas to prove this conjecture. The key is to study a more general problem that is better-suited to inductive arguments. We also propose a conjectural generalization of this phenomenon whenever 2d/(k-d^2) = m/(2^s) with odd m and any s greater than or equal to 1, using a sequence {n_i} depending in a subtle way on k. We give a parametric formula for all such k and prove many instances of this generalized conjecture for s=2,3. 

Awards won at the 2008 ISEF
Second Award of $500 - American Mathematical Society
Second Award of $500 U.S. Savings Bond - Ashtavadhani Vidwan Ambati Subbaraya Chetty (AVASC) Foundation
Second Award of $1,500 - Mathematical Sciences - Presented by
________________________________________
2009 - MA022 
AN INVESTIGATION OF THE CLOSURE OF THE SET OF SINGLETON SETS OF NATURAL NUMBERS UNDER UNION, INTERSECTION, ¯, +, ×
Jason Saul Gross
Commack High School, Commack, NY

The closure, under point-wise addition, point-wise multiplication, union, intersection, and complementation with respect to the natural numbers (nonnegative integers), of the set of singleton sets of natural numbers was investigated. The closure of this set under subsets of these operations was investigated. For some of these sub-problems, useful classifications were made; criteria were determined for deciding membership in the closure. Zeta-dimension was the primary starting point for searching for such criteria. The 0-1 conjecture made by Doty et al. was investigated and modified, but remains an open problem. Modifications to zeta-dimension, as well as with combinations of zeta-dimension with other criteria, were made and used to classify the closure investigated in various sub-problems. A conjecture, analogous to the 0-1 conjecture, was made about the closure of the set of subsets of natural numbers with zeta-dimension 0. A connection between the 0-1 conjecture and the reciprocals of the twin primes was made. 

Awards won at the 2009 ISEF
Fourth Award of $500 - Mathematical Sciences - Presented by Intel
________________________________________
2010 - MA023 
SUPER KOHLER-RICCI FLOW
Joshua William Pfeffer
North Shore Hebrew Academy High School, Great Neck, NY

The Ricci flow, particularly simple on Kähler manifolds, “smoothes out” a manifold’s irregularities while preserving its fundamental features, helping mathematicians distinguish between different classes of manifolds. The Ricci flow played a key role in Perelman's proof of the Poincaré conjecture.<br><br> Recently, mathematicians and physicists have studied supersymmetric generalizations of manifolds called supermanifolds. This paper is the first to define a flow on supermanifolds analogous to Ricci flow on Kähler manifolds. <br><br> My flow gives a second-order differential equation for the metric of the bosonic submanifold. Fixed points are Kähler-Einstein supermetrics. My flow resembles Perelman’s modified Ricci flow; Perelman’s artificial extension arises quite naturally in my flow. I find that solutions to ordinary Ricci flow can be extended to solutions of my flow when the bosonic submanifold has vanishing average scalar curvature; such solutions are stable, with fluctuations in the Kähler potential decaying after sufficiently long time. I use this result to give a completely new and conceptual proof of the well-known theorem that scalar-flat implies Ricci-flat on compact Kähler manifolds which admit Ricci-flat metrics.<br><br> My flow’s fixed points are superextensions of scalar-flat metrics, which are the Calabi flow’s fixed points; with fewer derivatives on the metric, my flow offers a simpler alternative to the Calabi flow. I study perturbations about the fixed point given by the product metric of the two-sphere and a Riemannian surface; while such solutions are generically unstable under ordinary Ricci flow, I explicitly construct bounds on the eigenvalues of both spaces that guarantee stability under my flow. 

Awards won at the 2010 ISEF
Second Award of $500 - American Mathematical Society
All expense paid trip to tour CERN - European Organization for Nuclear Research-CERN
Intel ISEF Best of Category Award of $5,000 for Top First Place Winner - Mathematical Sciences - Presented by Intel
________________________________________
2009 - MA023 
MATCHING PRECLUSION FOR THE (N,K)-BUBBLE-SORT GRAPHS
David Alfred Sherman
Wylie E. Groves High School, Beverly Hills, MI

A perfect matching in a graph is a set of edges such that each vertex of the graph is the endpoint of exactly one edge in the set; each of the vertices is "matched" with another vertex by selecting common edges. The matching preclusion number of a graph with an even number of vertices is the minimum number of edges whose deletion results in a graph that has no perfect matchings. Matching preclusion has important applications in computer science because it is a good measure of the strength of a particular network design. The (n,k)-bubble-sort graphs, a generalization of the traditional bubble-sort graphs, are a class of interconnection networks. In this project, I investigate the property of matching preclusion for the (n,k)-bubble-sort graphs. Specifically, I find the matching preclusion number and classify all of the optimal solutions. 

Awards won at the 2009 ISEF
Third Award of $1,000 - Mathematical Sciences - Presented by Intel
________________________________________
2010 - MA023 
SUPER KOHLER-RICCI FLOW
Joshua William Pfeffer
North Shore Hebrew Academy High School, Great Neck, NY

The Ricci flow, particularly simple on Kähler manifolds, “smoothes out” a manifold’s irregularities while preserving its fundamental features, helping mathematicians distinguish between different classes of manifolds. The Ricci flow played a key role in Perelman's proof of the Poincaré conjecture.<br><br> Recently, mathematicians and physicists have studied supersymmetric generalizations of manifolds called supermanifolds. This paper is the first to define a flow on supermanifolds analogous to Ricci flow on Kähler manifolds. <br><br> My flow gives a second-order differential equation for the metric of the bosonic submanifold. Fixed points are Kähler-Einstein supermetrics. My flow resembles Perelman’s modified Ricci flow; Perelman’s artificial extension arises quite naturally in my flow. I find that solutions to ordinary Ricci flow can be extended to solutions of my flow when the bosonic submanifold has vanishing average scalar curvature; such solutions are stable, with fluctuations in the Kähler potential decaying after sufficiently long time. I use this result to give a completely new and conceptual proof of the well-known theorem that scalar-flat implies Ricci-flat on compact Kähler manifolds which admit Ricci-flat metrics.<br><br> My flow’s fixed points are superextensions of scalar-flat metrics, which are the Calabi flow’s fixed points; with fewer derivatives on the metric, my flow offers a simpler alternative to the Calabi flow. I study perturbations about the fixed point given by the product metric of the two-sphere and a Riemannian surface; while such solutions are generically unstable under ordinary Ricci flow, I explicitly construct bounds on the eigenvalues of both spaces that guarantee stability under my flow. 

Awards won at the 2010 ISEF
First Award of $1,000 - Mu Alpha Theta, National High School and Two-Year College Mathematics Honor Society
The SIYSS is a multi-disciplinary seminar highlighting some of the most remarkable achievements by young scientists from around the world. The students have the opportunity to visit scientific institutes, attend the Nobel lectures and press conferences, learn more about Sweden and experience the extravagance of the Nobel festivities. Valid passport required for travel. - Seaborg SIYSS Award
Award of three $1,000 U.S. Savings Bonds, a certificate of achievement and a gold medallion. - United States Army
________________________________________
2008 - MA023 
ROCKET SCIENCE
Sean Thomas Abrahamson
Piedra Vista High School, Farmington, NM

The purpose of this experiment was to see if it is possible to predict the height of a model rocket mathematically. The hypothesis is that if the rocket height is only dependent on air resistance, mass, engine thrust, and gravity, then the maximum height of a model rocket could be predicted. The topic was chosen because the researcher is interested in rocket science. To predict the height of a rocket many equations must be known to cover all of the factors. The model rocket was invented by a man named Robert H. Goddard. The model rocket engine was made by Orville and Robert Carlisle who thought it would be better for kids to have a safe way to make rockets and not hurt themselves making them. <br><br> Using complicated equations that involve many variables fitted into a detailed calculator program, the predicted height was found. When the rockets were launched, two angle measuring devices and some trigonometry were used to find the actual height. The measured heights and the predicted heights were then compared and their differences found. <br><br> The results show that it is possible through just using theories and equations that it is possible to predict the maximum height that a rocket will go. Statistically it was determined that there was not enough evidence to reject the hypothesis. 

Awards won at the 2008 ISEF
First Award of $3,000 - United States Air Force
________________________________________
2010 - MA023 
SUPER KOHLER-RICCI FLOW
Joshua William Pfeffer
North Shore Hebrew Academy High School, Great Neck, NY

The Ricci flow, particularly simple on Kähler manifolds, “smoothes out” a manifold’s irregularities while preserving its fundamental features, helping mathematicians distinguish between different classes of manifolds. The Ricci flow played a key role in Perelman's proof of the Poincaré conjecture.<br><br> Recently, mathematicians and physicists have studied supersymmetric generalizations of manifolds called supermanifolds. This paper is the first to define a flow on supermanifolds analogous to Ricci flow on Kähler manifolds. <br><br> My flow gives a second-order differential equation for the metric of the bosonic submanifold. Fixed points are Kähler-Einstein supermetrics. My flow resembles Perelman’s modified Ricci flow; Perelman’s artificial extension arises quite naturally in my flow. I find that solutions to ordinary Ricci flow can be extended to solutions of my flow when the bosonic submanifold has vanishing average scalar curvature; such solutions are stable, with fluctuations in the Kähler potential decaying after sufficiently long time. I use this result to give a completely new and conceptual proof of the well-known theorem that scalar-flat implies Ricci-flat on compact Kähler manifolds which admit Ricci-flat metrics.<br><br> My flow’s fixed points are superextensions of scalar-flat metrics, which are the Calabi flow’s fixed points; with fewer derivatives on the metric, my flow offers a simpler alternative to the Calabi flow. I study perturbations about the fixed point given by the product metric of the two-sphere and a Riemannian surface; while such solutions are generically unstable under ordinary Ricci flow, I explicitly construct bounds on the eigenvalues of both spaces that guarantee stability under my flow. 

Awards won at the 2010 ISEF
Tuition Scholarship Award in the amount of $8,000 - Office of Naval Research on behalf of the United States Navy and Marine Corps
________________________________________
2004 - MA024 
HOW TO MAKE AND BREAK CIPHERS
Daniel Charles Schaffer
Orion Junior High School Harrisville, UT USA

Throughout several millennia, people have thought that Ciphers, a method of transforming text to conceal its meaning, were a safe way to pass secret information. The purpose of this project was to establish whether or not that was always true and if it's concealment could be improved. To determine this, the identity of different types of Ciphers, how to unlock their meaning as well as how to create more complex and difficult Ciphers, had to be identified.<br><br>To develop a conclusion, the first procedure was to learn about the types of Ciphers throughout history, and the advantages and disadvantages they caused. Second, identify how to duplicate different types of basic Ciphers. Third, determine how pattern words and letter frequency could be used to identify the weaknesses of Cipher-text. Next, apply the acquired information to a Cipher to see if it would reveal its message. Then, study the unbreakable ciphers created in the past. Finally, apply keywords and transposition to Cipher, mixing up the patterns.<br><br>Data showed that weaknesses are not always hard to find if it is known what to look for. Data also showed that all unbreakable Ciphers have something that alters the patterns and frequencies beyond recovery if the key is unknown.<br><br>The conclusion shows that Ciphers can be broken only if patterns can be discovered or frequencies analyzed. It also shows that a Cipher can be made unbreakable by changing positions and creating keywords that will change the values and scatter the patterns in the Ciphered text.<br><br> 
________________________________________
2010 - MA024 
DELIGNE CATEGORIES AND REPRESENTATION THEORY IN COMPLEX RANK
Akhil Mathew
Madison High School, Madison, NJ

Many key invariants in the representation theory of classical groups (symmetric groups S_n, matrix groups GL(n), O(n), Sp(2n)) are polynomials in n (e.g., dimensions of irreducible representations). This allowed Deligne to extend the representation theory of these groups to complex values of the rank n. Namely, Deligne defined (generically semisimple) families of tensor categories parametrized by n complex, which at positive integer n specialize to classical representation categories. Using Deligne's work, Etingof proposed a similar extrapolation for many non-semisimple representation categories built on representation categories of classical groups, e.g., degenerate affine Hecke algebras (dAHA). It is expected that for generic n such extrapolations behave as they do for large integer n ("stabilization"). <br><br><br>The goal of our work is to provide a technique to prove such statements. Namely, we develop an algebro-geometric framework to study categories indexed by a parameter n, in which the set of values of n for which the category has a given property is constructible. This implies that if a property holds for integer n then it holds for generic complex n. We use this to give a new proof of generic semisimplicity of Deligne categories. We also apply this method to Etingof's extrapolations of dAHA, and prove that when n is transcendental, "finite-dimensional" simple objects are quotients of certain standard induced objects, extrapolating Zelevinsky's classification of simple dAHA-modules for natural n. Finally, we obtain similar results for the extrapolations of categories associated to wreath products of the symmetric group with associative algebras. 

Awards won at the 2010 ISEF
Honorable Mention Award - American Mathematical Society
________________________________________
2009 - MA024 
CONVERGENCE ACCELERATION FOR THE POWER SERIES REPRESENTATION OF THE EXPONENTIAL INTEGRAL
Michael Christopher Yurko
Detroit Catholic Central High School, Novi, MI

The Exponential Integral function Ei(x) is an important function that has many applications which range from the modeling of radiative heat transfer in stellar atmospheres to the computation of non-equilibrium groundwater flow in the Theis solution. However, as a transcendental function, it is difficult to compute to a great degree of accuracy as required by certain applications such as number theory where it is often necessary to have thousands of bits of accuracy. The aim of this project is to study the efficacy of the application of various convergence acceleration methods to the power series representation of this function in an attempt to increase the convergence speed. 

Awards won at the 2009 ISEF
Honorable Mention Award - American Mathematical Society
________________________________________
2010 - MA024 
DELIGNE CATEGORIES AND REPRESENTATION THEORY IN COMPLEX RANK
Akhil Mathew
Madison High School, Madison, NJ

Many key invariants in the representation theory of classical groups (symmetric groups S_n, matrix groups GL(n), O(n), Sp(2n)) are polynomials in n (e.g., dimensions of irreducible representations). This allowed Deligne to extend the representation theory of these groups to complex values of the rank n. Namely, Deligne defined (generically semisimple) families of tensor categories parametrized by n complex, which at positive integer n specialize to classical representation categories. Using Deligne's work, Etingof proposed a similar extrapolation for many non-semisimple representation categories built on representation categories of classical groups, e.g., degenerate affine Hecke algebras (dAHA). It is expected that for generic n such extrapolations behave as they do for large integer n ("stabilization"). <br><br><br>The goal of our work is to provide a technique to prove such statements. Namely, we develop an algebro-geometric framework to study categories indexed by a parameter n, in which the set of values of n for which the category has a given property is constructible. This implies that if a property holds for integer n then it holds for generic complex n. We use this to give a new proof of generic semisimplicity of Deligne categories. We also apply this method to Etingof's extrapolations of dAHA, and prove that when n is transcendental, "finite-dimensional" simple objects are quotients of certain standard induced objects, extrapolating Zelevinsky's classification of simple dAHA-modules for natural n. Finally, we obtain similar results for the extrapolations of categories associated to wreath products of the symmetric group with associative algebras. 

Awards won at the 2010 ISEF
First Award of $1,000 U.S. savings bond - AVASC-Ashtavadhani Vidwan Ambati Subbaraya Chetty Foundation
Third Award of $1,000 - Mathematical Sciences - Presented by Intel
UTC Stock with an approximate value of $2,000 - United Technologies Corporation
________________________________________
2010 - MA025 
BENFORD'S LAW FAILURE IN DIFFERENT BASES OF RECURSIVE FUNCTIONS
Aaron Lawrence Zweig
Randolph High School, Randolph, NJ

Benford’s law is a statistical examination of the first digit of various data sets. Benford’s law states that the probability of an integer X being the first digit of one of these values in base B is logB(1+1/X). Many numerical sequences follow Benford’s law in base 10, including Fibonacci numbers and factorials.<br><br> Benford’s law is base-invariant in most cases, although it does fail under certain circumstances. For example, if the base of the recursive function 2^x is a power of 2 itself, then the first digits will be limited to only a few specific values, as opposed to the distribution that Benford’s law predicts. But this failure can also occur in pseudo-random functions, such as Fibonacci numbers.<br><br> In this project I examine recursive functions and develop a conjectural formula for Benford base failures in functions defined as f[x] = c * f[x-1] + d * f[x-2] for positive integer constants c and d. I show these functions can be classified into three groups based on the relationship between c and d, irrespective of the starting values. I characterize the Benford base failures for the cases that 1) d<c+1, 2) d=c+1 and 3) d>c+1. For cases 1) and 3) I show that Benford’s law fails at bases that follow the same recursion formula which generates the function. For example, Lucas and Fibonacci numbers (c=d=1) fail Benford’s law at bases equal to Lucas numbers. For case 2) I suggest a proof that explains why Benford’s law fails at bases equal to d^n. 
________________________________________
2009 - MA025 
ON G-DIFFERENCE: A PROPERTY OF PERMUTATIONS AND WORDS
Kristin Rose Cordwell
Manzano High School, Albuquerque, NM

Consider a simple graph G labeled such that a vertex v corresponds to n_v in the natural numbers. Let x and y be permutations of 1 , . . . , n, where x = (x_1 , . . . , x_n) and y = (y_1 , . . . , y_n). If for some 1 <= i <= n, x_i = n_v and y_i = n_w with v, w in G and connected, then x and y are defined as G-different in Korner, Malvenuto, and Simonyi's paper "Graph-different permutations". The maximum number of pairwise G-different permutations of length n is denoted kappa (G , n). Various bounds of kappa (G , n) on permutations for general and specific G were described in "Graph-different permutations". In this paper we continue the construction of bounds on permutations of kappa (G , n) for specific graphs G. We also look at kappa_w (G , n), the maximum number of pairwise G-different words of length n for a graph G. We construct general bounds for kappa_w (G , n) as well as evaluate kappa_w (G , n) for specific G. Finally, we relate kappa_w (G , n) to the fractional chromatic number of G. 

Awards won at the 2009 ISEF
Second Award of $1,500 - Mathematical Sciences - Presented by Intel
________________________________________
2009 - MA026 
DECONSTRUCTING EXTRASOLAR PLANETARY SYSTEMS THROUGH THE USE OF THE LOMB-SCARGLE PERIODOGRAM ALGORITHM
Samantha Emily Keith
Weber High School, Pleasant View, UT

Radial velocity graphs show the effect extrasolar planets have on their parent star. Analysis of these graphs result in the classification of the planet(s) in orbit around the star.<br><br>The purpose of this project was to determine if application of the Lomb-Scargle Periodogram Algorithm would accurately calculate the components of the radial velocity graphs. I predicted that this algorithm would have successful results, due to the equation’s ability to interpolate unevenly spaced data.<br><br>The Lomb-Scargle Periodogram Algorithm calculates power output at prospective periods. A power maximum signifies the possibility of a planet orbiting the star with an orbital period of the time at which the power maximum occurred. After periodograms of extrasolar systems were created, phase folding was implemented to test the existence of extrasolar planets. The time of each data point was “folded” about the period of a power maximum by taking the absolute value of the fractional part of the delta time divided by the chosen period. <br><br>Planets were discovered with a power maximum and smooth folded phase graph. The calculated properties of discovered planets-orbital period, orbital radius, and mass-were compared to published values in the Extrasolar Planets Encyclopedia. The percent error ranged from 0.6% to 5%. Compared to earlier methods of analyzing radial velocity data, the Lomb-Scargle Periodogram Algorithm is by far the most efficient way to analyze radial velocity data, and should be applied to future radial velocity data. 
________________________________________
2010 - MA026 
DECYCLING DENSITIES OF TESSELLATIONS
Jacob Benjamin Hurwitz
Montgomery Blair High School, Silver Spring, MD

Motivated by Stanley's claim that the greedy tree is the densest induced tree in the integer lattice, we wish to find densest induced trees in other graphs. The complement of a densest induced tree is a decycling set, meaning the decycling set is the smallest set that, when removed, makes the graph acyclic. Research on decyclings has applications to artificial intelligence and logic circuit design. Our work focuses on decycling numbers of particular graphs and decycling densities of certain tessellations.<br><br> <br><br>First, we find an equivalent description for the greedy tree in two dimensions, and this description gives us an upper bound on the decycling number of the finite grid. We also provide a lower bound for the decycling density of any infinite graph where all vertices have the same degree, such as for certain tessellations. This bound allows us to compute the decycling density of each of the three regular and eight semiregular tessellations in two dimensions.<br><br> <br><br>Turning towards other types of graphs, we decycle root lattices of Lie algebras and hypercubical lattices. Most interestingly, mapping the Cayley graph of the d-dimensional hypercubical lattice from Z^d to Z gives an exact value for the lattice's decycling density. In the future, we would like to prove some conjectures regarding the decycling densities of other root lattices and Cayley graphs. We also hope to extend our work to tilings of other types, in higher dimensions, and in non-Euclidean geometries. 

Awards won at the 2010 ISEF
Third Award of $250 - American Mathematical Society
________________________________________
2008 - MA026 
COMPUTATION OF THE ALEXANDER-CONWAY POLYNOMIAL ON THE CHORD DIAGRAMS OF SINGULAR KNOTS
Sana Raoof
Jericho High School, Jericho, NY

The inherent complexity of ambient isotopies, as outlined by the Reidemeister moves, necessitates the use of knot invariants to discriminate between planar representations of mathematical knots. The concept of finite-type invariants reduces the computation of the Alexander-Conway polynomial to the level of combinatorial objects called chord diagrams. In this research, some relations are proven for the delta invariant, which is the formal logarithm of the Alexander-Conway polynomial. <br><br> The specific family of chord diagrams, denoted Sk,m, contains two disjoint sets of chords arranged in a lattice pattern. Sk,m chord diagrams are characterized by complete bipartite intersection graphs. This paper shows that<br><br>delta(Sk,m)= 0 for k not equal to m<br><br> = m!(m-1)! for k=m<br><br> The theorems presented in this paper increase our knowledge of the Alexander-Conway polynomial invariant for chord diagrams, as well as prove when the invariant can be accurately used to guarantee knot equivalence. These findings pertain to the indentification of tangled organic molecules such as DNA and RNA, and are applicable to the Protein Folding Problem. 

Awards won at the 2008 ISEF
Honorable Mention Award - American Mathematical Society
________________________________________
2005 - MA026 
SOME RESULTS ON INCLUSIVE AND EXCLUSIVE PARTITIONS OF COMPLETE GRAPHS
Robert Thomas Cordwell
Manzano High School, Albuquerque NM, USA

The general problem of covering graphs with cycles leads to a number of interesting combinatorial problems. A new approach to generating and solving these problems is the idea of combining an embedding with a partition. <br><br> Consider the embedding of the n-vertex complete graph K_n into the complex plane such that the vertices are the roots of the equation x^n = 1 and all edges are represented by straight lines. We prove that, if and only if n is odd, the edge set of the graph K_n can be partitioned into a set of cycles with the inclusive property such that every edge is contained in exactly one cycle. Here, a cycle considered to be inclusive if, in the embedding, no two edges cross each other and its convex hull contains the center of the circle. Also, if and only if n is equivalent to 1, 7 (mod) 8, we show that K_n can be partitioned into a set of cycles with the exclusive property, where each cycle consists of non-crossing edges and the convex hull does not include the center of the circle. We provide constructive proofs for both of the aforementioned results. Some extensions of these main results and alternate proofs are also given. <br><br> This approach and these results link graph theory and number theory, two normally disparate areas of mathematics. Some number theory problems that would otherwise be very difficult to solve can be translated into the realm of graph theory and there solved successfully, and visa-versa.<br><br> 

Awards won at the 2005 ISEF
Honorable Mention Award - American Mathematical Society
________________________________________
2006 - MA026 
DISCRETE DYNAMICAL SYSTEMS IN DIRECTED GRAPHS
Jeff D Nanney
Plano East Senior High, Plano, TX, USA

The contents of this paper primarily focus on analyzing dynamical systems, defined as iterated processes on positive integers, which are aptly modeled by directed graphs. In particular, we characterize the dynamical systems that correspond to an elegant generalization of the famous Collatz conjecture. We develop a unique methodology of analyzing dynamical systems through the implementation of original encoding matrices, distinct from any existing encoding structure. The novel construction of these modified encoding matrices allows us to better analyze the directed graphs we consider, especially in how path concatenation in the graph is paralleled by a defined matrix concatenation. Furthermore, we construct and implement effective counting functions in order to generalize a major result on the original Collatz conjecture, a theorem concerning the implication of the property of uniform positive predecessor density. Though this property is logically independent of the Collatz conjecture, the result powerfully suggests the conjecture's validity, and more importantly, our technique prompts further research on the generalized Collatz functions. Therefore, this paper not only contributes original and significant results on a generalized class of Collatz functions, but also introduces an original methodology that applies to a wider class of discrete dynamical systems. 

Awards won at the 2006 ISEF
Tuition Scholarship Award of $5,000 per year for 4 years for a total value of $20,000 - Indiana University
________________________________________
2010 - MA026 
DECYCLING DENSITIES OF TESSELLATIONS
Jacob Benjamin Hurwitz
Montgomery Blair High School, Silver Spring, MD

Motivated by Stanley's claim that the greedy tree is the densest induced tree in the integer lattice, we wish to find densest induced trees in other graphs. The complement of a densest induced tree is a decycling set, meaning the decycling set is the smallest set that, when removed, makes the graph acyclic. Research on decyclings has applications to artificial intelligence and logic circuit design. Our work focuses on decycling numbers of particular graphs and decycling densities of certain tessellations.<br><br> <br><br>First, we find an equivalent description for the greedy tree in two dimensions, and this description gives us an upper bound on the decycling number of the finite grid. We also provide a lower bound for the decycling density of any infinite graph where all vertices have the same degree, such as for certain tessellations. This bound allows us to compute the decycling density of each of the three regular and eight semiregular tessellations in two dimensions.<br><br> <br><br>Turning towards other types of graphs, we decycle root lattices of Lie algebras and hypercubical lattices. Most interestingly, mapping the Cayley graph of the d-dimensional hypercubical lattice from Z^d to Z gives an exact value for the lattice's decycling density. In the future, we would like to prove some conjectures regarding the decycling densities of other root lattices and Cayley graphs. We also hope to extend our work to tilings of other types, in higher dimensions, and in non-Euclidean geometries. 

Awards won at the 2010 ISEF
Renewable Scholarships to the IIT College of Science and Letters - Illinois Institute of Technology
________________________________________
2008 - MA026 
COMPUTATION OF THE ALEXANDER-CONWAY POLYNOMIAL ON THE CHORD DIAGRAMS OF SINGULAR KNOTS
Sana Raoof
Jericho High School, Jericho, NY

The inherent complexity of ambient isotopies, as outlined by the Reidemeister moves, necessitates the use of knot invariants to discriminate between planar representations of mathematical knots. The concept of finite-type invariants reduces the computation of the Alexander-Conway polynomial to the level of combinatorial objects called chord diagrams. In this research, some relations are proven for the delta invariant, which is the formal logarithm of the Alexander-Conway polynomial. <br><br> The specific family of chord diagrams, denoted Sk,m, contains two disjoint sets of chords arranged in a lattice pattern. Sk,m chord diagrams are characterized by complete bipartite intersection graphs. This paper shows that<br><br>delta(Sk,m)= 0 for k not equal to m<br><br> = m!(m-1)! for k=m<br><br> The theorems presented in this paper increase our knowledge of the Alexander-Conway polynomial invariant for chord diagrams, as well as prove when the invariant can be accurately used to guarantee knot equivalence. These findings pertain to the indentification of tangled organic molecules such as DNA and RNA, and are applicable to the Protein Folding Problem. 

Awards won at the 2008 ISEF
A Scholarship of $50,000. - Intel Foundation Young Scientist Award
Intel ISEF Best of Category Award of $5,000 for Top First Place Winner - Mathematical Sciences - Presented by
________________________________________
2006 - MA026 
DISCRETE DYNAMICAL SYSTEMS IN DIRECTED GRAPHS
Jeff D Nanney
Plano East Senior High, Plano, TX, USA

The contents of this paper primarily focus on analyzing dynamical systems, defined as iterated processes on positive integers, which are aptly modeled by directed graphs. In particular, we characterize the dynamical systems that correspond to an elegant generalization of the famous Collatz conjecture. We develop a unique methodology of analyzing dynamical systems through the implementation of original encoding matrices, distinct from any existing encoding structure. The novel construction of these modified encoding matrices allows us to better analyze the directed graphs we consider, especially in how path concatenation in the graph is paralleled by a defined matrix concatenation. Furthermore, we construct and implement effective counting functions in order to generalize a major result on the original Collatz conjecture, a theorem concerning the implication of the property of uniform positive predecessor density. Though this property is logically independent of the Collatz conjecture, the result powerfully suggests the conjecture's validity, and more importantly, our technique prompts further research on the generalized Collatz functions. Therefore, this paper not only contributes original and significant results on a generalized class of Collatz functions, but also introduces an original methodology that applies to a wider class of discrete dynamical systems. 

Awards won at the 2006 ISEF
Third Award of $1,000 - Mathematics - Presented by Lucent Technologies
________________________________________
2004 - MA026 
GENERALIZATIONS OF SCHUR'S PROBLEM AND THE SEARCH FOR S(5)
Andrew Lewis Matteson
Randall High School, Amarillo TX, USA

This project in its present form is an analysis of Schur’s Problem of sum-free sets, a problem closely tied to multicolor Ramsey numbers. The author approaches the problem from three perspectives: combinatoric number theory, graph theory and computer analysis. Primarily, the author pursues a proof of the Abbott Conjecture: a statement of equality between two functions S(k) and h(k). The author proves several novel theorems and lemmas that represent major steps toward a resolution of the conjecture.<br><br>With the aid of the Mathematica programming language, the author has also devised several algorithms to calculate values of S(k) and h(k). These functions are notoriously difficult to calculate (neither is known for k > 4). By combining genetic algorithms and backtrack programming, the author devises two novel generalizations that can reduce the calculation time for small values of S(k) and h(k) from months to hours. By appealing to Radizowski’s survey paper of Ramsey numbers, the author presents novel limits on S(5) that improve on all published bounds. Based on his studies into algorithms and the Abbott Conjecture, the author also shows that S(5) is calculable within a year using a large parallel processing network.<br><br> 

Awards won at the 2004 ISEF
Third Award of $1,000 - Mathematics - Presented by Panasonic Consumer Electronics Company
________________________________________
2005 - MA026 
SOME RESULTS ON INCLUSIVE AND EXCLUSIVE PARTITIONS OF COMPLETE GRAPHS
Robert Thomas Cordwell
Manzano High School, Albuquerque NM, USA

The general problem of covering graphs with cycles leads to a number of interesting combinatorial problems. A new approach to generating and solving these problems is the idea of combining an embedding with a partition. <br><br> Consider the embedding of the n-vertex complete graph K_n into the complex plane such that the vertices are the roots of the equation x^n = 1 and all edges are represented by straight lines. We prove that, if and only if n is odd, the edge set of the graph K_n can be partitioned into a set of cycles with the inclusive property such that every edge is contained in exactly one cycle. Here, a cycle considered to be inclusive if, in the embedding, no two edges cross each other and its convex hull contains the center of the circle. Also, if and only if n is equivalent to 1, 7 (mod) 8, we show that K_n can be partitioned into a set of cycles with the exclusive property, where each cycle consists of non-crossing edges and the convex hull does not include the center of the circle. We provide constructive proofs for both of the aforementioned results. Some extensions of these main results and alternate proofs are also given. <br><br> This approach and these results link graph theory and number theory, two normally disparate areas of mathematics. Some number theory problems that would otherwise be very difficult to solve can be translated into the realm of graph theory and there solved successfully, and visa-versa.<br><br> 

Awards won at the 2005 ISEF
Fourth Award of $500 - Mathematics - Presented by Science News
________________________________________
2010 - MA026 
DECYCLING DENSITIES OF TESSELLATIONS
Jacob Benjamin Hurwitz
Montgomery Blair High School, Silver Spring, MD

Motivated by Stanley's claim that the greedy tree is the densest induced tree in the integer lattice, we wish to find densest induced trees in other graphs. The complement of a densest induced tree is a decycling set, meaning the decycling set is the smallest set that, when removed, makes the graph acyclic. Research on decyclings has applications to artificial intelligence and logic circuit design. Our work focuses on decycling numbers of particular graphs and decycling densities of certain tessellations.<br><br> <br><br>First, we find an equivalent description for the greedy tree in two dimensions, and this description gives us an upper bound on the decycling number of the finite grid. We also provide a lower bound for the decycling density of any infinite graph where all vertices have the same degree, such as for certain tessellations. This bound allows us to compute the decycling density of each of the three regular and eight semiregular tessellations in two dimensions.<br><br> <br><br>Turning towards other types of graphs, we decycle root lattices of Lie algebras and hypercubical lattices. Most interestingly, mapping the Cayley graph of the d-dimensional hypercubical lattice from Z^d to Z gives an exact value for the lattice's decycling density. In the future, we would like to prove some conjectures regarding the decycling densities of other root lattices and Cayley graphs. We also hope to extend our work to tilings of other types, in higher dimensions, and in non-Euclidean geometries. 

Awards won at the 2010 ISEF
Fourth Award of $500 - Mathematical Sciences - Presented by Intel
________________________________________
2004 - MA026 
GENERALIZATIONS OF SCHUR'S PROBLEM AND THE SEARCH FOR S(5)
Andrew Lewis Matteson
Randall High School, Amarillo TX, USA

This project in its present form is an analysis of Schur’s Problem of sum-free sets, a problem closely tied to multicolor Ramsey numbers. The author approaches the problem from three perspectives: combinatoric number theory, graph theory and computer analysis. Primarily, the author pursues a proof of the Abbott Conjecture: a statement of equality between two functions S(k) and h(k). The author proves several novel theorems and lemmas that represent major steps toward a resolution of the conjecture.<br><br>With the aid of the Mathematica programming language, the author has also devised several algorithms to calculate values of S(k) and h(k). These functions are notoriously difficult to calculate (neither is known for k > 4). By combining genetic algorithms and backtrack programming, the author devises two novel generalizations that can reduce the calculation time for small values of S(k) and h(k) from months to hours. By appealing to Radizowski’s survey paper of Ramsey numbers, the author presents novel limits on S(5) that improve on all published bounds. Based on his studies into algorithms and the Abbott Conjecture, the author also shows that S(5) is calculable within a year using a large parallel processing network.<br><br> 

Awards won at the 2004 ISEF
Scholarship award of $1,000 - Mu Alpha Theta, National High School and Two-Year College Mathematics Honor Society
________________________________________
2008 - MA026 
COMPUTATION OF THE ALEXANDER-CONWAY POLYNOMIAL ON THE CHORD DIAGRAMS OF SINGULAR KNOTS
Sana Raoof
Jericho High School, Jericho, NY

The inherent complexity of ambient isotopies, as outlined by the Reidemeister moves, necessitates the use of knot invariants to discriminate between planar representations of mathematical knots. The concept of finite-type invariants reduces the computation of the Alexander-Conway polynomial to the level of combinatorial objects called chord diagrams. In this research, some relations are proven for the delta invariant, which is the formal logarithm of the Alexander-Conway polynomial. <br><br> The specific family of chord diagrams, denoted Sk,m, contains two disjoint sets of chords arranged in a lattice pattern. Sk,m chord diagrams are characterized by complete bipartite intersection graphs. This paper shows that<br><br>delta(Sk,m)= 0 for k not equal to m<br><br> = m!(m-1)! for k=m<br><br> The theorems presented in this paper increase our knowledge of the Alexander-Conway polynomial invariant for chord diagrams, as well as prove when the invariant can be accurately used to guarantee knot equivalence. These findings pertain to the indentification of tangled organic molecules such as DNA and RNA, and are applicable to the Protein Folding Problem. 

Awards won at the 2008 ISEF
Trip to attend the Taiwan International Science Fair. - National Taiwan Science Education Center
________________________________________
2006 - MA026 
DISCRETE DYNAMICAL SYSTEMS IN DIRECTED GRAPHS
Jeff D Nanney
Plano East Senior High, Plano, TX, USA

The contents of this paper primarily focus on analyzing dynamical systems, defined as iterated processes on positive integers, which are aptly modeled by directed graphs. In particular, we characterize the dynamical systems that correspond to an elegant generalization of the famous Collatz conjecture. We develop a unique methodology of analyzing dynamical systems through the implementation of original encoding matrices, distinct from any existing encoding structure. The novel construction of these modified encoding matrices allows us to better analyze the directed graphs we consider, especially in how path concatenation in the graph is paralleled by a defined matrix concatenation. Furthermore, we construct and implement effective counting functions in order to generalize a major result on the original Collatz conjecture, a theorem concerning the implication of the property of uniform positive predecessor density. Though this property is logically independent of the Collatz conjecture, the result powerfully suggests the conjecture's validity, and more importantly, our technique prompts further research on the generalized Collatz functions. Therefore, this paper not only contributes original and significant results on a generalized class of Collatz functions, but also introduces an original methodology that applies to a wider class of discrete dynamical systems. 

Awards won at the 2006 ISEF
Award of three $1,000 U.S. Savings Bonds, a certificate of achievement and a gold medallion. - United States Army
________________________________________
2008 - MA026 
COMPUTATION OF THE ALEXANDER-CONWAY POLYNOMIAL ON THE CHORD DIAGRAMS OF SINGULAR KNOTS
Sana Raoof
Jericho High School, Jericho, NY

The inherent complexity of ambient isotopies, as outlined by the Reidemeister moves, necessitates the use of knot invariants to discriminate between planar representations of mathematical knots. The concept of finite-type invariants reduces the computation of the Alexander-Conway polynomial to the level of combinatorial objects called chord diagrams. In this research, some relations are proven for the delta invariant, which is the formal logarithm of the Alexander-Conway polynomial. <br><br> The specific family of chord diagrams, denoted Sk,m, contains two disjoint sets of chords arranged in a lattice pattern. Sk,m chord diagrams are characterized by complete bipartite intersection graphs. This paper shows that<br><br>delta(Sk,m)= 0 for k not equal to m<br><br> = m!(m-1)! for k=m<br><br> The theorems presented in this paper increase our knowledge of the Alexander-Conway polynomial invariant for chord diagrams, as well as prove when the invariant can be accurately used to guarantee knot equivalence. These findings pertain to the indentification of tangled organic molecules such as DNA and RNA, and are applicable to the Protein Folding Problem. 

Awards won at the 2008 ISEF
Tuition Scholarship Award in the amount of $8,000 - Office of Naval Research on behalf of the United States Navy and Marine Corps
________________________________________
2008 - MA027 
NUMBER ONE PHENOMENON: BENFORD'S LAW IN THE NATURAL WORLD
Abby Elizabeth Stevens
InTech Collegiate High School, North Logan, UT

This study evaluated the use of Benford’s Law in describing the distribution of natural environmental systems data. Benford’s Law states that for certain data sets, the probability that a number within the data set begins with a one is about 30% while the probability that the number begins with a nine is only about 4%. This law is used in various business applications to detect fraud, but has not been applied to natural systems data. To determine if Benford’s Law could be used to describe natural systems data, ten data sets from each of three environmental parameters, which included river flow rate, river turbidity, and air pollution, were first evaluated. The turbidity data sets provided the best fit, with eight of the ten data sets following Benford’s distribution. The two data sets that did not follow Benford’s Law came from rivers that were regulated by structures such as reservoirs or dams. To determine if Benford’s Law could be used to differentiate between natural rivers and rivers impacted by humans, ten additional turbidity data sets were evaluated. Each data source was examined on Google Earth to determine the degree of human influence, and the data’s conformance with Benford’s Law was evaluated. The results showed that only six of the ten data sets followed the hypothesis that environmental systems data follow Benford’s Law only when the samples are acquired from natural systems. This experiment shows that more research is needed to determine the use of Benford’s Law in natural systems. 
________________________________________
2003 - MA027 
CONTINUED FRACTIONS OF QUADRATIC LAURENT SERIES
Ethan James Street
Winston Churchill High School, Livonia, Michigan, USA

It is both natural and interesting to replace the ring of integers and field of real numbers with the ring F[x] and the field F((1/x)) for a field F, and to try to use continued fractions in F((1/x)) to solve Pell's equation in F[x].<br><br>I hypothesized that the solvability of Pell's equation in this context is equivalent to the eventual periodicity of the associated continued fraction (a non-trivial constraint for infinite F) and that such periodicity exhibits symmetry properties analogous to the classically studied case.<br><br>I proved my hypothesis, overcoming numerous obstacles not seen in the classical case, such as non-trivial units and lack of order structure. The method applies in characteristic 2, using a generalized form of Pell's equation. The technique of proof is a mixture of non-Archimedean methods and polynomial algebra, the central breakthrough being a close study of the properties<br><br>of a concept that I call a "reduced quadratic surd". After proving some important technical properties of reduced surds, I show that eventual periodicity of continued fractions implies the specific periodic and symmetric structure analogous to the classical case. I then use this result to prove that Pell’s equation has solutions if and only if the associated continued fraction is periodic – a result not seen in the classical theory.<br><br>As a result, the problem of Pell's equation in F[x] and the periodicity structure of quadratic surds in F((1/x)) is solved for arbitrary coefficient fields F, giving us interesting insight into the classical case.<br><br> 

Awards won at the 2003 ISEF
Second Place Awards of $500 - American Mathematical Society
________________________________________
2009 - MA027 
"MATHEMAGICAL" POOL
Wenhan Cui
Cookeville High School, Cookeville, TN

Let’s play pool! In this science project, an attempt was made to mathematically determine whether a target ball could be sent into an intended hole through a direct hit using basic principles of algebra and geometry. Given the positions of the cue ball, target ball, and intended hole on an established coordinate system, this project addressed (a) what conditions must be met in order for a target ball to go into an intended hole, and (b) beyond what region is it impossible to make the shot from a given position of the cue ball. Detailed analysis, including all relevant equations and figures, was presented and the proposed method was demonstrated through an example. It should be pointed out that this method is applicable to any size of the pool table and balls, as well as any positions of the hole and balls. 

Awards won at the 2009 ISEF
Honorable Mention Award - American Mathematical Society
________________________________________
2003 - MA027 
CONTINUED FRACTIONS OF QUADRATIC LAURENT SERIES
Ethan James Street
Winston Churchill High School, Livonia, Michigan, USA

It is both natural and interesting to replace the ring of integers and field of real numbers with the ring F[x] and the field F((1/x)) for a field F, and to try to use continued fractions in F((1/x)) to solve Pell's equation in F[x].<br><br>I hypothesized that the solvability of Pell's equation in this context is equivalent to the eventual periodicity of the associated continued fraction (a non-trivial constraint for infinite F) and that such periodicity exhibits symmetry properties analogous to the classically studied case.<br><br>I proved my hypothesis, overcoming numerous obstacles not seen in the classical case, such as non-trivial units and lack of order structure. The method applies in characteristic 2, using a generalized form of Pell's equation. The technique of proof is a mixture of non-Archimedean methods and polynomial algebra, the central breakthrough being a close study of the properties<br><br>of a concept that I call a "reduced quadratic surd". After proving some important technical properties of reduced surds, I show that eventual periodicity of continued fractions implies the specific periodic and symmetric structure analogous to the classical case. I then use this result to prove that Pell’s equation has solutions if and only if the associated continued fraction is periodic – a result not seen in the classical theory.<br><br>As a result, the problem of Pell's equation in F[x] and the periodicity structure of quadratic surds in F((1/x)) is solved for arbitrary coefficient fields F, giving us interesting insight into the classical case.<br><br> 

Awards won at the 2003 ISEF
Intel ISEF Best of Category Award of $5,000 for Top First Place Winner - Mathematics - Presented by Panasonic Consumer Electronics Company
________________________________________
2010 - MA027 
ON THE CONSTRUCTION OF AN UNCOUNTABLE CLASS OF TRANSCENDENTAL NUMBERS
Yevgeniy Rudoy
Stuyvesant High School, New York, NY

We will construct an uncountably large class of numbers and prove that all such numbers are transcendental. To do so, we will define a construct called a pseudonumber, and use it to examine decimal expansions of numbers without carrying when a digit reaches or exceeds 10. This will allow us to show that if P(x) is a polynomial of degree N with positive integer coefficients, and T is an element of the aforementioned class, the decimal expansion of P(T) will closely resemble the decimal expansion of T^N. That in turn will allow us to show that given two polynomials Q(x) and R(x) with positive integer coefficients and different degrees, Q(T) and R(T) will have different decimal expansions, and thus Q(T) - R(T) will be non-zero. However, since any polynomial with integer coefficients can be expressed as the difference of two polynomials with positive integer coefficients of different degrees, every polynomial with integer coefficients will be nonzero at T, which will show, by definition, that T is transcendental. 

Awards won at the 2010 ISEF
Fourth Award of $500 - Mathematical Sciences - Presented by Intel
________________________________________
2005 - MA027 
SOME RESULTS OF THE 3X+1 CONJECTURE
Daniel Zi Wang
Jasper High School, Plano, Texas, USA

The 3x+1 Conjecture: For every , there exists a such that Tk(x) = 1 where Tk is the k-fold composition of T with itself and T(x) equals n/2 if x is even and equals if x is odd. Inspite of the function’s simplicity, the 3x + 1 conjecture has remained unsolved for about 75 years. In this project, I propose a possible approach to the solution of the 3x+1 conjecture and present some results I have obtained in the direction. <br><br>Definition: The following sets of positive integers:<br><br>i) L1 = {n | n is a power of 2, that is, n = 2k}.<br><br>ii) Lk+1 = { | n is odd and with p in Lk, q is an integer}, for k = 2, 3, 4, …<br><br>Theorem 1: The 3x+1 conjecture is true if and only if , here N is the set of positive integers.<br><br>Theorem 2: <br><br>Theorem 3: <br><br>I am currently extending the method used above to general Lk in my forthcoming work.<br><br>(requires an equation editor)<br><br> 

Awards won at the 2005 ISEF
Fourth Award of $500 - Mathematics - Presented by Science News
________________________________________
2003 - MA027 
CONTINUED FRACTIONS OF QUADRATIC LAURENT SERIES
Ethan James Street
Winston Churchill High School, Livonia, Michigan, USA

It is both natural and interesting to replace the ring of integers and field of real numbers with the ring F[x] and the field F((1/x)) for a field F, and to try to use continued fractions in F((1/x)) to solve Pell's equation in F[x].<br><br>I hypothesized that the solvability of Pell's equation in this context is equivalent to the eventual periodicity of the associated continued fraction (a non-trivial constraint for infinite F) and that such periodicity exhibits symmetry properties analogous to the classically studied case.<br><br>I proved my hypothesis, overcoming numerous obstacles not seen in the classical case, such as non-trivial units and lack of order structure. The method applies in characteristic 2, using a generalized form of Pell's equation. The technique of proof is a mixture of non-Archimedean methods and polynomial algebra, the central breakthrough being a close study of the properties<br><br>of a concept that I call a "reduced quadratic surd". After proving some important technical properties of reduced surds, I show that eventual periodicity of continued fractions implies the specific periodic and symmetric structure analogous to the classical case. I then use this result to prove that Pell’s equation has solutions if and only if the associated continued fraction is periodic – a result not seen in the classical theory.<br><br>As a result, the problem of Pell's equation in F[x] and the periodicity structure of quadratic surds in F((1/x)) is solved for arbitrary coefficient fields F, giving us interesting insight into the classical case.<br><br> 

Awards won at the 2003 ISEF
The SIYSS is a multi-disciplinary seminar highlighting some of the most remarkable achievements by young scientists from around the world. The students have the opportunity to visit scientific institutes, attend the Nobel lectures and press conferences, learn more about Sweden and experience the extravagance of the Nobel festivities. - Seaborg SIYSS Award
________________________________________
2003 - MA028 
FINDING THE CHORD COEFFICIENTS IN A GEODESIC DOME
Eric Barrera
Gladys Porter High School, Brownsville, Texas, U.S.

The Purpose of this project was to find an equation to calculate the chord coefficients of Geodesic domes to make easier the construction of such structures. My hypothesis was that in order to find an equation to calculate the chord coefficients I would have to use Spherical Trigonometry and the Law of Sines. The procedure I followed was to get the radius, get the chord length, make a triangle with the radius and chord length, bisect the triangle to make two right triangles, get the angle that forms with the bisecting line of the triangle, and use the equation "Factor of a side = [sin^-1(sin a * sin r)]/r, which was found with the use of Napier’s Circle. The results showed that the chord factors of a geodesic dome with any given radius are constant. In conclusion the equation I used to get the chord factors of a geodesic dome was obtained using Spherical Trigonometry and the Law of Sines and therefore my hypothesis was correct. 
________________________________________
2010 - MA028 
AN ANALOGUE OF THE COLLATZ PROBLEM AND ITS CONNECTION TO CANTOR SETS
Se-Ho Kim
Marriotts Ridge High School, Marriottsville, MD

The Collatz problem is an unsolved problem in number theory that has been studied extensively since its development in the 1930’s. Ever since its inception, the Collatz problem has not shown significance to any other problem or in any other field of mathematics. This project investigates the relationship between a polynomial analogue of the Collatz Problem and a dyadic diagonal Cantor set. The analogue of the Collatz problem is presented over the binary field Z2[x] and, unlike the original Collatz conjecture, can be proven to hold for all such polynomials. An integer parallel to this analogue can be created by evaluating the polynomial at x=2, and by plotting consecutive terms in the paths of this integer parallel as ordered pairs in the Cartesian plane, a self-similar, fractal-like pattern can be obtained. It can be proven that both functions, given the same initial point, create the same set after one iteration. Also, after this initial iteration, it can be proven that each iteration is essentially the same for both functions. This project thus proves using induction that a modification of this fractal pattern is fundamentally equivalent (given trivial transformations, a rotation and dilation) to a dyadic diagonal Cantor set. 

Awards won at the 2010 ISEF
Scholarship Award of $12,500 per year, renewable annually - Florida Institute of Technology
Renewable Scholarships to the IIT College of Science and Letters - Illinois Institute of Technology
________________________________________
2006 - MA028 
THE EVOLUTION OF OPERANT CONDITIONING AND COOPERATION IN DELAYED GRATIFICATION GAMES
Benjamin Scott Hamner
Academic Magnet High School, North Charleston, SC, USA

While Iterated Prisoner’s Dilemma (IPD) research has yielded important insights into how cooperative strategies may have paradoxically evolved among selfish, unrelated individuals, most IPD studies have not allowed strategists to learn. Many organisms change strategy based on life experience, be it optimizing one’s strategy against a teacher / reinforcer (individual learning) or eliciting a learner's cooperation (reinforcement). Here we study the effects of learning and reinforcing strategies on delayed-gratification evolutionary games (including the IPD) in which defection yields an immediate benefit but cooperation may provide long-term rewards. Strategies include non-learning one–move-memory strategists, Learners who start using a one-move-memory strategy and then generally switch to the optimal response against non-Learners (but may develop counter-productive optimal counter-replies when facing other Learners), and Reinforcers who generally determine the best way to elicit cooperation from Learners. Learning costs are assumed low and payoffs are sufficiently varied so that the optimal reply to a strategist is initially unclear. In this model, Always Defect (ALLD) is the only evolutionarily stable strategy (ESS) although Learners and Reinforcers are best against one another. Assuming arbitrary role differences exist between strategists, two additional ESSs new to IPD games but a staple of operant conditioning emerge: the right-handed “Role 1: Reinforce / Role 2: Learn” and the left-handed “Role 1: Learn / Role 2: Reinforce.” Each role difference ESS pits Reinforcers against Learners, best against each other and the cornerstone of operant conditioning. The evolution of cooperation and operant conditioning may be intertwined. 

Awards won at the 2006 ISEF
First Award of $3,000 - United States Air Force
________________________________________
2003 - MA029 
THE SKEWING OF THE BELL CURVE: A STUDY OF GRADE INFLATION IN OKLAHOMA HIGH SCHOOLS
Christina Nicole Wall
Latta High School, Ada, Oklahoma, USA

This project compared the Bell Curve to high school student’s grades to determine whether grade inflation is becoming a problem in most high schools, and whether the size of the school, or the course affects the degree of grade inflation. <br><br> The hypothesis was that when high school student’s grades in the subjects of Biology, Algebra 2, English 2, and American History are compared to the Bell Curve, a negative skew will be evidenced. Additionally, when comparing the sizes of schools, a negative skew will be more pronounced in the smaller schools. Grades for four subjects were collected from 13 Oklahoma schools of all sizes. Grades were compared to the predicted Bell Curve distribution for three comparisons: #1 all grades for all courses and all school sizes combined, #2 the courses were compared to the degree of grade inflation, and #3 the school sizes were analyzed. <br><br> There was a significant difference between the school’s percentages and the Bell Curve’s percentages mainly with A and C grades. There were significantly more A grades and significantly less C grades than predicted by a Bell Curve distribution. Smaller schools had the greatest amount of grade inflation, larger schools were next, and mid-sized schools held the least. There were no major differences between course GPA’s. <br><br> The hypothesis of the experiment was supported. A negative skew was seen in the grades compared to the Bell Curve, and the smallest group of schools had a more prominent skew than the other groups. <br><br> 
________________________________________
2004 - MA029 
CHAOS THEORY: AN ANALYTICAL MODEL OF HEART FIBRILLATIONS
Alan Ho-Yin Gee
Frederick High School, Frederick, Maryland, USA

<br><br> Chaos theory, a branch of non-linear mathematics, is a framework for understanding erratic fluctuations. Heart fibrillations are a form of chaotic behavior in the body. In atrial fibrillation, the atria depolarize irregularly. Consequently, blood accumulates in the ventricles increasing the risk of clotting. Atrial Fibrillation is the most common arrhythmia, especially in middle-aged men. If undetected and untreated, an individual can experience strokes or heart attacks. The project’s purpose is to provide doctors with an efficient tool by determining a mathematical prediction of atrial fibrillations.<br><br> The procedure follows: Obtain electrocardiograms (EKG’s) of hearts in atrial fibrillation (identified by rapid fluctuations before normal Q-R-S complexes). Graph time length of P waves. Calculate the least squares regression line (LSRL), which predicts overall pattern of the plots, for each EKG. From the LSRL, calculate y-hat, a predicted time value, for each P wave. From y-hat, calculate the residual, the difference between the observed value and predicted value, comparing the overall pattern of the EKG. Model and predict the fibrillation by using QuickBasic to program software for logistic and bifurcation maps. The equation used in the logistic map is dx/dt=K*x*(1-x).<br><br> According to the residual plot, the normal sinus rhythms display uniform patterns with little variation. However, fibrillating hearts demonstrate random patterns, creating multiple curves on the residual plot. Analyzing the graph indicates the logistic map accurately modeled the overall P wave time patterns; the normal sinus rhythm displayed a constant line on the graph. The bifurcation map distinguishes which K parameters lead to fibrillation.<br><br> 
________________________________________
2010 - MA029 
RANDOMIZED ARTIFICIAL MAPS FOR EPIDEMIOLOGICAL STUDIES OF SPORADIC AMYOTROPHIC LATERAL SCLEROSIS (ALS)
Jane Margaret Cox
Timpview High School, Provo, UT

Sporadic Amyotrophic Lateral Sclerosis (ALS) or Lou Gehrig’s Disease is a fatal progressive paralytic disease with no known cause or cure. Death occurs on average within 2.2 years of clinical diagnosis. Neurologists at Dartmouth Medical School recently mapped the residences of ALS patients in New Hampshire, and found that they cluster around lakes contaminated by cyanobacteria, but not near uncontaminated lakes. Patient privacy concerns, however, prevent publication of the maps. I sought a procedure to encrypt the maps so that important information is preserved while protecting patient privacy. Standard encryption techniques such as RSA cannot be used since these techniques encrypt all of the data. I first measured the distances from ALS patient residences to the lakeshore and to the next patient residence using Google Earth. To generate the operational equivalent of a trap-door function, I then inserted a stochastic step by converting geographical data from a patient’s residence into polar coordinates on an artificial lake, using a randomized angle. I used the measured distances to the next nearest patient and the lake, calculating the other polar coordinates based on the original map coordinates. This keeps the distances between each patient and the lake the same, and between proximal patient residences the same, but makes it impossible to determine actual patients residences from the artificial map. This approach may prove useful in other epidemiological studies where there may be a geographical factor related to disease causation. 
________________________________________
2005 - MA029 
THE FRACTAL NATURE OF MUSIC
Vinay Venkatesh Ramasesh
North Crowley High School, Fort Worth, Texas, United States

This project investigates the existence of fractal properties - scale-independence, self-similarity, and infinite detail - in Western music. It was hypothesized that these properties would be strongly evident in pieces from the Early Baroque, Late Baroque, Classical, and Romantic Periods but not as strongly in Contemporary music.<br><br> Five pieces were selected, one from each period, and each was subjected to three processes - cantoring, thinning, and duration scaling - to bring out the fractal properties. <br><br>The cantoring process removes the middle third of a piece, then removes the middle thirds of the remaining sections, and so on for three iterations. The resulting piece is then played to test its resemblance to the original. <br><br>Thinning a piece consists of graphing the frequencies versus time of each piece's notes, and removing alternate frequencies for three iterations. If the music is fractal, the thinned graphs would strongly resemble the originals.<br><br>In duration scaling, the notes of each duration-value are counted, and the logarithm of this frequency is plotted against the logarithm of the duration value. A strong linear relationship, indicated by correlation coefficients close to one, suggests fractal nature.<br><br> Cantored music from all five periods closely resembled the original for two iterations; however, third-iteration Classical music did not. All thinned graphs had the same structure as the original, but were less detailed. Duration-scaled plots exhibited correlation coefficients greater than 0.85, implying a strong linear relation and power-law distribution. Therefore, it is concluded that fractal properties are highly evident in Western music. 

Awards won at the 2005 ISEF
$1,000 Honorable Mention Scholarship Award in Mathematics - Robert Luby, Jr. of IBM
________________________________________
2006 - MA030 
THE GOLDEN RATIO: AN ALTERNATIVE APPROXIMATION
Austin Hung-Huan Hu
Memorial High School, Victoria, TX, United States of America

This investigation was conducted to determine whether or not a new and original representation of the famous golden ratio (phi) could be derived, using a square root algorithm, that was more precise in terms of efficiency, meaning that the representation converged to the actual value of phi faster with fewer recursive iterations. After studying the definition of the golden ratio and its standard representations, the researcher developed his own representation. Four representations of the golden ratio (phi): the experimenter’s own implementing the Babylonian square root algorithm, a nested square root representation, a Fibonacci approximation, or an irrational (continuous fractions) were to be tested. The researcher expects his own representation that utilized the square root algorithm to be the most precise with a set number of iterations. Each representation was rewritten into a recursive function (for iteration purposes) and tested. The execution was done by a computer program which printed the results onto a text document. The results showed that the function that was designed by the experimenter converged to the golden ratio value the fastest with only six interations, followed by the nested square root function that converged after thirty-one iterations. The Fibonacci and irrational functions finished last at the same time with forty iterations. Thus, the researcher’s own representation of the golden ratio was the most efficient compared to the other standard representations of today. 
________________________________________
2004 - MA030 
DIOPHANTINE EQUATIONS: WHICH NUMBERS ARE LINEAR COMBINATIONS?
Ginger Beardslee Howell
Trinity Collegiate School, Darlington SC, USA

Diophantine equations sparked my interest from the time I first learned about them in number theory last semister. The linear Diophantine equation is<br><br>, where all the variables are nonnegative integers and <br><br>A number b is a linear combination of through if there exist nonnegative integers through that satisfy the equation. The simplest instance is the equation . Consider the example . One solution to this is <br><br>Therefore, fourteen is a linear combination. Fifteen is not a linear combination, but sixteen is because 7(1)+9(1)=16. As the value of b<br><br>increases, the frequency of linear combinations will also increase. Eventually, there will be a number g where every number above g is a linear combination, but g is not. g is known as the Frobenius number. Based on resulta I obtained playing with examples, I hypothesized that for two numbers, . For three numbers where but the gcd of any two numbers exceeds one, . I then took it a step further and said that for n numbers, where each is the product of all but one of a set of relatively prime factors, <br><br>My project consists of proofs of these conjectures. For each of the three formulae, I proved both that g is not a linear combination, and that every integer greater than g is.<br><br>These proofs show that my conjecture was accurate. However, they by no means solve all the intrigue of Diophantine equations. Math is the basis of all the sciences, and even a small step in mathematics can lead to a giant leap of scientific progress. 

Awards won at the 2004 ISEF
Honorable Mention Award - American Mathematical Society
________________________________________
2009 - MA030 
AN ANALYSIS OF ERDOS'S CONJECTURE
Matthew Henry Stoffregen
Woodoland Hills High School, Pittsburgh, PA

We examine Erdos's 1974 Conjecture that, for any odd primes p, q, and r, there exist infinitely many integers n such that 2n choose n is indivisible by any of p, q, and r. We will provide a proof in the case of two primes p and q, though we will fail to prove the 3-prime case. Also, we will provide a heuristic to show that the Conjecture is unlikely to be true in the case of 4 primes. To prove the 2 prime case we will exploit numbers that are "great" - that are in neighborhoods for which many n have 2n choose n indivisible by p, q, and r. We also prove that we can choose many numbers k such that 2n choose n is indivisible by p and 2 (n+k) choose n+k is indivisible by q. Insight is also provided into the case of three primes. 

Awards won at the 2009 ISEF
Honorable Mention Award - American Mathematical Society
Third Award of $1,000 - Mathematical Sciences - Presented by Intel
________________________________________
2003 - MA030 
THE MATHEMATICAL MODELING OF BREAK DANCING
Brandon James Hopkins, The Waterford School
Sandy, UT, USA

A mathematical model of an idealized break-dancer was developed. Foot position curves, foot velocities, total angular momentum, net torque, kinetic and potential energy, work, and power functions were derived on Maple. Break-dancing moves were evaluated for pleasing aesthetic appearance, power, and ease. During aesthetically pleasing moves, the feet nearly touch or “sweep” the floor and make large loops at rapid rates. During high “power” moves, the feet have high velocities, the whole system has a high angular momentum, and kinetic energy, work, and power reach high levels. Moves that can be carried out with a minimum amount of net torque require the least effort. Highly efficient moves maximize angular momentum and kinetic energy with minimum torque. Videotapes of actual break-dancers and the author’s own break-dancing experience were used to determine reasonable parameters for modeled moves.<br><br> Windmills, halos, and head-spins with various angles with the floor and between the legs were studied using inverse kinematic analysis. Moves with high aesthetic appeal, power, and efficiency were identified. Break-dancers favor moves with high efficiency and aesthetic appeal. Times when potential impact forces could be maximized were identified. Substantial energy savings could be expected if energy was expended cyclically when large amounts of torque were needed. The model suggests that before attempting “power” moves, large amounts of angular momentum could be generated and stored with minimum effort by applying torque in the head-spin position. These findings have potential significance in dance, martial arts, robotics, and a wide variety of rotating mechanical systems.<br><br> 

Awards won at the 2003 ISEF
Fourth Award of $500 - Mathematics - Presented by Panasonic Consumer Electronics Company
________________________________________
2009 - MA030 
AN ANALYSIS OF ERDOS'S CONJECTURE
Matthew Henry Stoffregen
Woodoland Hills High School, Pittsburgh, PA

We examine Erdos's 1974 Conjecture that, for any odd primes p, q, and r, there exist infinitely many integers n such that 2n choose n is indivisible by any of p, q, and r. We will provide a proof in the case of two primes p and q, though we will fail to prove the 3-prime case. Also, we will provide a heuristic to show that the Conjecture is unlikely to be true in the case of 4 primes. To prove the 2 prime case we will exploit numbers that are "great" - that are in neighborhoods for which many n have 2n choose n indivisible by p, q, and r. We also prove that we can choose many numbers k such that 2n choose n is indivisible by p and 2 (n+k) choose n+k is indivisible by q. Insight is also provided into the case of three primes. 

Awards won at the 2009 ISEF
First Award of $1,000 - Mu Alpha Theta, National High School and Two-Year College Mathematics Honor Society
________________________________________
2003 - MA032 
VARIANTS OF TIT FOR TAT IN THE EVOLUTIONARY ITERATED PRISNER'S DILEMMA
Benjamin Scott Hamner, Academic Magnet High School
Charleston, South Carolina, United States of America

This project tested variants of the strategy TIT FOR TAT in the evolutionary Iterated Prisoner's Dilemma (IPD). Ten variants of TIT FOR TAT, along with TIT FOR TAT, were placed in an environment that included nine other strategies in order to test the ability of these variants to end runs of mutual recrimination. Ending runs of mutual recrimination translated into higher scores for each of the variants. The ten variants were TIT FOR TWO TATS, StTFT1, StTFT2, StTFT3, StTFT4, StTFT5, CTFT05, CTFT10, CTFT15, and CTFT20. Six of the other strategies, SUSPICIOUS TIT FOR TAT, TAT FOR TIT, DTFT05, DTFT10, DTFT15, and DTFT20, were constructed to start runs of mutual recrimination. In a computer program written in Visual Basic by the author, each strategy played ten games of the IPD against each strategy, including itself. Each game had 1,000 rounds. The score from each set of ten games was averaged (arithmetic mean) and used by the program in the evolutionary IPD. Nine of the ten variants of TIT FOR TAT performed better than TIT FOR TAT in the evolutionary IPD. The only variant that did not perform better than TIT FOR TAT was TIT FOR TWO TATS. This was because it was too forgiving. The "nasty" strategies take more advantage of TIT FOR TWO TATS than of TIT FOR TAT. CTFT15 performed the best in both the IPD and evolutionary IPD. In the evolutionary IPD, it was followed by CTFT10 and CTFT05. This shows that CTFT15 was the best at ending runs of mutual recrimination in this environment. 
________________________________________
2010 - MA032 
REGIOMONTANUS' ANGLE MAXIMIZATION PROBLEM: ACCOUNTING FOR DISTORTION
William Grant Murray
Christ the King Cathedral High School, Lubbock, TX

The purpose of this project was to find out if people would always account for distortion when viewing a picture above eye level. To perform this project, a picture was set above all test subject's eye levels and subjects were asked to stand where they would stand most naturally to view the picture. The measurement between the wall and their position as well as the distances from their eye level to the top and bottom of the picture would allow me to tell if they accounted for distortion or not and to what degree that they did. My results showed that everyone accounted for distortion, not just size, however no one accounted to the same degree. Therefore the weights between the size and distortion are varied according to personal preferences of the individual. 
________________________________________
2004 - MA032 
A PROOF OF SEYMOUR'S CONJECTURE FOR ALL ORIENTED GRAPHS
Brett Alexander Harrison
Half Hollow Hills High School West, Dix Hills NY, United States

The objective of this paper is to provide a proof of Seymour's Conjecture for all oriented graphs. In 1993, Seymour conjectured that "every oriented graph G has a vertex whose outdegree in G squared is at least twice its outdegree in G." In 1995, Dean and Latka proved Seymour's Conjecture for the special cases of regular tournaments, almost regular tournaments, and tournaments with minimum outdegree less than or equal to 5. In 1996, Fisher proved Seymour's Conjecture for the larger yet still mainly restricted realm of all tournaments. In 2003, Mackey expanded this realm to include some regular oriented graphs. This paper provides a unique method that proves Seymour's Conjecture for all oriented graphs. This method also strengthens the conjecture by proving that certain vertices of minimum outdegree are expected to at least double in outdegree in the square of the graph. The study of the squares of oriented graphs holds important applications in the area of structural rigidity in three-dimensional structures. 

Awards won at the 2004 ISEF
First Award of $1,000 - American Mathematical Society
________________________________________
2006 - MA032 
ON THE REDUCIBILITY OF CYCLOTOMIC POLYNOMIALS OVER FINITE FIELDS
Brett Alexander Harrison
Half Hollow Hills High School West, Dix Hills, New York, 11746

The irreducibility of a polynomial in a prime modulus implies irreducibility over the rationals. However, the converse is certainly not true. In fact, there are some polynomials that are irreducible over the rationals yet reducible in every prime modulus. In this paper, it is shown that the nth cyclotomic polynomial reduces modulo all primes if and only if the discriminant of the nth cyclotomic polynomial is a square. Applications of cyclotomic polynomials to the fast Fourier Transform (FFT) and to digital signal processing in general are then explored. Finally, further questions are posed about the specific factorization of cyclotomic polynomials over finite fields in relation to the discriminant. 

Awards won at the 2006 ISEF
Second Award of $500 - American Mathematical Society
________________________________________
2004 - MA032 
A PROOF OF SEYMOUR'S CONJECTURE FOR ALL ORIENTED GRAPHS
Brett Alexander Harrison
Half Hollow Hills High School West, Dix Hills NY, United States

The objective of this paper is to provide a proof of Seymour's Conjecture for all oriented graphs. In 1993, Seymour conjectured that "every oriented graph G has a vertex whose outdegree in G squared is at least twice its outdegree in G." In 1995, Dean and Latka proved Seymour's Conjecture for the special cases of regular tournaments, almost regular tournaments, and tournaments with minimum outdegree less than or equal to 5. In 1996, Fisher proved Seymour's Conjecture for the larger yet still mainly restricted realm of all tournaments. In 2003, Mackey expanded this realm to include some regular oriented graphs. This paper provides a unique method that proves Seymour's Conjecture for all oriented graphs. This method also strengthens the conjecture by proving that certain vertices of minimum outdegree are expected to at least double in outdegree in the square of the graph. The study of the squares of oriented graphs holds important applications in the area of structural rigidity in three-dimensional structures. 

Awards won at the 2004 ISEF
Scholarship award of $5,000 per year for 4 years - Lewis & Clark College
Second Award of $1,500 - Mathematics - Presented by Panasonic Consumer Electronics Company
________________________________________
2006 - MA032 
ON THE REDUCIBILITY OF CYCLOTOMIC POLYNOMIALS OVER FINITE FIELDS
Brett Alexander Harrison
Half Hollow Hills High School West, Dix Hills, New York, 11746

The irreducibility of a polynomial in a prime modulus implies irreducibility over the rationals. However, the converse is certainly not true. In fact, there are some polynomials that are irreducible over the rationals yet reducible in every prime modulus. In this paper, it is shown that the nth cyclotomic polynomial reduces modulo all primes if and only if the discriminant of the nth cyclotomic polynomial is a square. Applications of cyclotomic polynomials to the fast Fourier Transform (FFT) and to digital signal processing in general are then explored. Finally, further questions are posed about the specific factorization of cyclotomic polynomials over finite fields in relation to the discriminant. 

Awards won at the 2006 ISEF
Second Award of $1,500 - Mathematics - Presented by Lucent Technologies
________________________________________
2009 - MA032 
PARAMETERIZING KNOTS WITH CHEBYSHEV POLYNOMIALS
Jenna Kay Freudenburg
Kalamazoo Area Math and Science Center, Kalamazoo, MI

Recently, it has been the goal of many researchers to construct knot parameterizations using polynomials. Typically, this is difficult to achieve, and the resulting polynomials are complicated. This project instead approaches the problem by defining a family of polynomial embeddings using Chebyshev polynomials and determining which knots result. Specifically, we consider mappings f from the real line to threespace by f = (Ti , Tj , Tk), where Ti , Tj , Tk are Chebyshev polynomials of degree i, j, k. <br><br>The first main result of the project determines precisely which (i, j, k) yield embeddings. The second main result provides a criterion for determining whether two embeddings (i, j, k) are algebraically equivalent. These findings allow us to narrow the range of specific triples (i, j, k) to investigate. A table of such triples is compiled for small degrees.<br><br>The results of this project have potential applications in knot theory and in many other scientific fields, including biology, genetics, chemistry, physics, computer science, cryptography, and material science. In particular, these results represent a significant step towards a better method of distinguishing and classifying knots; they may also be used to study enzyme action on DNA, where knot theory is known to play a role. 

Awards won at the 2009 ISEF
Fourth Award of $500 - Mathematical Sciences - Presented by Intel
Tuition Scholarship Award in the amount of $8,000 - Office of Naval Research on behalf of the United States Navy and Marine Corps
________________________________________
2006 - MA032 
ON THE REDUCIBILITY OF CYCLOTOMIC POLYNOMIALS OVER FINITE FIELDS
Brett Alexander Harrison
Half Hollow Hills High School West, Dix Hills, New York, 11746

The irreducibility of a polynomial in a prime modulus implies irreducibility over the rationals. However, the converse is certainly not true. In fact, there are some polynomials that are irreducible over the rationals yet reducible in every prime modulus. In this paper, it is shown that the nth cyclotomic polynomial reduces modulo all primes if and only if the discriminant of the nth cyclotomic polynomial is a square. Applications of cyclotomic polynomials to the fast Fourier Transform (FFT) and to digital signal processing in general are then explored. Finally, further questions are posed about the specific factorization of cyclotomic polynomials over finite fields in relation to the discriminant. 

Awards won at the 2006 ISEF
Tuition Scholarship Award in the amount of $8,000 - Office of Naval Research on behalf of the United States Navy and Marine Corps.
________________________________________
2007 - MA033 
RIDGES, FIBONACCI TILING, AND DIMENSIONS: STATISTICAL RELATIONSHIPS OF CRUMPLED PAPER
Bryson Jay Williams
Miami High School, Miami, OK, United States

Crumpling is a poorly understood phenomenon even though researchers have studied the physics and some mathematical views of the crumpling process. This project evaluates the statistical relationships of crumpled paper. The different variables tested were the number of ridges, compression radius, mean length of the ridges, and the radius divided by the surface area. The second part of this investigation tested the location of the most concentrated amount of energy held within a piece of crumpled paper using a Fibonacci Tiling Sequence. First, it was hypothesized that there would be a correlation of .900 or better for all the variables tested. The variables tested include: Ridges vs. Radius, Ridges vs. Mean Length, Radius vs. Mean Length, Radius divided by Surface Area vs. Ridges, Radius divided by Surface Area vs. Mean Length, and every individual Fibonacci tile Ridges vs. Radius. The variable Fibonacci Tile Ridges vs. Radius is tested to identify the concentration of energy held within the piece of paper. The second hypothesis is that the Fibonacci tiling will identify which area holds the concentrated energy. Crumpling must be standardized and data collected must be accurate in order to receive the best result. In conclusion, the data suggested that the hypothesis for investigation one can be accepted with variables Radius vs. Ridges and Radius divided by Surface Area vs. Mean Length. All other variables were rejected. In hypothesis two, the data was able to identify the concentrated of energy was held along the edges of the crumpled paper, showing the best correlation. 
________________________________________
2004 - MA033 
SHIFTS AND TRANSFORMATIONS IN THE MANDELBROT SET
Anthony Matteo DeGennaro
Bishop McNamara High School; Forestville, Maryland, USA

This project is centered around a visual geometric shape called the Mandelbrot Set. This is a complex graph of c values denoted by the equation f(z) = z^2 + c. The graph assumes that the initial z value is always zero. The coordinates are colored according to whether the represented c value yields orbits that diverge to infinity or not. Black represents orbits that do not diverge to infinity; however, any color scale can be used to denote orbits that do diverge. The color used for such a point is contingent upon how many iterations are present before it becomes apparent that the orbits are diverging (if the orbits grow greater than the absolute value of two, one can assume that the orbits diverge). It is virtually impossible to graph this Set unaided, therefore, computer programs are used. The program that I used was Fractal Explorer.<br><br> Curiosity led to my problem: “What is the effect of the complex variables a and b on orbital values and the Mandelbrot Set?” Restated, I wanted to investigate Mandelbrot Set transformations by adding the complex variables a and b to the base equation, forming f(z) = (z + a)^2 + (c + b). I hypothesized that b would shift the Set along the real and imaginary axes (depending upon which value was manipulated), while a would transform the shape of the Mandelbrot Set. My testing supported my hypothesis, although it did not include some apparent behavior, such as the simultaneous shifting of the Set for the variable a. 
________________________________________
2003 - MA033 
WHEN DIRECTION VANISHES - WALKING STRAIGHT OR IN CIRCLES
Andrea Lynn Axtell
James Bowie High School, Austin, Texas, United States

News reports and children's literature have noted people who are lost or do not know where they are going...walk in circles. This experiment was designed to determine what path a person would take when visual clues are eliminated and all direction vanishes. Thirty-two subjects were blindfolded and asked to walk forward in a straight line from one goal post to another on a football field. The walk path was plotted every five yards until the person reached the opposite goal line or sideline. The distance between the center hatch mark and respective marker was measured and recorded. Only control subjects walked in a straight line. All test subjects angled to the sideline and with only one exception curved in the direction of their dominant side. Right-handed subjects curved to the right sideline, left-handed subjects curved to the left sideline. The point at which subjects deviated from the centerline and length of the arc was correlated to the person's height. Those who were shorter walked in a tighter arc than those who were taller. The data identified no statistical differences between males and females. While the experiment did not allow subjects to continue indefinitely, a best fit curve was determined to calculate the radius, diameter, and circumference of the circle they started. Mathematical models indicated subjects would likely continue the walk path they started and eventually walk in an ellipse or circle. Research indicates this is likely affected by brain dominance, physiology, and equilibrium. This research has applications for travelers, sports enthusiasts, and military personnel. While one should carry a compass or map when traveling in unknown territories, strategies and tactics should be developed to help prevent walking in circles.<br><br> 

Awards won at the 2003 ISEF
First Award of $500 and a plaque - American Statistical Association
________________________________________
2005 - MA033 
MINIMIZING INCONVENIENCE IN DIRECTED MESHES: A MODEL FOR TRAFFIC OPTIMIZATION
Matthew Huang
Scarsdale High School, Scarsdale, NY, USA

This research involves minimizing inconvenience in directed meshes, and applies to traffic. The graph theoretical model for an optimal road network has tried to capitalize on the advantages of one-way streets, speed, safety and capacity, while minimizing the inconvenience that they cause by only allowing one direction of travel. Within the model, directed edges serve as one-way streets. After proving that all strongly connected, directed meshes contain a directed 4-cycle - through a series of lemmas reducing an arbitrary mesh to a directed 4-cycle - a direct relationship between inconvenience and the number of directed 4-cycles was found. When compared to a known efficient road network, that of Manhattan, the model was found to be theoretically more efficient, however, practically the model fails. Navigation within the model is confusing and it lacks a means for long distance travel. The model fails because it assumes that intersections are non-preferential - that people will not care whether they turn left, right, or go straight - which they do. Thus, the model can be applied to networks where intersections are non-preferential such as the internet and various circuitry. 

Awards won at the 2005 ISEF
Second Award of $500 U.S. Savings Bond - Ashtavadhani Vidwan Ambati Subbaraya Chetty (AVASC) Foundation
$3,000 First Place Scholarship Award in Mathematics - Robert Luby, Jr. of IBM
________________________________________
2003 - MA034 
DATA DETECTIVES: A SECOND YEAR STUDY OF CRC GENERATOR POLYNOMIALS: 32 BIT
Alice Wan Chai, Plano Senior High School 
Plano, Texas, U.S.A

Cyclic Redundancy Check, a parity check, uses an error detection method of modulo 2 polynomial division in data transmission. The generator polynomial, by definition an irreducible and primitive polynomial, usually contains the factor (x+1) to detect for odd/even errors. Once multiplied, the polynomial should contain as few terms as possible and be cyclic at 2^(n-1). <br><br>This year, the 32-bit primitive polynomials were analyzed according to the devised method for 16-bit generator polynomials to search for different 32-bit generator polynomials. After running all degree 31 primitive polynomials through a simple C program to transcribe the polynomials from hexadecimal to binary, multiplying by (x+1) and counting the terms, and checking for irreducibility, forty-seven different generator polynomials were found. Sample polynomials were proven to be cyclic and comparable to the current standard by detecting odd/even errors, burst errors, and combination errors. <br><br>Once the IEEE-802 standard and the CRC32C were verified to be primitive from the degree 32 primitive list, the nature of the IEEE-802 standard and the CRC32C were analyzed to find the reason behind the current popular use, a matter of transfer method that uses an inverse remainder before sent and a unique remainder upon receipt for checking. <br><br>Currently, Ethernet packets use the IEEE-802 standard, but other high-speed data transmissions also transfer in 32-bits. By increasing the field of generator polynomials, different systems can increase their efficiency in data transmission. 

Awards won at the 2003 ISEF
Full-tuition scholarships - Drexel University
________________________________________
2004 - MA034 
DETERMINANTAL SEQUENCES OF POLYNOMIALS
Samuel Mohun Bhagwat
Churchill High School, Livonia, MI, USA

This paper proves a conjecture of Matthew Satriano on the form of doubly infinite sequences {a_n(x)} = {.., a_-1(x), a_0(x), a_1(x), ....}, where all a_n are polynomials a_i(x) with coefficients in a field F such that a_k(x)a_k+3(x) - a_k+1(x)a_k+2(x) = 1 for all k; such sequences are called determinantal. The study of examples led Satriano to conjecture that each such sequence contains three consecutive terms of the form<br><br> g(x),(-2g(x)-b)/(b^2), g(x)+b <br><br> <br><br>where g(x) is a non-constant polynomial and b is a nonzero constant (and F must have characteristic not equal to 2). <br><br>For each g(x) and b, there is one such determinantal sequence, up to shift. An example of such a sequence, taking g(x) = x^2 + 2, b = 1, and F = Q is: <br><br> .., -2x^2 - 1, x^2 + 1, -2x^2 - 3, x^2 + 2, -2x^2 -5, x^2 + 3, -2x^2 - 7, x^2 + 1, -2x^2 - 3,... <br><br>We prove Satriano's conjecture, and the extra understanding obtained in doing this proof enables us to give simple proofs of several other results.<br><br> 

Awards won at the 2004 ISEF
Tuition scholarship of $5,000 per year for 4 years for a total value of $20,000 - Indiana University
________________________________________
2003 - MA034 
DATA DETECTIVES: A SECOND YEAR STUDY OF CRC GENERATOR POLYNOMIALS: 32 BIT
Alice Wan Chai, Plano Senior High School 
Plano, Texas, U.S.A

Cyclic Redundancy Check, a parity check, uses an error detection method of modulo 2 polynomial division in data transmission. The generator polynomial, by definition an irreducible and primitive polynomial, usually contains the factor (x+1) to detect for odd/even errors. Once multiplied, the polynomial should contain as few terms as possible and be cyclic at 2^(n-1). <br><br>This year, the 32-bit primitive polynomials were analyzed according to the devised method for 16-bit generator polynomials to search for different 32-bit generator polynomials. After running all degree 31 primitive polynomials through a simple C program to transcribe the polynomials from hexadecimal to binary, multiplying by (x+1) and counting the terms, and checking for irreducibility, forty-seven different generator polynomials were found. Sample polynomials were proven to be cyclic and comparable to the current standard by detecting odd/even errors, burst errors, and combination errors. <br><br>Once the IEEE-802 standard and the CRC32C were verified to be primitive from the degree 32 primitive list, the nature of the IEEE-802 standard and the CRC32C were analyzed to find the reason behind the current popular use, a matter of transfer method that uses an inverse remainder before sent and a unique remainder upon receipt for checking. <br><br>Currently, Ethernet packets use the IEEE-802 standard, but other high-speed data transmissions also transfer in 32-bits. By increasing the field of generator polynomials, different systems can increase their efficiency in data transmission. 

Awards won at the 2003 ISEF
Third Award of $1,000 - Mathematics - Presented by Panasonic Consumer Electronics Company
________________________________________
2006 - MA034 
CLOSED FORMS FOR QUADRATIC RECURSIONS
Aleksandr Arkhipov
Dr. Ronald E. McNair Academic High School, Jersey City, NJ, USA

Consider a hypothetical model of a population of beetles in which the population for next year depends only on the population this year. We can express this relationship as Q{n+1}=q(Q{n}), where Q{n} is the population one year and Q{n+1} is the population the following year. Starting from an initial population Q{0}, we can find Q{n}, the beetle population at year n, by applying the function q(x), called the recursion, n times. Finding a closed-form representation for Q{n} in terms of Q{0} and n would to allow us to calculate Q{n} in a single step. For example, if q(x) is a linear function, q(x)=mx+k, then Q{n}=m^n*Q{0}+k*(m^n-1)/(m-1), a completely general solution. <br><br> I focused on finding similar closed forms for quadratic recursions, which are of the form q(x)=ax^2+bx+c with given numerical coefficients a, b, and c. I demonstrated that recursions that have the same respective values of (b-1)^2-4ac are linear conjugates, and generate sequences that are linked by a linear transformation, reducing the set of linearly-distinct quadratic recursions into a one-parameter class.<br><br> I exploited relationships involving rational, trigonometric, and hyperbolic functions to show how certain quadratic recursions are topological conjugates, allowing me to express their closed forms. However, by using Euler's Formula and the power-reducing formulas, I demonstrated that this method could only generate closed forms for recursions that have the same value of (b-1)^2-4ac, and are thus in effect equivalent. The fact that I was able to find closed forms only for a narrow class of quadratic recursions mirrors the qualitative leap in complexity from linear recursions to quadratic ones.<br><br> 

Awards won at the 2006 ISEF
Third Award of $1,000 - Mathematics - Presented by Lucent Technologies
________________________________________
2004 - MA034 
DETERMINANTAL SEQUENCES OF POLYNOMIALS
Samuel Mohun Bhagwat
Churchill High School, Livonia, MI, USA

This paper proves a conjecture of Matthew Satriano on the form of doubly infinite sequences {a_n(x)} = {.., a_-1(x), a_0(x), a_1(x), ....}, where all a_n are polynomials a_i(x) with coefficients in a field F such that a_k(x)a_k+3(x) - a_k+1(x)a_k+2(x) = 1 for all k; such sequences are called determinantal. The study of examples led Satriano to conjecture that each such sequence contains three consecutive terms of the form<br><br> g(x),(-2g(x)-b)/(b^2), g(x)+b <br><br> <br><br>where g(x) is a non-constant polynomial and b is a nonzero constant (and F must have characteristic not equal to 2). <br><br>For each g(x) and b, there is one such determinantal sequence, up to shift. An example of such a sequence, taking g(x) = x^2 + 2, b = 1, and F = Q is: <br><br> .., -2x^2 - 1, x^2 + 1, -2x^2 - 3, x^2 + 2, -2x^2 -5, x^2 + 3, -2x^2 - 7, x^2 + 1, -2x^2 - 3,... <br><br>We prove Satriano's conjecture, and the extra understanding obtained in doing this proof enables us to give simple proofs of several other results.<br><br> 

Awards won at the 2004 ISEF
Fourth Award of $500 - Mathematics - Presented by Panasonic Consumer Electronics Company
________________________________________
2003 - MA034 
DATA DETECTIVES: A SECOND YEAR STUDY OF CRC GENERATOR POLYNOMIALS: 32 BIT
Alice Wan Chai, Plano Senior High School 
Plano, Texas, U.S.A

Cyclic Redundancy Check, a parity check, uses an error detection method of modulo 2 polynomial division in data transmission. The generator polynomial, by definition an irreducible and primitive polynomial, usually contains the factor (x+1) to detect for odd/even errors. Once multiplied, the polynomial should contain as few terms as possible and be cyclic at 2^(n-1). <br><br>This year, the 32-bit primitive polynomials were analyzed according to the devised method for 16-bit generator polynomials to search for different 32-bit generator polynomials. After running all degree 31 primitive polynomials through a simple C program to transcribe the polynomials from hexadecimal to binary, multiplying by (x+1) and counting the terms, and checking for irreducibility, forty-seven different generator polynomials were found. Sample polynomials were proven to be cyclic and comparable to the current standard by detecting odd/even errors, burst errors, and combination errors. <br><br>Once the IEEE-802 standard and the CRC32C were verified to be primitive from the degree 32 primitive list, the nature of the IEEE-802 standard and the CRC32C were analyzed to find the reason behind the current popular use, a matter of transfer method that uses an inverse remainder before sent and a unique remainder upon receipt for checking. <br><br>Currently, Ethernet packets use the IEEE-802 standard, but other high-speed data transmissions also transfer in 32-bits. By increasing the field of generator polynomials, different systems can increase their efficiency in data transmission. 

Awards won at the 2003 ISEF
Award of $3000 in savings bonds, a Certificate of Achievement and a gold medallion. - U.S. Army
Second Awards of $1,500 - U.S. Air Force
________________________________________
2004 - MA035 
THE EXISTENCE OF IDENTITIES FOR CYCLICAL INTEGRAL FUNCTIONS
Kaylin Embrey
Mecosta Osceola Math Science Technology Center, Big Rapids, MI, USA 

This study involved the examination of cyclical integral functions generated by a simple number puzzle. The puzzle that is used is to take two integers and multiply one of the integers by the units place of the other integer, then add the digit in the 10’s place to the product. The mathematical function that this puzzle describes is termed <a, b>. For integers a and b we defined <a, b> as the set of values of b(i) for i = 1 to n, where n is the number of bi’s before the function repeats itself. To generate b(i) use the function<br><br>b(i) = b(i-1)(mod10) a + int(b(i-1)/10),<br><br>where b(i-1)(mod10) is understood to be the least residue of b(i-1) modulus 10. <br><br>Using this algorithm leads to many cyclical patterns. At the beginning of this study charts were completed. Single digit numbers were examined first. There are 81 possible combinations to test within these limits. However, it was observed quickly that integers that were included in already completed set generated the same set. This lead to the observation that the sets generated were finite fields. <br><br>Another pattern arose. Certain pairs of numbers generated a in the first iteration. These pairs of numbers were defined as the Identities. These observations lead to a conjecture.<br><br>Conjecture: All of the “Identities” to the function <a, b> are one less than 10 times the a value. Also the numbers included in the charts would be all the integers up to one less than the “Identity”.<br><br><br><br><br><br><br> 
________________________________________
2008 - MA035 
SUDOKU UNIQUENESS: AN INVESTIGATION ON BIPARTITE GRAPHS
Teresa Dan Tuyen Le
Westmoore High School, Oklahoma City, OK

This study investigated the relationship between the given entries of a Sudoku puzzle and uniqueness through the use of bipartite graphs. A unique Sudoku puzzle has only one solution. Unique and non-unique Sudoku puzzles were converted into bipartite graphs to examine relationships between the 81 spaces in a Sudoku puzzle, which were considered as vertices. The given entries in Sudoku puzzles were converted into the vertices of bipartite graphs according to the space each entry occupied. Characteristics of each Sudoku bipartite graph were translated into number sequences for examination. Relationships among the number sequences of unique and non-unique Sudoku puzzles were discovered. Conditional statements were formed from the relationships. The conditional statements signified the existence of a relationship between the given entries of a Sudoku puzzle and uniqueness. The findings of this study may be a basis for the creation of new methods to determine the uniqueness of a Sudoku puzzle. 
________________________________________
2007 - MA035 
INVESTIGATIONS IN ANALYSIS
Akhil Mathew
Madison High School, Madison, New Jersey, USA

This work is a collection of results I have derived, primarily dealing with analysis, organized into six chapters. In Chapter 1, I discuss some identities that one may derive by expanding certain functions in Fourier series and applying Jordan's test. In Chapter 2, I provide a proof of Wallis' product as well as some other infinite products for pi using a gamma function identity. In Chapter 3, I apply Hille's theorem in functional analysis to certain Lp spaces and the space of continuous functions on [0,1], thus deriving Taylor-like formulae. In Chapter 4, I supply a correction to a proof of Julia's corollary to Picard's theorem on essential singularities. For Chapter 5, I look at some applications of Baire's theorem to certain function spaces to show that, loosely speaking, most functions are badly behaved. Finally, in Chapter 6 I discuss a generalization of the classical Darboux integral to abstract spaces by first giving the basic definitions and then proceeding to prove some elementary properties. Generalizations of many classical results, ranging from Lebesgue’s differentiation theorem to a change-of-variables formula, form the remainder of the chapter. 

Awards won at the 2007 ISEF
Third Award of $1,000 - Mathematical Sciences - Presented by Acatel-Lucent
________________________________________
2003 - MA035 
PERFECT MUSICAL HARMONY: A MATHEMATICAL ANALYSIS OF FOUR HISTORICAL TUNINGS
Michael F. Page
Pingry School, Martinsville, NJ, USA

In Western music, a musical interval defined by the frequency ratio of two notes is considered pleasing or consonant when the ratio is composed of small integers. Perfect Harmony is the division of an octave into 12 notes, each of which can be used to create six other consonant intervals. The purpose of this study was to determine which of four historical tunings, or definitions of a 12-note scale, provides the best approximation to Perfect Harmony, which has no exact solution. A population of 72 musical intervals for each tuning (5-Limit Just Intonation, Quarter Comma Meantone Temperament, Well Temperament (Werckmeister III), and Equal Temperament) was compared to Perfect Harmony. My hypothesis was that Equal Temperament is as good as or better than the other tunings in approximating Perfect Harmony, not just a method used for the convenience of instrument makers as critics have suggested. The variables analyzed were the deviations and absolute deviations of each tuning from Perfect Harmony. Equal Temperament was as good as or better than the other tunings for 7 of 10 population parameters and all 6 distance measures examined. All four tunings were found to have a zero mean deviation, an unexpected result. This led to my proving that any tuning will have a zero mean deviation from Perfect Harmony. I also proved that any tuning has mean values for every musical interval that exactly equal the values for Equal Temperament, which acts like a “mean temperament.” Overall, the analysis showed that my hypothesis is valid. 

Awards won at the 2003 ISEF
Award of $250 for Excellence in a Written Report - Society for Technical Communication
________________________________________
2010 - MA036 
BATTER UP
Demi Shareece McCoy
Dr. Henry A. Wise, Junior High School, Upper Marlboro, MD

The purpose of this project was to prove whether or not batting statistics are affected significantly by baseball stadium dimensions in order to determine if the millions of dollars spent on new stadiums and stadium improvements each year have been worth paying for. My original theory was that baseball stadium dimensions had a fairly considerable effect on batting statistics. However, after researching the topic and examining both stadium dimensions and MLB park factors closely, I hypothesized that the stadium dimensions do not affect batting statistics with significance. I experimented with MLB batting statistics from the 2009 season by running a series of fourteen paired t-tests performed at a 5% significant level. There were seven tests performed for both the National and American League. The tests evaluated individual player statistics from the 2009 season with a focus on runs, home runs, hits, double hits, triple hits, bases on balls, and averages at their home field and at the control fields used (Chase Field and Oriole Park at Camden Yards). The p-values produced from the t-statistics found for both leagues were 0.4012, 0.3979, (runs) 0.8596, 0.0850 (home-runs) 0.5772, 0.5274, (hits) 0.1921, 0.7664, (double hits) 0.3222, 1.000, (triple hits) 0.1711, 0.5863, (bases on balls) and 0.0718, 0.6098, (averages). Since all the p-values were greater than 5% I failed to reject my null hypothesis and concluded that the baseball stadium dimensions do not affect batting statistics significantly. 
________________________________________
2007 - MA036 
REPTEND PRIMES IN ALTERNATE BASES
Eric Nathan Sporkin
Syosset High School, Syosset NY, United States of America

Reptend Primes are defined as primes whose reciprocals' nonfraction (such as decimal) expansion has one less repeating digit than the value of the prime. Using computer technology, I created a chart of Reptend Primes in various number bases. The program uses the formula b^k = 1 (mod p), where b, p and k are positive integers representing the number base, a prime number, and the modulo order of b mod p respectively. If the least value of k is p - 1 then b is a primitive root of p and p is a Reptend Prime in base b. I proved that Reptend Primes appear in a periodic fashion across bases. For example, 2 is a Reptend Prime in every odd base. I also proved any base b^n, where b and n are positive integers and n>1, has fewer Reptend Primes than a base not of this form. A base b^2 can have no odd Reptend Primes at all. 
________________________________________
2003 - MA036 
GENERALIZED EXPANSIONS OF THE FIBONACCI SEQUENCE AND THEIR CONVERGENTS
Drew Lewis Matteson
Randall High School, Amarillo, TX, United States

Many physical phenomena and most patterns of growth are connected intrinsically to the golden mean, Phi. One method of approximating Phi is to take the nth term of the Fibonacci sequence and divide it by the (n-1)th term. This value, the convergent of the sequence, gradually approaches Phi as n approaches infinity. This project set out to prove theorems relating to expansions on the Fibonacci sequence. Experimentation with and research about the Fibonacci and n-acci sequences led to these hypotheses: 1. undiscovered expansions exist upon the Fibonacci sequence, 2. the convergents of the n-acci, d-gap, and other unstudied expansions have real world applications relating to life and physical occurrences, and 3. sequences based on the summation of an infinite number of previous terms or a finite number of terms separated by an infinite number of unsummed elements have calculable convergents that govern all sequences defined by summing previous terms. Using techniques of analytical algebra, geometry and calculus, the convergents of infinite-acci and the infinite-gap sequences were proven to be calculable in three special-case theorems. Also, a method of calculating subsequent digits without knowing the recursive properties of a sequence was determined for all sequences classified by n-acci or d-gap. Practical implications of this research are discussed for the fields of cosmology, biology and sequential analysis. 

Awards won at the 2003 ISEF
Scholarship award of $5,000 per year for four years - Oberlin College
________________________________________
2007 - MA037 
BASIC POLY VDW
Darnell Primus
Eleanor Roosevelt HS, Greenbelt, MD, United States

Van der Waerden Theorem states that there is no such thing as complete disorder. The purpose of this experiment is to create a simple formula that will serve as an upper bound, whereby any two coloring of the naturals from one through the upper bound will contain two numbers that are the same color; a perfect square apart. I wanted to design two boxes a perfect square away from block number one to be separated by a minimum distance of d squared. Using a variation of Pythagorean Theorem, I was able to find a formula for the length l in terms of our variables d and c. All three of the key boxes are separated by a perfect square that is at least d squared. This is in correlation with the pigeonhole principle, which states that at least two of the boxes are the same color and therefore create the perfect square progression needed. Every once in while I found that certain d total row bounds were obtained from the row bound of (d+x). Sometimes when then whole numbers were obtained using the formula they did not prove to be true answers as the minimum separation was (n-c) squared instead of d squared. Plans for the future include monitoring the effects on the value l of increasing the number of colors and changing the polynomial used for separation. This project provides the basis for Ramsey Theory, which has various real world applications.<br><br> 
________________________________________
2008 - MA037 
MODELING TRAFFIC WITH MATHEMATICS: OPTIMIZING INTERSECTIONS AND FUEL EFFICIENCY
Andrew Bryce Larsen
Waterford School, Sandy, UT

My main objective is to determine the extent to which intersection and vehicle optimizations for different variables differ from one another. Traffic engineers often seek to "optimize" the timing of lights at signalized intersections; however, what should be optimized is often unclear. A single intersection can be optimized for more than one thing - an intersection is highly complex, organic, and variable. Here are a few examples of the complexity of an intersection: Cars move through an intersection from many directions, some cars turn and others go straight. Distances between cars vary greatly and speeds of different cars vary greatly. Cars accelerate and decelerate through intersections. Cars line up at red lights while other lanes have green lights. These variables and many more must be taken into account to provide only one optimized output: a sequence of red, yellow and green lights. <br><br>In addition to studying how we can optimize intersections, I studied methods of braking and accelerating in cars that optimized fuel efficiency. I found that by shifting some of the focus to finding “smart” driving patterns rather than just “smart” intersections, the results of optimization could be more immediate and accessible. 
________________________________________
2009 - MA037 
GRAPH CROSSINGS AND CYCLIC PERMUTATIONS: TOWARDS A PROOF OF ZARANKIEWICZ’S CONJECTURE
Joshua Vekhter
Williamsville East High School, East Amherst, NY

It is known that the general problem of finding graphs with the fewest number of crossings is computationally hard, and tight bounds on crossing numbers do not exist for even the most basic families of graphs. There is a long standing open conjecture by Zarankiewicz which suggests a lower bound on the minimum number of crossings in a complete bipartite graph. This research approached this conjecture by using rotation systems to define particular drawings of graphs as sets of cyclic permutations. The research defined structures that govern which flips, or pairwise interchanges between elements, need to be performed in order to make two cyclic permutations equivalent. Within these structures, operations to generate equivalent sequences of flips were defined. These operations formed the basis of a polynomial time reduction algorithm that finds the shortest sequence of flips between any two cyclic permutations. Bounds on the lengths of minimal flip sequences were also established. These bounds were then used to develop an approach to Zarankiewicz’s conjecture. The structures developed in this project pertain to graph crossing numbers and are applicable to computer chip layout problems. 

Awards won at the 2009 ISEF
First Award of $1,000 - American Mathematical Society
________________________________________
2006 - MA037 
GENERALIZED 3X+1 DYNAMICAL SYSTEMS UNDER SEMIGROUP ACTION
Daniel Zi Wang
Plano Senior High School, Plano, Texas, United States of America

The Collatz conjecture, proposed in 1937 by Lothar Collatz, deals with the iterates of the piecewise function - T(x) = x/2 when x is even, and (3x+1)/2 when x is odd. This conjecture states that for any positive integer x, there exists some k such that T^k(x) = 1, where T^k is the k-fold composition of T(x). This function, requiring simple algebra to understand, is misleadingly simple. After 70 years, this conjecture has remained unsolved. Therefore, we provide a possible approach to the solution of this problem as well as an equivalent conjecture. However, a limited study over the specific 3x+1 conjecture can be encompassed as well as expanded upon through the use of the generalized Collatz functions. We use two such generalizations to produce the generalized 3x+ 1 semigroups. We then move on to an attempt at simplifying the semigroups by proving that a sub-semigroup is isomorphic to a structurally predictable semigroup. Many of the directions that we take are made in order to simplify and better understand the properties of these conjectures. Thus, we move to group structure, where we determine the actual structure of these dynamical systems generated by the generalized 3x+1 functions. Furthermore, we determine density, which infers minimality, as well as the chaotic nature of these dynamical systems through transitivity, sensitivity, and density of the periodic points. Finally, we write the Collatz conjectures in the language of the backwards orbit and tie back this language with the initial approach for the solution of the problem. 

Awards won at the 2006 ISEF
Tuition Scholarship of $105,000 - Drexel University
________________________________________
2004 - MA037 
A PRIME NUMBER DETERMINING ALGORITHM IN POLYNOMIAL TIME
Anarghya A Vardhana
Jesuit High School, Portland OR, USA

Determining whether a given integer is prime or not has been a mystery that has eluded mankind since the times of Euclid and Eratosthenes. This project is based upon a unconditional deterministic polynomial-time algorithm. The algorithm is distinguished by its claim to be deterministic and in polynomial time. Most other primality testing algorithms are probabilistic, possibly resulting in a 2% error. As the complexity of modern day technology increases (RSA cryptosystem), it is not prudent to substitute efficiency of probabilistic algorithms, for accuracy, only obtained in a deterministic algorithm. <br><br> The polynomial time characteristics of the algorithm make it quite efficient. Presently, the algorithm's time complexity runs in the order of six, that is, the number of digits in the input raised to the sixth power number of steps to complete the computation. If the number of steps can be brought down to three, this algorithm will be practical.<br><br> The student, through detailed analysis, charts, and calculations, will thoroughly understand the time complexity of the algorithm and will make necessary changes and modifications to explain it in simplistic terms. The student will also numerically explore a conjecture presented by the algorithm, which, if proven, will yield a faster time complexity for the algorithm. The student will explore her idea, that the prime-determining algorithm could also supplement a prime generator by testing consecutive numbers. The algorithm will be implemented using Mathematica. The student hopes to improve the time complexity, or at least have enough mathematical background to prove that the claims of the algorithm are indeed true. <br><br> <br><br> 

Awards won at the 2004 ISEF
Scholarship award of $5,000 per year for 4 years - Lewis & Clark College
Scholarship award of $5,000 per year for four years - Oregon State University
________________________________________
2003 - MA037 
THE TOPOLOGY OF DISTANCE: ASYMMETRIC FUNCTIONS
George Alexander Khachatryan
Kinkaid School, Houston TX, USA

The goal of the project was to define and develop a theory of cyclical asymmetric distance functions (ADFs). The functions were defined on the analogy of metric functions; the symmetric axiom was removed and replaced by two axioms which together encode a certain circularity to spaces on which an ADF can be defined.<br><br>Examples of ADF-metrizable spaces include the circle, the torus, and the sphere identified with itself at one point. The plane, the sphere, the real number line, and any segment of the real number line are not ADF-metrizable.<br><br>In the course of the research, theorems are proven stating that asymmetric distance functions are preserved under continuous transformation and that every ADF-metrizable function is metrizable in the standard sense; a method for constructing a metric with the suitable induced topology is given. It is proven that no loop on an ADF-metrizable space can be homotopic to the trivial loop. An additional theorem limits the ADF-metrizability of 2-manifolds in terms of simple-connectedness. This result suggests the corollary that every continuous vector field over a sphere contains at least one zero vector, although the result is not proven in the scope of the project. Additional results concerning loop homotopy and algebraic structures are proven in the development of the theory.<br><br>Potential applications of the theory include the study of vector fields on surfaces, classification of embedded 2-manifolds, differential geometry and differential equations. It should be noted that asymmetric distance functions represent something of a generalization of vector fields. 

Awards won at the 2003 ISEF
Second Award of $1,500 - Mathematics - Presented by Panasonic Consumer Electronics Company
________________________________________
2006 - MA037 
GENERALIZED 3X+1 DYNAMICAL SYSTEMS UNDER SEMIGROUP ACTION
Daniel Zi Wang
Plano Senior High School, Plano, Texas, United States of America

The Collatz conjecture, proposed in 1937 by Lothar Collatz, deals with the iterates of the piecewise function - T(x) = x/2 when x is even, and (3x+1)/2 when x is odd. This conjecture states that for any positive integer x, there exists some k such that T^k(x) = 1, where T^k is the k-fold composition of T(x). This function, requiring simple algebra to understand, is misleadingly simple. After 70 years, this conjecture has remained unsolved. Therefore, we provide a possible approach to the solution of this problem as well as an equivalent conjecture. However, a limited study over the specific 3x+1 conjecture can be encompassed as well as expanded upon through the use of the generalized Collatz functions. We use two such generalizations to produce the generalized 3x+ 1 semigroups. We then move on to an attempt at simplifying the semigroups by proving that a sub-semigroup is isomorphic to a structurally predictable semigroup. Many of the directions that we take are made in order to simplify and better understand the properties of these conjectures. Thus, we move to group structure, where we determine the actual structure of these dynamical systems generated by the generalized 3x+1 functions. Furthermore, we determine density, which infers minimality, as well as the chaotic nature of these dynamical systems through transitivity, sensitivity, and density of the periodic points. Finally, we write the Collatz conjectures in the language of the backwards orbit and tie back this language with the initial approach for the solution of the problem. 

Awards won at the 2006 ISEF
Fourth Award of $500 - Mathematics - Presented by Lucent Technologies
________________________________________
2009 - MA037 
GRAPH CROSSINGS AND CYCLIC PERMUTATIONS: TOWARDS A PROOF OF ZARANKIEWICZ’S CONJECTURE
Joshua Vekhter
Williamsville East High School, East Amherst, NY

It is known that the general problem of finding graphs with the fewest number of crossings is computationally hard, and tight bounds on crossing numbers do not exist for even the most basic families of graphs. There is a long standing open conjecture by Zarankiewicz which suggests a lower bound on the minimum number of crossings in a complete bipartite graph. This research approached this conjecture by using rotation systems to define particular drawings of graphs as sets of cyclic permutations. The research defined structures that govern which flips, or pairwise interchanges between elements, need to be performed in order to make two cyclic permutations equivalent. Within these structures, operations to generate equivalent sequences of flips were defined. These operations formed the basis of a polynomial time reduction algorithm that finds the shortest sequence of flips between any two cyclic permutations. Bounds on the lengths of minimal flip sequences were also established. These bounds were then used to develop an approach to Zarankiewicz’s conjecture. The structures developed in this project pertain to graph crossing numbers and are applicable to computer chip layout problems. 

Awards won at the 2009 ISEF
Fourth Award of $500 - Mathematical Sciences - Presented by Intel
________________________________________
2004 - MA037 
A PRIME NUMBER DETERMINING ALGORITHM IN POLYNOMIAL TIME
Anarghya A Vardhana
Jesuit High School, Portland OR, USA

Determining whether a given integer is prime or not has been a mystery that has eluded mankind since the times of Euclid and Eratosthenes. This project is based upon a unconditional deterministic polynomial-time algorithm. The algorithm is distinguished by its claim to be deterministic and in polynomial time. Most other primality testing algorithms are probabilistic, possibly resulting in a 2% error. As the complexity of modern day technology increases (RSA cryptosystem), it is not prudent to substitute efficiency of probabilistic algorithms, for accuracy, only obtained in a deterministic algorithm. <br><br> The polynomial time characteristics of the algorithm make it quite efficient. Presently, the algorithm's time complexity runs in the order of six, that is, the number of digits in the input raised to the sixth power number of steps to complete the computation. If the number of steps can be brought down to three, this algorithm will be practical.<br><br> The student, through detailed analysis, charts, and calculations, will thoroughly understand the time complexity of the algorithm and will make necessary changes and modifications to explain it in simplistic terms. The student will also numerically explore a conjecture presented by the algorithm, which, if proven, will yield a faster time complexity for the algorithm. The student will explore her idea, that the prime-determining algorithm could also supplement a prime generator by testing consecutive numbers. The algorithm will be implemented using Mathematica. The student hopes to improve the time complexity, or at least have enough mathematical background to prove that the claims of the algorithm are indeed true. <br><br> <br><br> 

Awards won at the 2004 ISEF
Winners receive an all expense paid trip to Operation Cherry Blossom in Tokyo, Japan. Each trip winner will also receive $3,000 in savings bonds, $300 from the Association of the United States Army, a gold medallion and a certificate of achievement. - U.S. Army
________________________________________
2003 - MA037 
THE TOPOLOGY OF DISTANCE: ASYMMETRIC FUNCTIONS
George Alexander Khachatryan
Kinkaid School, Houston TX, USA

The goal of the project was to define and develop a theory of cyclical asymmetric distance functions (ADFs). The functions were defined on the analogy of metric functions; the symmetric axiom was removed and replaced by two axioms which together encode a certain circularity to spaces on which an ADF can be defined.<br><br>Examples of ADF-metrizable spaces include the circle, the torus, and the sphere identified with itself at one point. The plane, the sphere, the real number line, and any segment of the real number line are not ADF-metrizable.<br><br>In the course of the research, theorems are proven stating that asymmetric distance functions are preserved under continuous transformation and that every ADF-metrizable function is metrizable in the standard sense; a method for constructing a metric with the suitable induced topology is given. It is proven that no loop on an ADF-metrizable space can be homotopic to the trivial loop. An additional theorem limits the ADF-metrizability of 2-manifolds in terms of simple-connectedness. This result suggests the corollary that every continuous vector field over a sphere contains at least one zero vector, although the result is not proven in the scope of the project. Additional results concerning loop homotopy and algebraic structures are proven in the development of the theory.<br><br>Potential applications of the theory include the study of vector fields on surfaces, classification of embedded 2-manifolds, differential geometry and differential equations. It should be noted that asymmetric distance functions represent something of a generalization of vector fields. 

Awards won at the 2003 ISEF
Scholarship in the amount of $8,000 - U.S. Navy & Marine Corps
________________________________________
2008 - MA038 
FRACTAL COMPLEXITY: UNVEILING IRRATIONAL POINTS AND BEYOND WITHIN THE MANDELBROT SET
Jason Lugo
Dove Science Academy OKC, Oklahoma City, OK

By closely analyzing the Mandelbrot set, specific mathematical applications and irrational points were found within this complex structure. These include the Pi constant, which was discovered after understanding the escape criterion of the set and creating different iteration points using epsilon at different pinch points. After the discovery of this irrational constant, it was compelling to look into the characteristics of other irrational points within this set. Hence, I studied the continuity of mapping a complex value into the Hausdorff Dimension of the Julia sets. That was helpful in defining a partial order on the Mandelbrot set, which aided in defining various properties of other irrational points.<br><br>Furthermore, the structures of the Mandelbrot and Julia sets generated from a general complex cubic iteration were studied, and the relation of the critical points and Julia sets were analyzed. 
________________________________________
2009 - MA038 
AN INVESTIGATION IN TILING
Phil Ross Brockman
PRB Homeschool, Thompson, ND

This project explores the tiling properties of certain geometric shapes. Specifically, this project focuses on semi-tilemakers. A semi-tilemaker is a convex polygon with the property that an arbitrary development of the polygon can be placed side by side with itself to cover the plane with neither overlaps nor gaps between the developments. We obtain the development of a polygon P by taking the union of P and the polygon which is obtained by flipping P over an edge.<br><br>In a late-2007 mathematical paper written by Dr. Jin Akiyama, it was conjectured there were exactly four types of polygonal semi-tilemakers: (1) isosceles right triangles, (2) equilateral triangles, (3) half equilateral triangles, and (4) rectangles.<br><br>The hypothesis of this project originally reflected this conjecture, but later was refined due to unexpected results.<br><br>To prove the hypothesis, models of tilings of developments of triangles and quadrilaterals were made using the programming language Asympote and the program Inkscape. In this project, it was mathematically rigorously proven that:<br><br> (1) all triangles are semi-tilemakers<br><br> (2) a quadrilateral is a semi-tilemaker if and only if it is a parallelogram or a Perfect Right Trapezoid<br><br> (3) no polygon with seven or more sides can be a semi-tilemaker.<br><br>The full mathematical details are explored within the paper. Potential applications, including increasing the efficiency of packaging and shipping materials, and future directions are addressed. 
________________________________________
2007 - MA038 
GRAPHING CALCULATOR AGAINST HUMAN BRAIN IS (AB)^P UNEQUAL TO A^P*B^P?
Dai Shen
Oxford High School, Oxford, MS, U.S.

Whenever a complex number, such as (-56i)^1/12, is entered into a TI- 84 graphing calculator twice, once written in form of (ab)^p, the other time in form of a^p*b^p, it will return two different answers: (-56i)^1/12 = 1.387 ¨C 0.183i and (-56)^1/12 * i^1/12 = 1.292 + 0.535i. So De Moivre¡¯s theorem, which states that (x + y)^1/p = r^1/p [cos (THETA + 2(n)PI)/p + i sin (THETA + 2(n)PI)/p] is used to find all the values of the expression. All twelve values of this expression are: 1.292 + 0.535i, 0.851 + 1.11i, 0.183 + 1.387i, -0.535 + 1.292i, -1.11 + 0.851i, -1.387 + 0.183i, -1.292 - 0.535i, -0.851 - 1.11i, -0.183 - 1.387i, 0.535 - 1.292i, 1.11 - 0.851i, and 1.387 - 0.183i. So the calculator is not wrong, both of the answers are correct, because they are just two out of all twelve values. The calculator just chooses THETA differently, negative PI < THETA < PI instead of 0 < THETA < 2PI, because only so that the inverse of sin x will be a function. All in all, (ab)^p = a^p*b^p does apply to complex numbers.<br><br> 
________________________________________
2009 - MA039 
PAPPUS' CHAIN THEOREM
William Grant Murray
Christ the King Cathedral High School, Lubbock, TX

The goal of this project was to develop a way to prove Pappus' Chain Theorem using inversive geometry. I was able to draw Pappus' Chain and use circle inversion to invert it. Having inverted the chain I was able to obtain certain characteristics of the chain and its inversion to develop a proof. In the end I was able to develop a simple and efficient proof for Pappus' Chain Theorem. 
________________________________________
2003 - MA039 
THE DIFFERENCES BETWEEN THE PARABOLA AND THE CATENARY
Elizabeth Ann Cook
Oxford High School, Oxford MS, United States

The parabola and catenary are among many of the shapes used in architectural structures such as buildings and bridges. When one glances at these shapes in nature or produces their graphs, the differences between the two are subtle enough to have even deceived Galilieo. The general equation for the catenary is y=(emx + e-mx)/2, and the general equation for the parabola is y=a(k - h)2 + k. I hypothesized that the catenary would be able to support more weight than the parabola.<br><br> A closer look reveals some differences between the curves. Beginning at the same y-intercept, the graphs intersect, and the catenary's slope increases much faster than the parabola's does. Analytically, these results do not show why one structure should be preferred over another, and I could not find any tests comparing the two. Therefore, I derived an empirical stress function by observing the effect of various weights on models.<br><br> I built models of each curve and measured the difference in height that hanging weights caused. My empirical results show that parabolas are preferred to catenaries in a mid-range of weight while the catenary is preferred at low and high weight levels. However, the parabola shows a sharp decay at extreme weight levels. At the parabola's weight capacity, it had bent to 7.37% less than its original height. The catenary had bent only 5.53% below its original height and showed no signs of collapsing. I was able to conclude that my hypothesis was correct.<br><br> 
________________________________________
2006 - MA039 
ELLIPTICAL TRAVELING SALESMAN SOLUTION
Jacob Donald Mitchell
Westfield High School, Westfield, Massachusetts, United States of America

As the world’s limited energy resources become scarcer, more importance will be placed on efficient usage of these resources. One potential solution would be to create better approximation algorithms for scenarios like the Traveling Salesman Problem where the goal is to find the shortest route through several predefined locations and back to the original location. The purpose of this study was to create an approximation algorithm based on the elliptical path that TSP solutions tend to exhibit.<br><br> Eight random locations were plotted on a Cartesian graph. A series of six linear regressions were used to collectively describe the locations in the form of an ellipse. Normal lines were drawn from the ellipse to each of the eight locations. The order that these normal lines touched the ellipse determined the final sequence of the locations. This algorithm, along with the brute force algorithm, was programmed in Matlab. The program was run 30,000 times. <br><br> Given a histogram of total distances yielded by each route, the route found by the approximation algorithm tended to come 1.5% from the shortest route. As a percentile, the route found by the approximation algorithm tended to be shorter than 99.9% of the other possible routes. <br><br> In terms of calculation time, the approximation algorithm was 36.4 times faster than the brute force algorithm. The approximation algorithm is highly effective at finding one of the shortest routes and, as opposed to the brute force algorithm which exhibits factorial growth, has a relatively flat growth curve.<br><br> 
________________________________________
2008 - MA039 
EVOLUTIONARY PATTERNS IN THE INFLUENZA A HEMAGGLUTININ PROTEIN THAT SUPPORT INTERSPECIES TRANSMISSION: A BIOINFORMATICS AND COMPUTATIONAL APPROACH
Kevin Kyle Hawkins
GlenOak High School, Canton, OH

A rapidly and randomly replicating virus, influenza is composed of eight negative sense RNA segments and two surface glycoproteins. Bioinformatics and computing were used to investigate interspecies relationships in sequences of human, avian, and swine influenza viruses in segment four of the genome, coding for the hemagglutinin (HA) protein from the Influenza Virus Resources Database. Data was grouped by subtypes; then analyzed with San Diego Supercomputer programs (CLUSTALW, CLUSTALDIS, PROTDIST) to align sequences and calculate evolutionary distance using the Percent Accepted Mutation Model. Calculated distances were analyzed for patterns in interspecies transmission using the neighbor joining method and scatter plots. Tertiary structures were analyzed for residues critical in species specification. Phylogenic branch lengths were small and clustered when strains were compared to the same species, branch lengths were larger and multiple clusters formed when comparing strains collected from different species suggesting a relationship between HA protein and interspecies transmission. The HA protein contains multiple sites critical in the transmission of the virus between different species; however this study suggests a strong correlation between specifically site 62 and the transmission of the virus between species. Asparagine specifying avian, lysine is human, and glycine for swine. Statistical analysis supports site 62 significance and suggests that other sites could contribute to interspecies transmission. (Z-proportionality test: p<.01) The next step is to isolate regions of the proteins critical in interspecies transmission in the lab and facilitate drug development and allow more efficient vaccination selections. 

Awards won at the 2008 ISEF
Second Award of $1,500 - Mathematical Sciences - Presented by
First Award of $1,000 - Mu Alpha Theta, National High School and Two-Year College Mathematics Honor Society
________________________________________
2004 - MA040 
RETURN TO NORMAL: A STUDY OF TOPOLOGY AND COMPACT HOMEOMORPHIC SPACE MODELS
Alice Wan Chai
Plano Senior High School, Plano, TX, U.S.A.

Of algebraic topology, or the analysis of position, the basic method of topological space is associated with a group, or a sequence of groups, with the expectation that the geometrical properties of the space will be reflected in the structure and properties of the associated groups. Of different types of surfaces, homeomorphic surfaces are defined as space models that can be molded to be the same by stretching, but not breaking the "flexible skin" of the surface.<br><br> This project sought to determine if a universal order for simplification of connected compact surfaces exists, and can render all homeomorphic and non-orientable surfaces into the normal form using commutative properties, associative properties, TP=PK, and 2P=K when applied to n-dimensional forms. Consequently, this project sought to determine the most efficient way, using the least number of reduction stages to simplify unlimited length combinations of the four topologic functions: sphere (s), torus (t), Klein bottle (k), and the projective plane (p) into non-orientable normal forms.<br><br> Using altered, organized forms of the classification theorem, combinations of the four space models were rendered into the compact normal form without particular original order. In addition, when the formulas 2P=K and KP=TP, the commutability, and the associability of the connected sum operations were used to convert unlimited expressions for connected compact surfaces into the normal forms, an algorithm for determining a logical and efficient series of steps was proven to simplify up to 80 term-length models into (nT)P or (nT)K with the fewest number of stages (an average of 3.51 steps). <br><br> Future studies could include further studies on complex and twisted torus and projective planes. Applications include the mechanics behind differential topology, physics and architectural design.<br><br> 
________________________________________
2008 - MA040 
CIRCUITS OVER SETS OF NATURAL NUMBERS
Jason Saul Gross
Commack High School, Commack, NY

Circuits over Sets of Natural Numbers are circuits, finite directed acyclic graphs, with gates consisting of intersection, union, addition, and multiplication, with in-degree two, and complement with in-degree one. These circuits operate on sets of natural numbers. The inputs to a circuit are singleton sets of natural numbers. A circuit defines the set of natural numbers that is computed by the final gate in the circuit. The focus of this project was to prove sets undefinable using zeta-dimension and to address the question of computational complexity. This is the question of how long it takes a computer to decide if a given number is in the set defined by a given circuit. Assuming the existence of given procedures or the definability of given sets, the halting problem can be encoded in a circuit. If these methods can be shown to exist, then the membership problem is undecidable. If these methods push the lower bound for circuits above the known upper bound, then that proves that it is impossible to have both methods. 

Awards won at the 2008 ISEF
Third Award of $1,000 - Mathematical Sciences - Presented by
________________________________________
2009 - MA040 
MICRORNA EXPRESSION PATTERNS IN MOUSE LUNG DEVELOPMENT AND CANCER
Kevin Kyle Hawkins
GlenOak High School, Canton, OH

This novel study analyzed the expression patterns of microRNAs in normal lung development and cancer to compare the results to transcriptional profiles of genes during lung development and lung cancer. MicroRNAs (miRNAs) are a novel class of highly conserved small non-coding regulatory RNA. These molecules have been found to exert transcript-level control over the expression genes that contribute to diverse cellular processes including cell fate commitment. In addition to their role in normal biology, the inappropriate expression of miRNAs has been implicated in cancer. With lung cancer being the leading cancer killer and with little progress made in the last 30 years the push for a better understanding of lung cancer and the pathways involved in it are critical. In this study we compared the expression of microRNAs during mouse lung development and used this information as a framework for evaluating the expression of microRNAs in human and mouse lung tumors. miRNA expression data were collected for 5 time points across lung development (E11.5, E13.5, E14.5, E16.5, P5) in C57BL/6J mice. The study identified 21 miRNAs that with significantly different expression levels in normal human lung versus human lung tumors that were also expressed during normal mouse lung development. Furthermore the study suggests a way to improve target prediction algorithms through integration of a filter processes. The study also suggests a more complicated process for miRNA regulation of genes. Further analysis could lead to more precise miRNA target predictions and understanding for miRNAs role in lung development, and carcinogenesis 

Awards won at the 2009 ISEF
Fourth Award of $500 - Mathematical Sciences - Presented by Intel
________________________________________
2007 - MA040 
POTENTIAL PANDEMIC: H5N1 INFLUENZA A MATHEMATICAL STUDY
Shinjini Bakshi
Pennbrook Middle School, North Wales Pennsylvania, United States of America

Predicting the outcome of a pandemic outbreak of avian influenza, in the United States, is almost impossible; however, using simulations may help us maximize the allocation of money to avoid rapid transmission of the virus. The purpose of this mathematical model is to essentially find the most effective method to allocate money that will prevent the spread of avian influenza virus in the event of an outbreak, therefore optimizing the amount of money being spent. <br><br> Because most pandemic influenza models use epidemiology, they are in many ways too complex for the average person to comprehend. Using stochastic simulation technologies, also involve in-depth mathematical understanding, however the target model used in this project generates a realistic, yet easy to understand model of pandemic influenza. A Monte-Carlo simulation is used to more accurately estimate pandemic values from previous influenza strained outbreaks. This model, however, does not account for the number of dying or vaccination process used. <br><br> The “target model” will use certain restrictions to demonstrate if either the Institute of Medicine’s (IOM) K=0 model, or the federal government’s K=1 model will maximize the allocation of money between the states equitably and efficiently. In addition, three other models were generated for K= 1/4, K=1/2, and K= 3/4. It demonstrated that through appropriate allocation K= 3/4 produced better optimization of money than all other values of K.<br><br> 

Awards won at the 2007 ISEF
Award of $1,000 - Mu Alpha Theta, National High School and Two-Year College Mathematics Honor Society
________________________________________
2004 - MA041 
1, 2, 3, DO, RE, MI
Meredith Anne Stacy
Kossuth High School, Koosuth, MS, United States

In my science fair project "1, 2, 3, Do, Re, Mi", I tried to find mathematical patterns in Johann Sebastian Bach Preludes and Fugues for Piano/Organ. I thought that I would find repetitions in his music. I ordered sheet music on the internet to help with my experiment. I counted how many notes were in each measure of 24 compositions. I then put that data into a spreadsheet and graphed each composition. I concluded by studying the graphs that there were some patterns in every composition. 
________________________________________
2009 - MA041 
THE EFFECT OF 1/F NOISE ON HARMONIC RESONANCE
Anthony James Fidaleo
The Davidson Academy of Nevada, Reno, NV

This project is an attempt to correlate a ubiquitous, not-well-understood phenomenon called 1/f noise with a well-understood phenomenon called harmonic resonance. The reasoning behind this comes from the fact that they both require infinite energy in the limit.<br><br>The investigation was carried out using Wolfram Mathematica™ as a tool to solve the harmonic oscillator differential equation for different right hand sides exhibiting harmonic resonance to see if the solutions would exhibit 1/f noise. Then, Microsoft Excel was used to approximate and bin values to facilitate finding the power spectral density function (PSD), which needed to be in the shape of a 1/f curve to exhibit 1/f-like characteristics.<br><br>Most of the functions displaying 1/f noise did not cause harmonic resonance; only two of the 50 or so right-hand sides tried showed any 1/f-like characteristics at all. <br><br>From this, I concluded that there is no general link between 1/f noise and harmonic resonance. The fact that most of the project’s only results were negative means that it cannot have much application; however, the positive results might be used for the correction of so-called “flicker noise” in electronics to increase their efficiency. For extending this project, I am thinking of attempting to find something else that also requires infinite energy to try relating that to 1/f noise. 
________________________________________
2004 - MA042 
DO MOON PHASES AFFECT TORNADO FREQUENCY AND INTENSITY?
Kaci Nicole Hampton
Moore High School, Moore, OK, USA

The purpose of this project was to determine if the moon phases have an affect on tornado intensity or frequency to help improve meteorologist's weather predictions.<br><br>Research was conducted by gathering a list of tornadoes in Oklahoma from the past twenty years (1983-2003). Based on the tornadoes from that list, data was gathered about tornado intensity (based on the fujita scale) and what counties the tornadoes materialized in. From the list of tornado days, information was collected about the moon phase in affect at the time of each tornado. <br><br>Graphs and charts were created based on the information for better understanding and overall analysis of the material. <br><br>There was a significantly higher number of tornadoes in the waning gibbous moon phase (302 tornadoes) making up almost 30% of the reported tornadoes. In comparison, the next highest moon phase was the waxing crescent (195 tornadoes) making up 19% of the reported tornadoes. <br><br>From the research, it can be concluded that the moon phase does have an affect on tornado frequency. When specifically looking at the intensity of the tornadoes in relation to the moon phase, however, there appears to be no correlation. 
________________________________________
2006 - MA042 
A FINITENESS PROPERTY FOR INTEGRAL POINTS IN A FAMILY OF CONICS
Sohan Venkat Mikkilineni
Detroit Country Day School, Beverly Hills MI, USA

A d-determinantal sequence in a domain D is a doubly-infinite sequence {an} of nonzero an ∊ D such that the determinant |■(an&an+1@an+2&an+3)┤| is equal to d for all n, with d ≠ 0 in D. The parameter n ranges through all integers, not just non-negative integers. A basic question is whether one can exhibit all such sequences (up to simple operations) for a fixed d when D = ℤ. We focus on the essential case when all terms a2k+1 (respectively all terms a2k) have no non-trivial common factor. The case |d| = 1 was solved six years ago, and our handling of |d| > 1 is quickly reduced to the following problem: for d ∊ ℤ with |d| > 1, find all α, β ∊ ℤ such that αx2 – αβxy + βy2 = -d has solutions x, y ∊ ℤ with gcd(x, β) = gcd(y, α) = gcd(x, y) = 1. By using approximation methods from the theory of continued fractions, we prove the surprising fact that there are only finitely many such α, β (for each d with |d| > 1). In fact, |αβ – 2| ≤ 2√(d^2+1) aside from several exceptional pairs {α, β} that we give explicitly in terms of d. 

Awards won at the 2006 ISEF
Third Award of $250 - American Mathematical Society
________________________________________
2005 - MA042 
NONAGONAL NUMBERS IN THE FIBONACCI SEQUENCE AND RELATED DIOPHANTINE EQUATIONS
John Michael Sillcox
Jericho High School, Jericho, New York, United States of America

The integers of the form r(7r-5)/2, where r is a positive integer, are the nonagonal numbers. Nonagonal numbers are the 9th order polygonal numbers. A polygonal number of order j may be represented as a set of dots that, when connected, form j-gons of increasing size. All of these j-gons share a common vertex and the two sides adjacent to this vertex extend in the same directions. In this paper, we prove that the only generalized nonagonal numbers in the Fibonacci sequence are 0 and 1. We prove three lemmas that lead to this general result. We then use this general result and a few definitions to prove the main theorem. Using the main theorem and the results thereof, we will establish the solution sets of the Diophantine equations 4x² = 5y²(7y - 5)² ± 16. 

Awards won at the 2005 ISEF
Third Award of $250 - American Mathematical Society
________________________________________
2006 - MA042 
A FINITENESS PROPERTY FOR INTEGRAL POINTS IN A FAMILY OF CONICS
Sohan Venkat Mikkilineni
Detroit Country Day School, Beverly Hills MI, USA

A d-determinantal sequence in a domain D is a doubly-infinite sequence {an} of nonzero an ∊ D such that the determinant |■(an&an+1@an+2&an+3)┤| is equal to d for all n, with d ≠ 0 in D. The parameter n ranges through all integers, not just non-negative integers. A basic question is whether one can exhibit all such sequences (up to simple operations) for a fixed d when D = ℤ. We focus on the essential case when all terms a2k+1 (respectively all terms a2k) have no non-trivial common factor. The case |d| = 1 was solved six years ago, and our handling of |d| > 1 is quickly reduced to the following problem: for d ∊ ℤ with |d| > 1, find all α, β ∊ ℤ such that αx2 – αβxy + βy2 = -d has solutions x, y ∊ ℤ with gcd(x, β) = gcd(y, α) = gcd(x, y) = 1. By using approximation methods from the theory of continued fractions, we prove the surprising fact that there are only finitely many such α, β (for each d with |d| > 1). In fact, |αβ – 2| ≤ 2√(d^2+1) aside from several exceptional pairs {α, β} that we give explicitly in terms of d. 

Awards won at the 2006 ISEF
$5000 per year for four year scholarships. - Indiana University-Purdue University Indianapolis
________________________________________
2009 - MA042 
THE CLASSIFICATION OF CERTAIN FUSION CATEGORIES
Eric Kerner Larson
South Eugene High School, Eugene, OR

Fusion categories are algebraic structures that generalize finite groups, semisimple Hopf and quasi-Hopf algebras. They arise and have applications in various areas of mathematics, as well as theoretical physics and computer science, such as representation theory, low-dimensional topology, theory of operator algebras, string theory, conformal field theory and quantum computing. Classification of fusion categories of a given dimension is an important problem which extends the classical problem of classification of finite groups of a fixed order. This problem is very hard and was solved only for dimensions equal to p, pq or pqr, where p, q and r are distinct primes.<br><br>This project advances this classification program in two directions. First, it completely classifies fusion categories of dimension pq^2, where p and q are distinct primes. This case is especially interesting because it is the first class of integral fusion categories where not all members are group-theoretical. The second theorem is a classification of a certain family of Z/3Z-graded fusion categories. It is a generalization of the 1998 result of Tambara and Yamagami.<br><br>The proofs of our results are based on the recently developed theory of extensions of fusion categories. 

Awards won at the 2009 ISEF
Intel ISEF Best of Category Award of $5,000 for Top First Place Winner - Mathematical Sciences - Presented by Intel
________________________________________
2005 - MA042 
NONAGONAL NUMBERS IN THE FIBONACCI SEQUENCE AND RELATED DIOPHANTINE EQUATIONS
John Michael Sillcox
Jericho High School, Jericho, New York, United States of America

The integers of the form r(7r-5)/2, where r is a positive integer, are the nonagonal numbers. Nonagonal numbers are the 9th order polygonal numbers. A polygonal number of order j may be represented as a set of dots that, when connected, form j-gons of increasing size. All of these j-gons share a common vertex and the two sides adjacent to this vertex extend in the same directions. In this paper, we prove that the only generalized nonagonal numbers in the Fibonacci sequence are 0 and 1. We prove three lemmas that lead to this general result. We then use this general result and a few definitions to prove the main theorem. Using the main theorem and the results thereof, we will establish the solution sets of the Diophantine equations 4x² = 5y²(7y - 5)² ± 16. 

Awards won at the 2005 ISEF
Third Award of $1,000 - Mathematics - Presented by Science News
________________________________________
2009 - MA042 
THE CLASSIFICATION OF CERTAIN FUSION CATEGORIES
Eric Kerner Larson
South Eugene High School, Eugene, OR

Fusion categories are algebraic structures that generalize finite groups, semisimple Hopf and quasi-Hopf algebras. They arise and have applications in various areas of mathematics, as well as theoretical physics and computer science, such as representation theory, low-dimensional topology, theory of operator algebras, string theory, conformal field theory and quantum computing. Classification of fusion categories of a given dimension is an important problem which extends the classical problem of classification of finite groups of a fixed order. This problem is very hard and was solved only for dimensions equal to p, pq or pqr, where p, q and r are distinct primes.<br><br>This project advances this classification program in two directions. First, it completely classifies fusion categories of dimension pq^2, where p and q are distinct primes. This case is especially interesting because it is the first class of integral fusion categories where not all members are group-theoretical. The second theorem is a classification of a certain family of Z/3Z-graded fusion categories. It is a generalization of the 1998 result of Tambara and Yamagami.<br><br>The proofs of our results are based on the recently developed theory of extensions of fusion categories. 

Awards won at the 2009 ISEF
The SIYSS is a multi-disciplinary seminar highlighting some of the most remarkable achievements by young scientists from around the world. The students have the opportunity to visit scientific institutes, attend the Nobel lectures and press conferences, learn more about Sweden and experience the extravagance of the Nobel festivities. Valid passport required for travel. - Seaborg SIYSS Award
UTC Stock with an approximate value of $2,000 - United Technologies Corporation
________________________________________
2010 - MA043 
ON THE PERIOD LENGTHS OF THE PARALLEL CHIP-FIRING GAME
Tian-Yi Damien Jiang
North Carolina School of Science and Mathematics, Durham, NC

The parallel chip-firing game (PCFG) is a periodic automaton on graphs in which vertices "fire" chips to their neighbors. In 1989, Bitar conjectured that the period of a PCFG with n vertices is at most n. Though this conjecture was disproven in 1994 by Kiwi et. al., it has been proven for particular classes of graphs, specifically trees (Bitar and Goles, 1992) and the complete graph K_n (Levine, 2008). We prove Bitar's conjecture for complete bipartite graphs and characterize completely all possible periods for positions of the PCFG on such graphs. Furthermore, we extend our construction of all possible periods for games on the bipartite graph to games on complete c-partite graphs, c>2, and prove pertinent lemmas about games on general simple connected graphs. 
________________________________________
2004 - MA043 
A REMARKABLE PROPERTY OF THE GEOMETRIC CENTER OF MASS FOR POINTS ON A CONIC SECTION
Boris L. Hanin
Pittsford Sutherland High School, Pittsford, New York, USA

The starting point for this project was the following beautiful theorem that apparently goes back to Jacob Steiner, a 19th century German mathematician. <br><br> <br><br> Theorem. Let A , A ,..., A , n 2, be points on a circle. Denote by M their geometric center of mass. Let B be the point of intersection of line A M with the circle, k=1,2,..., n. Then <br><br> (1)<br><br> <br><br> Using geometry, vector algebra, linear transformations, and conic section representations of quadratic curves, I proved the theorem and generalized it for points on an ellipse and parabola as well as for points on a sphere and ellipsoid of any dimension. By analyzing the proof of the original theorem, I found the set of all points M in the plane or space for which property (1) holds for all the mentioned curves and surfaces except for parabolas. Finally, Steiner's formula about moments of inertia was generalized for mass distributions on a sphere thus providing a mechanical application of the results of this project.<br><br> 

Awards won at the 2004 ISEF
Scholarship award of $5,000 per year for 4 years - Lewis & Clark College
________________________________________
2005 - MA043 
PRIMALITY TESTING: UTILIZING CHARACTERISTICS OF PRIME NUMBERS THAT ALLOW FOR OPTIMAL DETERMINISTIC AND PROBABILISTIC PRIMALITY TESTING ALGORITHMS IN POLYNOMIAL TIME
Anarghya A Vardhana
Jesuit High School, Portland, OR, United States of America

<br><br>Primality testing in an efficient and accurate manner has eluded mathematicians for centuries. The AKS algorithm presents a deterministic and polynomial time algorithm to test primality. Unfortunately, although the algorithm presents accuracy, it lacks efficiency, thereby rendering it ineffective for practical use. There exist two other well-known and widely used primality-testing algorithms, those being the probabilistic Rabin-Miller Strong Pseudoprime Test, and the deterministic Elliptic Curve Primality Proving (ECPP) method, both being in polynomial time. <br><br>The student will explore and study, in depth, various aspects and methods of primality testing. The focus of the project will be a study of the AKS Primality Testing Algorithm, ECPP, and Rabin-Miller, all three being radically different from one another. The student should understand the three in depth because the harmonic intersection of the essence of the three algorithms is an integral part of this project. The student will study a few of the many properties and tools of number theory that are skillfully utilized in these primality tests: the Binomial Theorem, Fermat’s Little Theorem, modular arithmetic, and elliptic curves. The student will also explore other aspects of analytical number theory that may contribute to primality testing, such as Pascal’s Triangle, and methods of efficient multiplication. <br><br>The student developed a novel method for primality testing using ideas from the studied primality testing algorithms and other characteristic trends of prime numbers. The new algorithm is ideal in the fact that it utilizes the speed of probabilistic methods combined with the accuracy of deterministic methods. <br><br> 

Awards won at the 2005 ISEF
Tuition Scholarship of $5,000 per year for four years - Oregon State University
Scholarship Award of $20,000 - Department of Homeland Security, University Programs Office
________________________________________
2004 - MA043 
A REMARKABLE PROPERTY OF THE GEOMETRIC CENTER OF MASS FOR POINTS ON A CONIC SECTION
Boris L. Hanin
Pittsford Sutherland High School, Pittsford, New York, USA

The starting point for this project was the following beautiful theorem that apparently goes back to Jacob Steiner, a 19th century German mathematician. <br><br> <br><br> Theorem. Let A , A ,..., A , n 2, be points on a circle. Denote by M their geometric center of mass. Let B be the point of intersection of line A M with the circle, k=1,2,..., n. Then <br><br> (1)<br><br> <br><br> Using geometry, vector algebra, linear transformations, and conic section representations of quadratic curves, I proved the theorem and generalized it for points on an ellipse and parabola as well as for points on a sphere and ellipsoid of any dimension. By analyzing the proof of the original theorem, I found the set of all points M in the plane or space for which property (1) holds for all the mentioned curves and surfaces except for parabolas. Finally, Steiner's formula about moments of inertia was generalized for mass distributions on a sphere thus providing a mechanical application of the results of this project.<br><br> 

Awards won at the 2004 ISEF
Second Award of $1,500 - Mathematics - Presented by Panasonic Consumer Electronics Company
All expense-paid trip to attend the U.S. Space Camp in Huntsville, Alabama and a certificate - National Aeronautics and Space Administration
________________________________________
2004 - MA044 
ORDER PRESERVING COLORING AND RECURSIVE TREES.
Pranvera Mucaj
Central High School, Philadelphia, PA, United States of America

<br><br> The purpose of this research was to prove that the number of Order Preserving Coloring for recursive trees is a polynomial. f:v--l is defined to be an OP (Order Preserving) coloring if and only if f(i)<f(j),whenever i < j and {i;j} is an edge. Recursive trees are a special kind of trees. A tree is defined to be recursive if and only if each vertex is connected to a vertex of a lower number only.<br><br> Initially, a conjecture was developed. It stated that the number of proper colorings that preserved order in a recursive tree is a polynomial. The Method of Proof by Complete Induction was used to prove the conjecture. The number of ways a recursive tree can be properly colored using OP coloring is proved to be a polynomial. Also, a part of the proof is focused on the special cases of the linear and root-edge recursive trees.<br><br> A spanning tree in a directed graph on n vertices is just a subset of the edges that is itself a tree on n vertices. The second part of the research is focused in finding the average number of spanning recursive trees in a directed graph. The result showed that there is a substantial number of spanning trees that are recursive, in any directed graph, and a formula was derived to compute the average number of spanning recursive trees more explicitly.<br><br> 
________________________________________
2005 - MA044 
COMPLETE SEQUENCES OF POSITIVE INTEGERS
Kledin Dobi
J.R. Masterman High School, Philadelphia, Pennsylvania, USA

In this paper, complete sequences are investigated. It is proved that the sequences are complete. It is further generalized and proved that the family is complete. Sequences of primes are analyzed and it is shown that they are not complete. Also, connections to Ring Theory are found and a way to find using is provided. The connection to Ring Theory is also exploited to prove a powerful theorem concerning polynomial sequences . Additionally, an algorithm for proving whether or not a polynomial sequence is complete is developed. <br><br> Furthermore, the number of ways, , a number can be made using the first terms of a complete sequence is studied. An explicit formula for is derived and proved. The generating function for any complete sequence is also found, and, as a result, an interesting, unrelated theorem is proved.<br><br> Finally, the values of are analyzed. Upper and lower bounds for are found. Additionally, a novel and instructive interpretation of in statistics and stochastic processes is revealed, allowing us to prove the main result, an open problem in mathematics: the Complete Conjecture. The Complete Conjecture is strengthened, and it is applied to other numbers and sequences. Altogether, more than 15 new theorems concerning complete sequences are proved. 

Awards won at the 2005 ISEF
Second Award of $500 - American Mathematical Society
Intel ISEF Best of Category Award of $5,000 for Top First Place Winner - Mathematics - Presented by Science News
All expense-paid trip to attend the U.S. Space Camp in Huntsville, Alabama and a certificate - National Aeronautics and Space Administration
________________________________________
2009 - MA045 
FRACTAL COMPLEXITY: UNVEILING IRRATIONAL POINTS AND BEYOND WITHIN THE MANDELBROT SET
Jason Lugo
Dove Science Academy (OKC), Oklahoma City, OK

By closely analyzing the Mandelbrot Set, specific mathematical applications and irrational points were found within this complex structure. The Escape Criterion for the set was studied and different pinch points were analyzed. A vertical, horizontal, and parabolic route were taken at the pinch points to churn a Pi value. After this, different functions and limits were obtained to yield a Pi result (depending on the pinch point) and it was conjectured that a power function will always be used to yield Pi within the Mandelbrot set, regardless of what pinch point is analyzed.<br><br>Furthermore, it was discovered that the Mandelbrot set, originally constructed using quadratic iteration, can also be constructed using cubic iteration. Since you will obtain two critical points instead of one, two subsets were formulated, both parameterized by one critical point. The filled Julia set and corresponding Julia sets were also analyzed in the construction of the "new" Mandelbrot set. It was proven that this cubic mapping is contained within the four dimensional space and it also possess the same characteristics as the quadratic mapping of the set, with reference to the filled Julia set. The intersection of the two subsets when iterated formed the actual structure of the Mandelbrot set, thus proving that it can be constructed using cubic iteration. 
________________________________________
2007 - MA045 
INFINITE SERIES
Corey Jon Reutlinger
Lexington High School, Lexington, NE, USA

The purpose of this experiment is to see how infinite series can be applied to real life situations. Infinite series is a term in calculus that describes the summation of a sequence of numbers that would go on forever. Most finite series have modern applications, but that is not the same with infinite series. By using different projects, I could determine whether or not infinite series can actually be beneficial to the scientific and mathematical community. Projects would include discovering whether or not an object would disappear if you infinitely divide it into pieces, discovering if an object would have infinite surface areas, or determining whether or not harmonic vibration would go on infinitely. Examining different methods in solving infinite series would solve this problem. My hypothesis is to determine whether or not infinite series are only hypothetical or applicable to real-life situations. The process to prove this hypothesis will be testing different situations that could actually apply infinite series. 
________________________________________
2008 - MA045 
DIFFERENT SEARCH PATTERNS AND THEIR SUSCEPTIBILITY TO AN OBSTACLE
Connie Zhong
Skyline High School, Salt Lake City, UT

Search patterns are widely used for airplane surveillance, rescue missions, and commercial products. The objective of this research is to compare the efficiency of different search patterns within a square region and their susceptibility to a square obstacle with variations of obstacle size and position. The three different patterns investigated are reflection, perpendicular, and random. Using analytic geometry and defined rules, a mathematical model was developed to calculate the trajectory of the moving object. A computer program was written to simulate the results.<br><br>The efficiency of each search pattern is defined as the ratio between the covered area and the total travel distance at a designated number of strokes. The Reflection pattern could either perform better than or worse than the Random pattern, depending on a combination of geometric parameters. The Perpendicular pattern was the worst among the three patterns, never covering the whole area because of its convergence. The Random pattern was the most consistent. <br><br>With no obstacle, the initial conditions that result in a dead loop were examined. In addition, mathematical equations of convergence for the Perpendicular pattern were derived. <br><br>When a square obstacle is introduced, the size and position significantly impacts the Reflection and Perpendicular patterns, while the Random pattern shows no apparent change. The results show that the dead loop also exists in the Reflection pattern and the Perpendicular pattern still converges. A special Perpendicular pattern case was examined with mathematical equations. 

Awards won at the 2008 ISEF
Scholarship Award of $5,000 and 10-week summer research experience. Total value is $10,000 - Department of Homeland Security, University Programs Office
________________________________________
2004 - MA045 
SIMULATING THE BEHAVIOR OF THE SARS VIRUS USING THE MONTE CARLO METHOD
Eileen Ke
Germantown Academy, Fort Washington PA, USA

The purpose of the project is to determine, using the Monte Carlo method and a computer program to simulate the SARS virus outbreak in Block E of Amoy Gardens in Hong Kong, how damaging the effects of SARS could have been if the virus was not contained. Several key parameters such as infected patients, new cases, and deaths were analyzed. Data was found for Block E of Amoy Gardens for each of the first 18 days (before quarantine was enforced) of the outbreak. A set of probabilistic equations were set up and used to study the relationship among the key parameters and to estimate their values for the following days. The SAS program language was used to calculate the key parameters each day for up to 100 days. The Monte Carlo method was used to simulate the process 10, 100, and 1000 times in order to determine possible outcomes of each parameter. Graphs were used to display a curve for each simulation. Descriptive statistics were calculated for all simulation outcomes. In conclusion, this study showed that SARS, as an epidemic disease, can greatly damage the susceptible population within a relatively short period of time, if no immediate action is taken. 

Awards won at the 2004 ISEF
Third Award of $1,000 - Mathematics - Presented by Panasonic Consumer Electronics Company
________________________________________
2008 - MA045 
DIFFERENT SEARCH PATTERNS AND THEIR SUSCEPTIBILITY TO AN OBSTACLE
Connie Zhong
Skyline High School, Salt Lake City, UT

Search patterns are widely used for airplane surveillance, rescue missions, and commercial products. The objective of this research is to compare the efficiency of different search patterns within a square region and their susceptibility to a square obstacle with variations of obstacle size and position. The three different patterns investigated are reflection, perpendicular, and random. Using analytic geometry and defined rules, a mathematical model was developed to calculate the trajectory of the moving object. A computer program was written to simulate the results.<br><br>The efficiency of each search pattern is defined as the ratio between the covered area and the total travel distance at a designated number of strokes. The Reflection pattern could either perform better than or worse than the Random pattern, depending on a combination of geometric parameters. The Perpendicular pattern was the worst among the three patterns, never covering the whole area because of its convergence. The Random pattern was the most consistent. <br><br>With no obstacle, the initial conditions that result in a dead loop were examined. In addition, mathematical equations of convergence for the Perpendicular pattern were derived. <br><br>When a square obstacle is introduced, the size and position significantly impacts the Reflection and Perpendicular patterns, while the Random pattern shows no apparent change. The results show that the dead loop also exists in the Reflection pattern and the Perpendicular pattern still converges. A special Perpendicular pattern case was examined with mathematical equations. 

Awards won at the 2008 ISEF
Second Award of $1,500 - United States Air Force
________________________________________
2007 - MA046 
SOME RESULTS ON THE BARKER CODE AND CIRCULANT HADAMARD MATRIX CONJECTURES
Jean J Shiao
Jasper High School, Plano, TX, USA

The Barker Code and Circulant Hadamard Matrix Conjectures are long-standing, unsolved, open problems. In this project, both conjectures were studied, and several results have been obtained in two directions:<br><br> First, it was shown that each Barker Code of an even length generates a Circulant Hadamard matrix. On the converse, it was shown that if each row of a Circulant Hadamard matrix is a Barker Code, then both the Barker Code Conjecture and the Hadamard Circulant Matrix Conjecture are true, which leaves the question regarding whether or not each row of a Hadamard Circulant matrix is a Barker Code. On a different direction, both conjectures were studied by investigating the distribution of '+1' and '-1' on the rows of a Circulant Hadamard matrix or on a Barker Code. The results show that both conjectures are true if and only if there is only one '-1' subgroup or '+1' subgroup that has a length of 1. As a special case, the results show that if the lengths of all '-1' and '+1' subgroups are greater than or equal to 2, then there is no such existing Barker Code or Circulant Hadamard matrix. 
________________________________________
2005 - MA046 
PYTHAGOREAN TRIPLES
Benjamin Lawrence Mikell
Oxford High School, Oxford Mississippi, United States

The purpose of the project was to determine if a pattern existed that could generate all of the Pythagorean Triples. Familiar triples (3,4,5), (5,12,13), and (7,24,25) were examined. The first numbers in each triple were odd, the second numbers could be found by first adding 8, then 12, etc., to get the second number in the series, etc., and the third number was one more than the second number. Algebra was used to show the nth terms of the series to satisfy the Pythagorean theorem.<br><br>A search for Pythagorean triples that did not fit the odd pattern was conducted. Triples such as (8,15,17) and (20,21,29) were named "even" triples. A procedure similar to the one used by the Mayas was discovered. If a number is even, square the number; divide the square by each of the factors of the square that are multiples of 4 and less than the original number; add and subtract one for each multiple of 4 to get the second and third numbers of the triple.<br><br>If a number is odd, square the number; divide the square by even numbers that are not multiples of 4 and are less than the original odd number; add and subtract 0.5 for each multiple of 2 to get the second and third numbers of the triple. <br><br>This procedure will generate all possible Pythagorean triples and was verified using EXCEL. 
________________________________________
2008 - MA046 
RESTRICTIONS AND GENERALIZATIONS ON COMMA-FREE CODES
Alexander Lee Churchill
Lincoln East High School, Lincoln, NE

A significant sector of coding theory is that of comma-free coding; that is, codes which can be received without the need of a letter used for word separation. The major difficulty is in finding bounds on the maximum number of comma-free words which can inhabit a dictionary. <br><br> I introduce a new class of comma-free code called a self-reflective comma-free code. Call a set of words with self-reflective comma-freeness a self-reflective comma-free dictionary. I prove a series of bounds on the size of such a dictionary based upon word length and alphabet size. I also introduce other new classes such as self-swappable comma-free codes and prove preliminary bounds for these classes. <br><br> I also generalize comma-free codes to comma-free codes in multiple dimensions. I prove generalized bounds on the maximum size of multidimensional comma-free dictionaries. To accomplish this, I develop and prove Möbius inversion for multivariant functions.<br><br> Finally, I discuss the implications and applications of combining these original concepts, including their implications for the NP-complete Post Correspondence Problem. 

Awards won at the 2008 ISEF
First Award of $1,000 - American Mathematical Society
Fourth Award of $500 - Mathematical Sciences - Presented by
________________________________________
2006 - MA047 
DOES MISPRICING EXIST IN THE STOCK MARKET - A MATHEMATICAL APPROACH TO PROVING THE CONTRARIAN THEORY
Aditya Soni
Scarsdale High School, Scarsdale, NY, USA

This research investigates the relative merits of two views on stock valuation. The first is the Efficient Market Hypothesis (EMH), which claims that, given the amount of analysis that determines trading, stocks must accurately represent their firms’ worth. The proponents of the second, the Contrarian Theory, believe that, because of human tendency to overreact, stocks with dips in performance can become mispriced. They further believe the market eventually discovers its mistake, leading to the period of re-pricing in which the stock achieves large returns until it has appropriately adjusted itself. If investors can understand the nature of the market and market mispricing, they can invest more wisely and improve the efficiency of the market. <br><br> The experiment used data from the companies listed on the New York Stock Exchange from 1985 to 2003. Using the Contrarian Theory and additional previous research, five factors were determined to identify undervalued companies. These factors: earnings, assets, book to market ratio, trading volume, and sales growth, through a statistical algorithm developed in this experiment, were able to isolate mispriced companies. These companies were shown to be mispriced because they reflected the expected returns pattern: poor returns, a boost accompanying repricing, and finally, the shift back into normality. This statistical algorithm can now be used as an investment strategy, as it provides consistently high returns by investing against the market’s general movement. The discovered evidence for undervaluation undermines the EMH and gives insight into the strength of the Contrarian Theory.<br><br> 
________________________________________
2007 - MA047 
THE SIMPLICITY OF COMPLEXITY: UNVEILING THE MANDELBROT SET
Jason Lugo
Dove Science Academy, Oklahoma City, OK, USA

The aim of this project is to identify and explain how the Mandelbrot set is constructed and to try and find mathematical applications in it. If there are any applications, which ones or what kinds and what are their purposes? <br><br> By closely studying the numerical sequences used in the Mandelbrot set, specific applications were found: the Farey sequence, the Fibonacci sequence, the irrational constant Pi, and once I was lead to the discovery of this mathematical constant, I performed a more complex analysis to see if I could discover any more irrational points. I found out in this project that the Farey sequence can be used to find the rotation numbers of an orbit of x0 for a specific value of c. This sequence can only be used with the filled Julia set, but helps in determining the period of the bulb for which a c value pertains in the Mandelbrot set. The Fibonacci sequence is used in the categorization of the period numbers of the bulbs on the Mandelbrot set. The appearance Pi in the Mandelbrot set is another remarkable occurrence of this study, as well as the existence of other irrational points in the set.<br><br> The closer you look at this set and the more in depth you explore it, the more you will find.<br><br> 
________________________________________
2010 - MA049 
ANALYSIS AND DEVELOPMENT OF NOVEL COMPUTATIONAL COMPLEXITY CLASSIFICATIONS
Kenneth Ka Au
Downingtown High School West Campus, Downingtown, PA

Computational complexity serves as the foundation and science of computers and mathematics. This project attempts to resolve one of the world’s most disputable problem: the P and NP Problem. In order to approach this question, the project introduces a novel classification architecture depicting computational complexity in a two-way spectrum. It significantly differs from the status quo organization due to its single variable design. The fundamentals of computer science derive the bounds of this new model. In addition, this theory attempts to develop a correlation between the physical world and the computational world. Extensive research of computer recursion and states serves as the backbone to this novel idea. Classification of complexity leads to the theoretical possibility of algorithms to solve their respective problems, whether they are “easy” or “hard”. Mathematics and physics strongly extends and support this computational theory. <br><br> <br><br>In the end, the project funnels back to the original idea: proving P and NP through a new radical organization. The relationship of P and NP could change modern science and human society. Cryptography and complexity simulation are applications correlating to the results. Construction of future computers developed under this scheme may provide answers to all fields of science. <br><br> <br><br>From research to analysis to development, the project tackles the long debated problem. The project concludes with insightful future advancements, such as theoretical finalization and construction of a real or abstract machine that fulfills the theory. Lastly, it promotes advancements in cryptography and parallelization. 
________________________________________
2006 - MA050 
CHEATING IN BASEBALL: DOES IT HELP
Jon Charles Pavlicek
Linton Public School, Linton, North Dakota USA

I want to determine if applying a lubricant or distorting a baseball has a significant effect on the ball's movement. I want to determine direction of change and how reliable/predictable the change is.<br><br> I took twenty-four regulation balls of the same age and altered them, cutting, scuffing one side and two sides, and applying spit and Vaseline (four balls of each). The twenty-four balls along with four unaltered balls were pitched fifteen times each at a grid. Each type of alteration was pitched thirty times by myself and thirty times with a pitching machine. This was done indoors. I rested my arm after ten pitches.<br><br> I found that altering does have a significant effect on ball's movement. Balls scuffed on the left side dramatically drop and move to the left. If a right handed pitcher aimed high and to the right, the ball would drop into the strike zone; thus tricking the batter. The following is the ball's ranking in ability to fool the batter: left-scuff, Vaseline, scuff two-sided, cut, right scuff and spitball. I found the standard deviation for each type. Right scuff had the tightest deviation. The spitball had the largest deviation. This means it would take a highly skilled pitcher to use the spitball effectively. The balls I pitched moved, but the standard deviation was so great I would not use an illegal ball, but I can use this experiment/formulas on legal pitches (curveball, slider) to determine which pitches are best and most reliable for me.<br><br> 
________________________________________
2010 - MA050 
FRACTALS AND FUGUES: ANALYZING MUSIC WITH MATH
Rachel Danielle Perfecto
Roy C. Ketcham High School, Wappingers Falls, NY

It has always been said that there is a great correlation between music and math. For example, many have described the fugues and inventions of Bach as having the precision of mathematics. Fractal geometry, as determined by Mandelbrot, has become a branch of mathematics used to describe occurrences in nature and “rough” phenomena that Euclidean geometry cannot describe. The purpose of this project was to see whether such fractal properties would show themselves in the interval, amplitude, or structural scaling aspects of music. Fractality was determined in several ways: finding a linear correlation when plotting the log of how many times an interval occurred versus the log of the interval itself, signal processing analysis, and simply musically analyzing the piece. Fractality in interval scaling appeared to have the most potential for application. If the graph of the log of the total times an interval occurred versus the log of the interval itself appeared to show a linear trend, then the piece can could thinned, or reduced. In the future, this could help arrangers in the reduction of pieces for amateur players. 

Awards won at the 2010 ISEF
Third Award of $1,000 - Mathematical Sciences - Presented by Intel
________________________________________
2010 - MA057 
PLANET PARALLAX
Holly Ann Krumm
Strasburg Public School, Strasburg, ND

The purpose of my project was to determine if it was possible to create equations that determine the distance from each planet to the sun. My hypothesis was that it was possible to create equations that would serve this purpose. First, I had to research this area of study and obtain all the necessary forms. Second, I started experimenting. Third, I had to analyze and test my data, and repeated step two which was necessary. And finally, I made my conclusion. My results showed that I did create equations. The equations served the purpose I wanted. My conclusion was that I did create the equations. My hypothesis was indeed correct. 

Awards won at the 2010 ISEF
Second Award of $1,500 - Air Force Research Laboratory on behalf of the United States Air Force
________________________________________
2005 - MA301 
DETERMINING END BEHAVIOR IN CELLULAR AUTOMATA USING GRID COMPUTING
Erich John Kreutzer, Andrew Duchi
Upper Arlington High School, Upper Arlington Ohio, United States of America

In this study, the end behaviors of cellular automata known as “ants” were analyzed for patterns.  An “ant” is a cellular automata based on a recursive formula in which the ants’ movements are determined by the status of the grid.  In order to determine patterns in the end behavior, the end behaviors of ants with 240 different starting conditions were determined by first creating a program in C++ and then, using grid computing, the program was run in a timely fashion. The types of end behaviors were then graphed on a 9 by 9 grid. It was determined that there were patterns in the end behavior of the two ants based on the position of the ants in relation to each other.  Being able to determine the end behavior of a cellular automaton such as “ants” has applications in predictions of the natural world, predicting human behavior, and creating self-generating computer graphics. 
________________________________________
2009 - MA301 
SURVIVAL ANALYSIS OF GENE EXPRESSION DATA USING A HYBRID DIMENSION REDUCTION TECHNIQUE
Alicia Zhang, Jeffrey Chan, Sameer Deshpande
Liberal Arts and Science Academy High School, Austin, TX

DNA microarray technology has the ability to simultaneously screen thousands of gene expression profiles, revolutionizing how genetics are applied in medicine. However, when studying how genes affect the survival of disease patients, the sheer number of gene expression profiles available, the high correlation among these genes, and the complex networks that interacting genes form render common statistical methods ineffective. In response to this problem, many techniques have been developed to predict survival, but do not provide insight into which genes are implicated.<br><br> We propose a hybrid technique that constructs a gene network, and identifies individual and representative genes in a cluster that affect chance of survival, while considering the potential multi-collinearity among selected genes. Using a set of clinical data, we show that our hybrid method outperforms previous methods in model fitting and survival prediction. Our method not only enables clinical researchers to predict patient survival, but also provides the biomedical community with a smaller set of genes to better understand the basic biological processes affecting patient survival. This in turn will result in a more effective localized targeting of genes for the treatment of many diseases. 

Awards won at the 2009 ISEF
Third Award of $250 - American Mathematical Society
First Award of $1,000 - American Statistical Association
First Award of $1,000 - Mu Alpha Theta, National High School and Two-Year College Mathematics Honor Society
________________________________________
2008 - MA303 
THE CHAOS THEORY EXHIBITED IN NON-LINEAR OSCILLATING SYSTEMS: THE DOUBLE AND TRIPLE PENDULUMS
Danielle Louise Erhard, Audrey Bennett, 
Coon Rapids High School, Coon Rapids, MN

Our project this year focused on the Chaos Theory, which is the unpredictability of inherently predictable systems. Chaos can be seen in such things as simple as the shape of clouds to systems as complex as the human body and cancer cell mitosis. The purpose of the project was to view how the Chaos Theory is exhibited within the double and triple pendulums. Our prediction was that if we tested the single pendulum, we would find that it’s not chaotic due to being a linear system. We also believed we would see signs of chaotic behavior within the double and triple pendulums because they’re both non-linear systems. To test our hypotheses, we first showed how the single pendulum follows its predicted path in a given time frame. Using a mathematical equation, we calculated the time it should take the pendulum to complete ten periods and then measured the actual time. Comparing these times we found that our predictions were completely accurate. We tested the double pendulum by first using differential calculus equations to predict the expected motion of the pendulum. We filmed trials of the pendulum’s motion and then reviewed the footage to find the actual angles of the pendulum. Comparing the results between the actual and theoretical positions, we found that our double pendulum shows profound signs of chaotic behavior, being both sensitive to initial conditions and having infinite variations. Currently, we’re in the process of writing the triple pendulum equation ourselves because a published version hasn’t been found. 

Awards won at the 2008 ISEF
Fourth Award of $500 - Team Projects - Presented by Science News
________________________________________
2005 - MA303 
THE POWER DOMINATION PROBLEM ON TREES AND OTHER BIPARTITE GRAPHS
Edward Fu Schmerling, Jennifer Schmerling, Sarah Spikes
The Liberal Arts and Science Academy at LBJ High School, Austin, TX, USA

This project deals with power domination of trees and other bipartie graphs in graph theory. Power domination of graphs is accomplished by using PMUs (phase measurements units), and the most efficient way to dominate a graph always uses the least number of PMUs possible. Using a knowledge of of basic graph theory, through mathematical proofs and analysis of mathematical definitions, conclusions were reached on how to most efficiently dominate a type of graph called a tree. It was proven that a tree can always be broken up into sub-graphs called generalized caterpillars and caterpillars. The team proved the most efficient way to dominate a caterpillar, which led to the most efficient domination of a generalized caterpillar, and then to the domination of trees. They began their work at math camp over the summer, and have continued communicating throughout the last several months to complete the research. 

Awards won at the 2005 ISEF
Fourth Award of $500 - Team Projects - Presented by Ricoh
________________________________________
2007 - MA304 
MATHEMATICAL DEVELOPMENTS OF THE CONTACT NETWORK MODEL
David Michael Price, Stephanie Chan, Clements HS, Sugar Land, TX, USA, Jacob Shapiro, Upper Arlington
Liberal Arts and Science Academy at LBJ High School, Austin, TX, USA

Contact network models of epidemiology use graph theory to predict how a disease spreads through a population network. The probability of infected vertex i transmitting a disease to susceptible vertex j is called transmissibility and is a function of the infectivity of i, the susceptibility of j, and number of times the pair make contact. In 2002, Newman proposed a model which assumed these three components to be independent of each other. By using the mean transmissibility and the distribution of the contact degree, Newman derived analytical expressions for certain disease spread characteristics, including the distribution of the number of individuals that will be infected by a given infected individual, the distribution of the total number of people that will be infected in one outbreak, and the probability of an epidemic. <br><br>We generalize his model to allow all components of transmissibility to be dependent random variables that allow some risk factors to influence the susceptibility and infectivity. Using our model, we derive the disease spread characteristics proposed by Newman. A comparison between the two models is also made to demonstrate the improvement of our model. To explore a scenario in which transmissibilities change over time, we conduct a computer simulation by enhancing an existing algorithm and examine the change in expected outbreak size over time. We find our work to be successful in that it enhances the complexity of the existing model while maintaining its usefulness.<br><br> 

Awards won at the 2007 ISEF
Honorable Mention Award - American Statistical Association
Second Award of $1,500 - Team Projects - Presented by Science News
________________________________________
2009 - MA306 
FRACTAL ANTENNA, A STUDY OF DIFFERENT FRACTAL'S APTITUDE IN TRANSMITTING RADIO WAVES
Claudio Jacobo Gonzales, Christian Meyer, 
Sandia High School, Albuquerque, NM

"More Bars in More Places," one in a long line of catch phrases symbolizing the ever-rising need for wider ranges of signals. One of the most recent solutions is the use of fractals (self similiar figures at minute magnification). Nathan Cohen pioneered the use of fractal antennas and discovered their property to recieve a wide variety of signals with astonishing clarity, in the 1990's. However, how do these same fractals transmit radio signals in relation to their fractal dimension and iterative depth?<br><br>To test, a remote control car (manipulated via a transmitter with different fractal-shaped antennae) was driven until out of signal range, and its distance from the origin measured. In all three test fractals, the higher the iterations on the antenna, the shorter distance the radio transmitted; another trend we noticed in most (not the Koch's Curve) fractals was that, with higher iteration, test values were much closer than in less iterated configurations. This brings the conclusion that fractals do not transmit further distances, as hypothesized, but in fact less; their signal, however, is much more consistent. Additionally, the fractal with a non-integer dimensions transmited further than those with whole dimensions. We conclude that, for cell phones, that fractal antennas may not give a longer range of transmitivity, but rather a more consistent signal that would be able to defeat more sources of disruption than a traditional linear antenna. 
________________________________________
2006 - MA307 
EXTREMAL CAYLEY GRAPHS OF FINITE CYCLIC GROUPS
Joseph J. Lee, Elysia J. Sheu
Hudson High School, Hudson, OH, USA; Elkins High School, Missouri City, TX, USA

(Equations and symbols do not appear on this abstract due to the limitations of this text box. Please see hard copy for full abstract.)<br><br>Let be a finite group, and A a subset of . The Cayley graph Cay( , A) of generated by A is defined as the digraph with vertex set and edge set {(x,y) : }. Because of their symmetry, connectivity, and expandibility, Cayley graphs are widely used to model communication networks. One of the major objectives in this field of study is to maximize the number of points in a Cayley graph given a maximum distance, called diameter, or average diameter and a size for the generating set. We prove that<br><br><br>where denotes the largest integer M so that there exists a set of integers A = {0,±1, , . . . , } such that the average diameter of the Cayley graph Cay( , A) is at most r. For higher k-values, we wrote a computer program to determine M(d, k) for any d and k, where M(d, k) denotes the largest possible number of points in a Cayley graph, generated by a set of k elements, with a diameter d. The results can also be used to find the optimum k for any d and M as well as the optimum d for any k and M. Formulas for M(d, k) and with various k-values are found. Based on the numbers and formulas thus obtained, it is conjectured that for any d, r, and k, 

Awards won at the 2006 ISEF
Fourth Award of $500 - Team Projects - Presented by Science News
________________________________________
2005 - MA308 
SERIAL NUMERICAL RECOMBINATIONS: PHASE 2
Daniel B. Beksha II, Matthew J. Hall
Bishop Feehan High School, Attleboro, Massachusetts, USA

This year’s research includes advances to last year’s original number recombination algorithm. The extension of the domain to include fractional values for the fundamental parameters enables more numbers to be considered within the context of the theory. This increase in numbers served to illuminate several patterns in dependent parameters that had yet been masked. It is hypothesized that the extension of the domain to include all positive real numbers (and zero) would make the resultant tableau of last year’s research totally expressible formulaically. Also, making the fundamental parameters continuous variables would lead to the supposition that the summation formulae of last year’s research are actually precursors to simpler integral formulae, and that the tableaux are precursors to fields.<br><br> Perhaps the most compelling achievement of this year’s research is the implementation of the theory in an encryption technique. Exploiting the unique feature of the tableaux, namely that they are exceedingly random for small, select regions, but completely predictable on the whole given a certain fundamental parameter, an algorithm for encryption based on looped ciphering was developed. A manifestation is presented as a Windows-based application. Try to decipher the following: {738278928574107716438782858799165717598066287039241 429399510771643878285879916571759807457291912485763971574317181430865754874572919115743171814308657548165717598414293995828587991740034779165717598074572919149715279417400347799943055883148634362}
________________________________________
2009 - MA308 
UNCOVERING INTERNEURONAL GENETIC PATHWAYS THROUGH PARTIAL CORRELATION NETWORK ANALYSIS
Lennie Han Zhu, Jason Rudin, 
Half Hollow Hills High School West, Dix Hills, NY

The Purkinje and interneuronal basket cells are two cell types in the cerebellar cortex, noted for their GABAergic inhibitory relationship. However, in comparison to the relationships between Purkinje cells and other neurons, the Purkinje-basket relationship has been relatively under-analyzed. We attempt to fill this void by developing a novel partial correlation analysis method to examine the Purkinje and basket cell interactions, yielding significant biogenetic pathways, as well as the identity of “hub genes”: control centers for various gene functions. The implementation of a new technique, partial correlation network analysis, enabled the underlying genetic pathways to be constructed and visualized. Evidence supports our theory that HK2 is the basket cell gene modulating the down regulation of action potential and signal transduction in Purkinje cells. The gene PTMA has also been shown to possess a positive correlation with Purkinje cell axonogenesis. One set of non-corroborative results indicates the limitations of the traditional hierarchical clustering analysis, arguably the most widely adopted method for the given type of study, and alternatives were established. These results provide new insights into the interactions between the two cell types, and our methods set a precedent for the application of partial correlation network analysis to interneuronal genetic pathway research. This statistical method can also be implemented in multiple fields, such as data mining, climatology, and disease pathogenesis, which involve the analysis of complex datasets influenced by various factors. 

Awards won at the 2009 ISEF
Third Award of $1,000 - Team Projects - Presented by Science News
________________________________________
2004 - MA308 
ANALYZING SIMPLE LINEAR PATTERNS IN N-DIMENSIONAL K-WRAPPING SPACE
Aimee Ann Covert, Nathan Hembroff-Formsma
Battle Creek Area Math and Science Center, Battle Creek, Michigan, United States

The traditional game of tic tac toe is won by placing three points in a straight line pattern on a board consisting of a 3 by 3 grid in 2-D Euclidean space. This board can be modified in several ways to change the nature of the game. More dimensions can be added to the board, changing it to a 3-D, 4-D, … n-D Euclidean surface. Also, opposite edges of each dimension of the board can be wrapped around to form a continuous surface from one “end” of the board to the opposite end.<br><br> Let n be the number of dimensions in a given tic tac toe board, and k is the number of these dimensions that wrap. k and n are positive integers where k <= n.<br><br> For each combination of k and n, there is a specific number of possible wins. The purpose of this project is to derive a method for calculating the number of wins on a tic tac toe board in n-dimensional, k-wrapping space. Through visual and numerical analysis, combinatorics, and the development of several mathematical proofs, a procedure is developed to determine the number of ways to place three points on an n-dimensional, k-wrapping board so that they form a valid win. Possible applications include encryption. 

Awards won at the 2004 ISEF
Third Award of $1,000 - Team Projects - Presented by Science News
________________________________________
2006 - MA309 
DEMYSTIFYING THE HUMAN BRAIN AN EXPLORATION OF ARTIFICIAL NEURAL NETWORK DEVELOPMENT
Matthew Huang, Nathan Gould
Scarsdale High School, Scarsdale, NY, USA

Traditionally, artificial neural networks have been particularly useful in modeling brain function. Past studies have developed artificial neural networks that are able to perform the same tasks that the human brain can perform. The limitation of this traditional approach to neural networks is that, while able to produce results similar to the human brain's, it does not guarantee an analogous process. Therefore, this traditional approach provides no insight into the process that the brain goes through to complete tasks. In this study, we explore evolution and learning as two paradigms for artificial neural network development. The two paradigms were tested by simulating the completion of a gradient ascension task. The neural network was developed using, first, a process of evolution, and then, second, a process of integrated evolution and learning. The results of this study show that our development paradigms of evolution and learning parallel their biological analogues. These two paradigms can now be used in artificial neural network studies to gain insight into how the brain performs tasks. Future research will involve applying these new paradigms to old and new neural network problems alike to gain a deeper understanding of the biological brain. 
________________________________________
2003 - MA309 
NEW BOUNDS FOR THE DIAMETERS OF K-PATH GRAPHS
Jeremy Takashi Warshauer, Alan Taylor, Hannah Chung
San Marcos High School 1301 State Highway 123, San Marcos, TX, USA

A generalization and improvement on the bounds of the diameters for k-path graphs is presented. Let D(Pk(G)) and D(G) denote the diameters of Pk(G) and G, respectively. First, a lower bound for D(Pk(G)) is given by D(G) ¡V k. Belan and Jurica [3] provide an upper bound of D(G) + k2 ¡V 2 for D(Pk(G)), but only for 2 „T k „T 4. Adding reasonable restrictions on the girth and the degree of each vertex in a graph G, it is proven that D(Pk(G)) „T D(G) + 2k, for all positive integers k. <br><br>Graph theory has significant applications in interconnection networks. By limiting the size of the diameter of a graph that represents an interconnection network, the time delay in data transmission can be minimized. It is shown that the diameters of all path graphs admit a relatively small upper bound, thus lending them to be used as efficient models of interconnection networks.<br><br> 

Awards won at the 2003 ISEF
Honorable Mention Awards - American Mathematical Society
________________________________________
2003 - MA311 
PREVENTING SMALLPOX EPIDEMICS USING A COMPUTATIONAL MODEL
Chintan Hossain, Hiren Hemant Patel
The Charter School of Wilmington, Wilmington, DE, USA

Smallpox is a highly contagious disease that can cause epidemics, something that is of great concern in today’s geopolitical climate. Vaccination is an effective way to prevent epidemics. However, vaccinating large groups of people is expensive, takes a significant amount of time, and kills about one in a million people from the vaccine itself. Therefore, it would be ideal to vaccinate as few people as possible. Vaccinating a portion of the population may be sufficient to prevent a smallpox epidemic from occurring. The purpose of this study is to determine the minimum proportion of the population that needs to be vaccinated to prevent an epidemic. To find this proportion, a mathematical model is developed to study the spread of smallpox through a social network graph. The stages of smallpox are modeled using a Markov graph. A program is written in C++ to simulate the model. In order to prevent an epidemic, the simulation is run with the people who have the most number of contacts vaccinated. The percentage of the population vaccinated is varied, and the results are analyzed. It is found that for the communities simulated, vaccinating 50% of the population effectively prevents an epidemic. Although vaccinating a smaller proportion may not prevent an epidemic, it does reduce the severity of the epidemic. This model is very general, and can simulate other social network and diseases. It can also simulate various vaccination programs, including random vaccination, selective vaccination (e.g. first-responders), and secondary vaccinations. 
________________________________________
2005 - MA312 
BEE THE BUILDER
Rachael JoAnn Emory, John Oscar Hilton
Wagoner High School, Wagoner, Oklahoma, US, 74467

The purpose of this project is to prove that a grid, or floor plan, of regular hexagons is the ideal architectural format to use when building edifices with a large amount of rooms, such as office buildings, storage facilities, hotels, prisons, schools, apartments, etc.<br><br> To begin, a constant amount of material for each shape and for each grid must be determined. The length chosen will be the control of each set of equations used to determine the area of the individual shapes and of the grids. Find the area of the square, equilateral triangle, circle and hexagon. Then find the area of the grids of circles, hexagons, squares, and triangles.<br><br> The results of the experiment are what we hypothesized. The grid of hexagons has the greatest area out of all of the other grids. The circle has the greatest area out of all of the other shapes. Though the circle alone has a greater area, what can a cirlce do the architecture? If a circle was to be put into a grid there would be a dramatic loss in area with respect to the material. To use the gains that the circle has, a building would have to have only one room. The results can help to encourage a very different type of building architecture, hexagonal. Buildings can be built cheaper and there will be no lost area when the edifice looks like a bee hive. The look of the buildings will be unique and economically more efficient 
________________________________________
2009 - MA312 
FIBONACCI NUMBERS IN THE MUSIC OF CLASSICAL COMPOSITIONS
Qiuzi Zhu, Cara Marie Borelli, 
Hilton Head Preparatory, Hilton Head Island, SC

This project demonstrates the significance of Fibonacci numbers in the chords classical composers used in their compositions. Western music was chosen because it uses the diatonic scale. By assigning values to each note of the chord after counting the half step intervals from the base note, each chord was rewritten using numbers. After data was collected from twelve compositions by four different composers, data analysis programs were used to isolate each Fibonacci number.<br><br> The results support the hypothesis: Fibonacci numbers are prevalent in music; they constitute 52.98% of the total number of intervals. In other words, out of the 3613 intervals that were tested, 1914 of them were comprised of Fibonacci numbers. In theory, the Fibonacci numbers should appear (if the chords’ limit is 1-17 and the number 1 is excluded) 5/16, or 31.25% percent of the time. Therefore, the Fibonacci numbers occur 21.73% more often than they should if the chords were just “random.” A one-proportion z-test was conducted, providing significant evidence that supported the results.<br><br> Thus, Fibonacci numbers do influence chords. The experiment also discovered that the interval of one octave, found in both major and minor keys, is a “Fibonacci number chord”: 1-13. The most important discovery is the fact that the major chord triad is, in fact, a 1-5-8 chord. All three of these numbers are Fibonacci numbers, and all major chords are based on base chord. Consequently, this experiment showed that while the Fibonacci sequence in nature is universally acknowledged, they occur organically in classical music chords as well. 

Awards won at the 2009 ISEF
Fourth Award of $500 - Team Projects - Presented by Science News
________________________________________
2005 - MA001 
UNIQUE TRIANGLE
Tornike Iuri Mosiashvili
Georgian-American High School, Tbilisi, Georgia Republic

Bisector in contrast with medians and heights has special properties. Bisector does not always comply with the roles that medians and heights do. Among these properties is to be emphasized Sharigyn’s Problem that runs as follows: <br><br>If the two line segments joining bases of bisectors are equal, then the original triangle will not always be isosceles (with bases of bisectors we mean point of intersection of bisector and the opposite side of triangle). In case of medians and heights the original triangle will always be isosceles. Sharigyn’s Problem was solved at IMPC in 2000. In our work we aimed at finding some extra properties that always work with medians and heights, but do not always work with bisectors. We found such a property and got fantastic result. We are considering the case when the circle goes through the bases of the medians, heights and bisectors of the triangle and at the same time tangents one of the sides. In the case of medians and heights the original triangle is always isosceles. But in the case of bisectors the following theorem holds true.<br><br>Theorem: Triangle with the circle that goes through bases of the bisectors and tangents one of the sides is not always isosceles. <br><br>In our work we found all the triangles with above mentioned properties. It is to be noted that they are obtuse and their obtuse angle varies in a very narrow interval.<br><br>We also found out the geometric position of the centers of all the circles mentioned above. Sharigyn’s problem and our work clearly shows bisector’s unusual role.<br><br> 
________________________________________
2003 - MA001 
GAME THEORY IN ACTION: PROVING AND COMPUTING WINNING STRATEGIES FOR 'NIM' AND ITS VARIANTS
Hyeyoun Chung
St. Paul's Girls' School, London, United Kingdom

This project had two aims: the first was to produce and prove a set of winning strategies for the game ‘Nim’, a game played using counters, and five of its variants. The second was to write a computer program which could play the game against a person, using the winning strategies to beat the player every time.<br><br>Winning strategies were found and proved by playing the games and looking for patterns in their development. The winning strategies and their proofs both depended on writing the number of counters in each row in binary. This fact, together with the fact that the winning strategy required a series of steps to be repeated at each turn in the game, made ‘Nim’ ideal for incorporation into a computer program. <br><br>The computer language Java was used to write the code for the program. The program was provided with a Graphical User Interface which made it easier for users to interact with the computer, as it displayed the counters and the board on the screen. Two abstract classes were constructed to contain all the elements shared by the two main classes of games. These were then used as templates to construct specialised classes for each of the games, to prevent repetition of code. All of the algorithms required to carry out the winning strategies were collected together in two method classes.<br><br>Therefore, this project provides an unusual illustration of a mathematical proof, through a computer program which relies on the proof to function. <br><br> 

Awards won at the 2003 ISEF
Third Place Awards of $250 - American Mathematical Society
________________________________________
2007 - MA002 
RECURRENCE RELATION FOR CONGRUUM PROBLEM SOLUTIONS
Lado Meskhishvili
Georgian-American High School, Tbilisi , Georgia

A positive integer r is a congruent number if we can find a rational right triangle with area r. Otherwise, r is a congruent number if and only if there is a rational number x such that: triple x-r, x, x+r are each the squares of rational numbers. The latter is called the congruum problem for an integer r.<br><br> The congruum problem has a very old history. This problem , for r=5 , was posed by the mathematicians Theodore and Jean de Palerma in a mathematical tournament organized by Frederick II in Pisa in 1225. Well-known mathematician Fibonacci found solution: 1681 / 144,and proved that all numbers r (the congrua) cannot be a square. This result is equivalent to the Fermat's right triangle theorem.<br><br> The modern way to consider congruum problem is investigation rational points on the elliptic curve: y^2=x(x^2-r^2).<br><br> It is known, that if r is congruent number, then corresponding elliptic curve has infinitely many solutions. How to find these solutions? In the modern works they are searched by means of computer. In spite of computer's great abilities their possibilities are limited.<br><br> In my project it is decided finding congruum problem solutions. I have found out recurrence relation for congruum solutions, which enable as to find as many as possible congruum problem solutions. For example above-mentioned problem (case r=5) besides Fibonacci’s solution has other solutions, which are found by using my method. Second term (solution) in my recurrence sequence is <br><br> 11183412793921 / 2234116132416<br><br>and third one is<br><br> 249850594047271558346480641^2 / 53545229862821602092291248^2<br><br> Next solutions can be easily obtained by my recurrence formula.<br><br> 

Awards won at the 2007 ISEF
Honorable Mention Award - American Mathematical Society
________________________________________
2005 - MA002 
DIOPHANTINE RECTANGULAR PARALLELEPIPED
Lasha Emannuel Margishvili
Georgian-American High School, Tbilisi, Georgia Republic

Rectangular parallelepiped is a common space figure. We often come across to it in everyday life. For instance, most of the rooms have a shape of a rectangular parallelepiped. This figure has three plane diagonals, which are edge’s diagonals and one space diagonal. Together with rectangular parallelepiped’s three-dimension, they make up 7 numbers (Magnificent Seven). It is well know that there is a rectangular parallelepiped where three dimensions and plane diagonals are integers. It’s clear there is a rectangular parallelepiped where its dimensions and space diagonal are integers. We aimed at exploring a rectangular parallelepiped where dimensions, plane diagonals and space diagonal are integers. As long as the solution of the equation in integers started with Diophantous so we named such rectangular parallelepiped as Diophantine rectangular parallelepiped. Above mentioned problem was put forward by Academician Boltiansky in 1986, and as long as we know it has not been solved until now. In our work we proved the theorem that runs follows:<br><br>Theorem: Diophantine rectangular parallelepiped does not exist. <br><br> 

Awards won at the 2005 ISEF
Award of $1,000 - Mu Alpha Theta, National High School and Two-Year College Mathematics Honor Society
________________________________________
2009 - MA003 
NUMERICAL FRACTIONAL DIFFERENTIATION AND INTEGRATION
Alexander Slavik
Gymnazium Brno - Reckovice, Czech Republic, Brno, CZECH REPUBLIC

The purpose of the project is to describe and compare different methods for numerical fractional differentiation and integration. This means differentiation and integration of non-integer order, problems with which the theory of fractional calculus deals. The following three methods are discussed: Grünwald-Letnikov definition based, Riemann-Liouville definition based and Fourier series based.<br><br>In order to compare these methods, they were implemented in example programs written in Object Pascal language. All these methods process set of discrete points describing the differentiated/ integrated function, which corresponds with what is usually seen in practice. The Grünwald-Letnikov definition based method is the fastest one. However, the method is restricted only to equidistant sets of input points and it often gives significantly imprecise results when there are not enough input data. In some cases the result is imprecise even though there is enough data.<br><br>The Riemann-Liouville definition based method is almost only an ordinary numerical integration, therefore it can process input with variable spacing between points. Because the original Riemann-Liouville definition covers only fractional integrals, repeated ordinary differentiation is needed to achieve fractional derivatives. It generally computes with the best precision, which is compensated by higher computation complexity.<br><br>The Fourier series based method uses representation of the function in terms of Fourier series. This series can be fractionally differentiated or integrated term by term. Such method of computing is usually very imprecise and inefficient, but it can be used to compute Weyl fractional derivatives (that is, with lower limit negative infinity) of a periodic function. 
________________________________________
2007 - MA003 
REFLECTION WITHIN CONICS
Hao Chuien Hang
Hwa Chong Institution, Singapore, SINGAPORE

This project represents my effort to prove the following theoretical observations of the behaviour of light paths within conics. <br><br>Within an internally reflecting ellipse, a light ray that initially<br><br>1. passes between one of the foci and the nearer major vertex will have its paths tangent to another ellipse that is within this internally reflecting ellipse, with both ellipses sharing the same foci;<br><br>2. passes between the two foci will have its paths tangent to a hyperbola that shares the same foci.<br><br>Within an internally reflecting hyperbola, a light ray that initially<br><br>3. passes between one of the foci and the nearer vertex will have its paths tangent to another hyperbola that is within this internally reflecting hyperbola, with both hyperbolas sharing the same foci;<br><br>4. passes beyond one of the two foci will have its paths tangent to an ellipse sharing the same foci.<br><br>Within an internally reflecting parabola, a light ray which<br><br>5. does not pass through the focus at any point in time will have its paths tangent to another parabola.<br><br>My search through basic texts on geometry, including those in the references, and the internet did not reveal any available proofs of the results mentioned above, which I believe could have been done before. The proofs to these results are not trivial, though only elementary mathematics is needed. In my opinion, within my limited knowledge and experience in mathematics, I find these proofs neat and elegant. <br><br> 
________________________________________
2007 - MA004 
CYCLIC POLYNOMIALS COEFFICIENTS INVESTIGATION
Sergey Mikhailovich Drozdov
Lyceum 'Physical-Technical High School', St.-Petersburg, Russia

Consider the primitive n th roots of 1 and the polynomial which has all these roots. Call those polynomials cyclotomic (or cyclic). If n is less than 105 all the cyclic polynomials have coefficients equal to 1 and -1. 105th degree polynomial has coefficients -2, coefficient 3 appears at the degree 385, and coefficient 4 at the degree 1365. There are two assertions.<br><br>1. If n is a product of two prime numbers cyclic polynomial has coefficients 1 and -1<br><br>2. If n is a product of three prime numbers cyclic polynomial coefficients are not more than 2 multiplied by the least of these prime numbers; if that prime r is fixed, such p and q can be found, that r-7 coefficient exists.<br><br>Although (1) is well-known, (2) is proved in this work, using a method of representing negative degrees of polynomials as infinite series. Thus, cyclic polynomial is a product of simple polynomials and infinite series. So the problem is just a question of linear representation distribution.<br><br>If n is a product of two different prime numbers, it’s coefficients are 1 and -1. Similar method may be used if n is a product of three prime numbers, (2) is constructed using Dirichlet’s theorem.<br><br> 
________________________________________
2008 - MA004 
A RATIONAL EXPRESSION OF IRRATIONALITY: AN EXPLORATION OF PERIOD LENGTHS OF THE CONTINUED FRACTIONS OF IRRATIONAL NUMBERS
Rebecca Jean Rapf
Sheridan High School, Sheridan, WY

The purpose of the project was to use the periodic nature of the chain of terms in the continued fractions of irrational numbers to explore certain properties of those numbers. <br><br>It was predicted that one could use the length of the periods of continued fractions to examine the properties of irrational square roots.<br><br>Initial research was done on continued fractions and how to calculate them for irrational square roots. A Python program was written to calculate the continued fractions of irrational square roots through two repetitions, as well as to calculate period length. The resulting data was analyzed and trends were identified.<br><br>The result was that, after analysis, certain patterns based on period length emerged. These were especially apparent for numbers near squares. It appeared that these patterns will hold. 
________________________________________
2006 - MA005 
REGULARITY PROPERTY OF RANDOM WALKS
Dmitry Todorov
Centre of Mathematical Education, Saint-Petersburg, Russia

Random walks are used in physics, chemical kinetics and mathematics. They are related to algebra, functional analysis, ergodic theory and probability theory. We study its properties in the context of probability theory.<br><br>This paper is the first in the series of works describing some properties of random walks with deviation equal to square root of m. Using the regularity property of random walks in [1] an essential for ergodic theory theorem has been proved. Random walk can be defined as a sequence of elements of n-dimensional cube. Almost all of the random walks are regular. We show that it means that for most of the pairs of random walks there exists a number m for which both random walks have deviation less then square root of m. <br><br>The results obtained in this work could be used as an important tool for researches related to iterated metric on the sequences of the pasts of the Kalikow's endomorphism. Also using these results some new examples of measurable partitions of measure space with polynomial scaling can be obtained. The existence of such examples gives answers on some questions put by world-famous mathematicians. <br><br> <br><br> 
________________________________________
2003 - MA005 
RANDOM WALKS AND HANDSHAKES
Brian Todd Rice, Southwest Virginia Governor's School
Dublin, VA, USA

This project addresses the question: "Given two random walks restricted to a d-dimensional lattice and originating from the same point, what is the probability that they meet (both arrive at the same point) after a given n steps, and what is the expected total number of these meetings?"<br><br>The first part of the problem was solved combinatorially, with closed-form expressions for the cases d = 1 and d = 2 proven. Two solutions for the case d = 2 were found, one of which was applied to solving the case d = 3 and the general case of any positive integer d. A recursive formula for the general case was proven from the expression obtained using this method.<br><br>The second part of the problem was then addressed. The expected total number of meetings was proven to be infinite for the cases d = 1 and d = 2 using convergence tests. An alternate method of approach led to a connection with Pólya's theorem on random walks concerning the general case. 

Awards won at the 2003 ISEF
Honorable Mention Awards - American Mathematical Society
Fourth Award of $500 - Mathematics - Presented by Panasonic Consumer Electronics Company
________________________________________
2009 - MA006 
ORBITAL ORIGAMIS, STABILIZERS OF STAIR-ORIGAMIS AND THE PROPERTY CN.
Aliaksandr S. Minets
Lyceum of Belarusian State University, Minsk, BELARUS

Origami is a finite set of copies of the Euclidian unit square with identifications of edges by the following rules:<br><br>1. right edge of each square is glued by translation to the left edge of a square; <br><br>2. upper edge of each square is glued by translation to the lower edge of a square;<br><br>3. topological surface obtained by such a gluing is connected.<br><br>Examples: trivial origamis T(m, n), origamis L(m, n), stair-origamis E(2k-1) and E(2k). <br><br>Origamis form a particular family of translation surfaces (objects of Teichmuller theory). There are unsolved problems about stabilizers and orbits of origamis under the natural action of SL(2,Z).<br><br>In present work we introduce and explore orbital origamis and series of origamis – notions closely related to stabilizers of origamis. <br><br>Theorem 1. For every positive integer n, the series of stair-origamis E(n) are cyclic.<br><br>Theorem 2. For any m, n > 3 the origamis T(m, n), L(m, n), E(n), X(n), XS(n) are not orbital.<br><br>One of our main results is the full description of stabilizers of stair-origamis:<br><br>Theorem 3. For all positive integers k, the stabilizer of the origami E(2k-1) is index 3 subgroup of SL(2,Z) containing Gamma(2). The stabilizer of the origami E(2k) is the principal congruence group GAMMA(2).<br><br>We also investigate an important geometric construction. Say that an origami has the property c2 if the lengths of its cylinders are not greater than 2 (and 2 is achievable). <br><br>Theorem 4. The only origamis having the property c2 are X(1), T(1, 2), T(2, 1), T(2, 2) and E(n), EC(n) for n>=3. 
________________________________________
2006 - MA006 
A PROBLEM ABOUT THE SUM OF DIGITS OF A POSITIVE INTEGER IN ARBITRARY NUMERATION BASIS
Adrian Ioan Zahariuc
"Colegiul National Ferdinand I" High School, Bacau, Romania

Initially, our aim was to study the behaviour of the sum of digits of a^n where a is fixed. There are several problems and questions related to this, all being very hard and most of them unsolved. Even the fact that the sum of digits of a^n tends to infinity has no easy or elementary proof for most values of a (except a=2,4,6 or 8). A problem we will particularily be interested in is the following: the case when the sequences of the sum of digits of the powers of 2 positive integers coincide, i.e. s(a^n)=s(b^n) for all n, can occur only in the trivial case when their ratio is an integer power 10.<br><br>Studying this properties turned out to be exceptionaly hard and this is why, we will turn our attention to the sum of digits of the multiples of a given integer. The aim of this short project is to solve the analoguous problem of the upmentioned one, namely to prove that s(an)=s(bn) for all n implies lg a-lg b is integer. We will solve this problem in the more general case of an arbitrary numeration base b. Surprisingly, the proof depends a lot on some properties of the number b. We will see that the case b prime is very easy, while the case b nontrivial power is more difficult.<br><br>The main idea is to choose a sequence of multiples of one of the two numbers which is very close to the powers of b, which means that its sum of digits is bounded. With some work, we can reduce the problem to the case when one of the numbers divides the other. The final argument involves Kronecker's lemma. 
________________________________________
2005 - MA006 
GRAPHS AND MAPPINGS GENERALIZATION OF GRAPH COLORING
Alexandr Kazda
High School, Nad Aleji 1952, Prague 6, 162 00, Prague, Czech Republic

The project inquires into graph theory, an important part of discrete mathematics. Coloring a graph means to map the vertices of the graph to n colors so that no two neighboring vertices have the same color. The project proposes and studies a generalization of graph coloring that allows to define relationships between colors.<br><br>I have considered homomorphic mappings of one graph to another such that neighboring vertices are mapped to neighboring vertices. I have observed that coloring of a graph G by n colors is a homomorphism from G to a complete graph on n vertices -- therefore graph colorings are a special case of graph homomorphisms.<br><br>Such generalization brings new questions. For a given graph G, the chromatic function f_G(H) is given by the number of homomorphisms of the graph G to graph H. I have derived that no two non-isomorphic graphs can have the same chromatic function, while for any F finite set of graphs I can construct two non-isomorphic graphs whose chromatic functions agree on F.<br><br>I have used graph-to-graph mappings to formulate a novel proof of the famous Cayley theorem about the number of spanning trees of a complete graph on n vertices. My proof utilizes (non-homomorphic) mappings of a path of the length n-3 to a complete graph on n vertices.<br><br>Homomorphisms, like colorings, have applications in problem-solving. Many problems, for example assigning frequencies to transmitters so that no two neighboring transmitters interfere, can be formulated in the language of graph homomorphisms. 
________________________________________
2003 - MA006 
A NOVEL MATHEMATICAL MODEL OF B-DNA IN NUCLEOSOME
Daniel David Graves
Nicolet High School, Glendale Wisconsin, USA

The purpose of this project was to generate an idealized, detailed, atomic, mathematical model of B-DNA in a nucleosome. This project sets out to create a series of equations that can predict the location of atoms in any base pair sequence. The location of atoms in the idealized model can then be compared to x-ray diffraction results, enabling mutations and structural changes to be located.<br><br> To create a mathematical model of a nucleosome, three steps were necessary. First, a mathematical description of B-DNA that has a linear helix axis was created. Second, the methodology for finding the coordinates of every atom of each of the four nitrogenous base of DNA was developed. Finally, equations for B-DNA in a superhelical formation were derived by using vector calculus to wrap helices around a helix. As a means of using the equations to derive coordinates, a computer program was written to generate a Protein Databank file of an idealized nucleosome.<br><br> This study creates a new, unique procedure that can be used to create an idealized model of a compound with a repeating unit, such as a nucleotide. This can help with both visualization of a structure and with any analysis that seeks to determine how a specific structure is wound or altered from a “normal” version.<br><br> 

Awards won at the 2003 ISEF
Third Award of $1,000 - Mathematics - Presented by Panasonic Consumer Electronics Company
All expense-paid trip to attend the U.S. Space Camp in Huntsville, Alabama and a certificate - National Aeronautics and Space Administration
________________________________________
2004 - MA007 
CHARACTERIZATION OF THE FREE-SOLVABLE GROUP SOLVABILITY LEVEL 2 WITH N GENERATORS ON THE DIMENSION N INTEGER GRID
Sergey Sergeevich Sinchuk
Center of Mathematical Education, St. Petersburg, Russia

Let us observe the free group of dimension 2 with generators a and b, every element of this group can be represented as finite set of elements a and b and their back elements, for example x = aba-1b-1a. If we observe integer grid on the plane, we can match by the record of the element x the path on it, which is constructing with the following algorithm. The beginning of the path is (0,0) point. The present of the letter a on the some position in word x means one step to the right, the a-1 means step to the left, b one step up and b-1 one step down. This way, observing all letters of the path sequently we can construct the path on the integer grid. Then we observe a some special function, which is called geometrical image for any group with 2 generators as crossing the geometrical images of free group which are defining the one element of the group.<br><br>Then we say that the group fits some geometrical property if for each element of the group e can match one-to-one with its geometrical image. <br><br>In this work was proved that there is one and only one group that fits geometrical property. It is the free solvable group with solvability level 2.<br><br>Lemma 1. If H>Fn , then there are different elements in Fn || H which have the same geometrical image.<br><br>Lemma 2. If H>Fn , then there are also different elements in Fn || H which have the same geometrical image.<br><br>Theorem of characterization. In Group Fn|Fn and only in it for different elements the different geometric images are matched.<br><br> 
________________________________________
2005 - MA007 
MINIMIZATION OF A SHIFT AS A PROBLEM OF HARMONIC ANALYSIS
Alexander Rotkevich
Center of Mathematical Education, Saint-Petersburg, Russia

The problem, which I investigated here, concerns with a minimization of aconcrete nonlinear functional. The result may have practical applications to radio engineering, because the very form of that functional was introduced after study of some mathematical aspects of radiowave's emission and identification. The mathematical difficulties of a problem consist both in nonlinearity and in necessity to handle simultaneously a function and its Fourier transform. The situation is in a sense similar to that one for some classical theorem of harmonic analysis. Let's consider problem entering into more details. One may consider a bounded real-valued function with compact support defined on the real axes as emitting signal. Then it is standard to interpret the Fourier transform of it as the relevant radiowave. A receiver of a wave operates usually in a prescribed frequency range, so we interested only in such signals for which the most part of a wave is distributed in those range. It means mathematically a crucial restriction on the Fourier transform of a signal. The further circumstances stemming from the radio engineering give rise to the convolution of a signal with a shift of itself and to the main object of minimization - to the nonlinear functional I(f). The problem of a minimization of I(f) with regard to a wave's energy restriction and with a normalization of energy turned to be very difficult and hasn't yet an optimal solution. My achievements in solving of this mathematical problem are as follows:<br><br>1.I have proved a theoretical lower bound for I(f).<br><br>2.I have tested computationally and theoretically different functions which seem empirically near to best ones for minimizing I(f). So I have practical scope for the possible value of a mentioned functional. Perhaps it may be useful for a practical constructing of devices based on mentioned ideas. 
________________________________________
2003 - MA007 
ON DECOMPOSITIONS OF CONTINUOUS AND DIFFERENTIABLE FUNCTIONS ON PLANAR SETS
Alexey V. Baran
AES Centre of MSU, Moscow, Russia

Text is not transferable. Abstract is on file for review. Missing Formulas. JI 03/26/03<br><br>The notion appeared in studies of Hilbert's 13th problem on superposition. Denote . A closed subset of square is called C-basic, if for every continuous function there exist continuous functions and such that for each point from . Let be a subset of plane . Then a function is called continuous at point , if , where and . V.Arnold posed the following problem: find conditions on a subset , under which is C-basic. This problem was solved by Y.Sternfeld. A closed subset of square is called D-basic, if for each differentiable function there exist differentiable functions and such that for any point from . Let be subset of plane . Than a function is called differentiable at point , if , where and . In 1997 V.Arnold posed the following smooth version of his problem: find conditions on a closed subset under which is D-basic. Solution of this problem is unknown. Main theorem. a) There exist a D-basic but not C-basic subset of the plane. We can take ; b) There exist a C-basic but not D-basic subset of the plane. We can take . 

Awards won at the 2003 ISEF
Honorable Mention Awards - American Mathematical Society
Awards of $5,000 - Intel Foundation Achievement Awards
________________________________________
2010 - MA008 
PROBLEMS OF DISTANCES AND THEIR APPLICATIONS TO THE MEASUREMENTS IN PRACTICE
Uy Van Ha Nguyen
Le Quy Don High School for Gifted Students, Da Nang, VIETNAM

In the Math text-book for (Vietnamese) students of 8th grade, we met a practical problem where the students are asked to measure the height of a flag-pole, say LU, using a ruler of 1 meter in length and a sighting mechanism. The students are instructed to measure the distance between an appropriate position of observers, say AB, and the flag-pole LU and to measure the angle between AL and the “horizontal” direction (by using the sighting mechanism) and to apply the relations in right-angled triangles to find the length LU.<br><br>That measuring method would not work if LU is not “vertical.” Moreover, in the above method, the students had to go back and forth (to measure the distance between AB and LU).<br><br>In the present project, we are interested in a more general problem: measuring the distance between any two points “hanging in the mid space.” Using the relations in general triangles (not only right-angled ones), we first prove mathematically that the problem can be solved without pacing back and forth. We can go further to solve more complicated problems. For instance, we could measure the perimeter, area, radius of circumcircle of any triangle in the space.<br><br>To accomplish the project, we next try to make a measuring device that allows us to measure (at least) two angles at the same time. The key to the question is that our device carries two sets of flexibly movable protractors, the distance between the two sets being kept constant. The device is our handiwork, made from available materials. 
________________________________________
2008 - MA008 
THE DISCRETE SIMPLEX ASSEMBLY OF THE HYPERCUBE AND ITS ALGEBRAIC CONSEQUENCES
Philip Samuel Chodrow
Robert E. Lee High School, Staunton, VA

This project investigates the problem of assembling the hypercube using discrete simplexes. We first define a discrete simplex and derive an expression for its content. We then demonstrate that for every discrete figurate able to be represented by a polynomial in a spatial parameter, there exists a unique set of discrete simplexes depending on that same spatial parameter of equal content. We then derive an expression to specify this set for the special case of the hypercube and offer other examples for a variety of figurates. However, it is generally impossible to assemble the cube or other figurates while maintaining the geometrical integrity of the component simplexes. By taking limits, we demonstrate agreement with the analogous problem for continuous hypercubes and simplexes.<br><br> The expression derived to specify the simplex set also has interesting algebraic properties, which we examine. We prove a recurrence relation governing the numbers of simplexes in each simplex set for the hypercube. We then demonstrate that this expression generalizes the Faulhaber formula, offering a closed form of iterated power sums. 
________________________________________
2004 - MA008 
AN INTERESTING PROPERTY IN TRIANGLES
Sameer Yeleswarapu
S F S High School, Hyderabad, Andhra Pradesh, India

One of the fundamental properties of a triangle is that the sum of two sides is greater than the third side. This is a statement of inequality but does not quantify the value by which the sum of two sides is greater than the third side. <br><br> <br><br>A measure 'DELTA' by which the sum of two sides exceeds the third side has been defined. This measured DELTA has been defined to the best of our knowledge for the first time in this work. This work reports a detailed study of several properties of DELTA.<br><br> <br><br>Using DELTA measurements of different triangles, the author discovered some new properties and also proved them using trigonometry. The basic property discovered can be stated as “ In a given triangle, for a pair of parallel lines to any of the sides that form sub similar triangles, the difference between the DELTA measures of the sub similar triangles, is a constant and is independent of the location of the pair of parallel lines, as long as the separation between the pair of parallel lines is fixed."<br><br> <br><br>Working on the properties of the measure DELTA and related quantities helped the author to arrive at an efficient method for computing the perimeter of a right angle triangle.<br><br> 

Awards won at the 2004 ISEF
Honorable Mention Award Certificates for International students and students under the age of 16 - National Aeronautics and Space Administration
________________________________________
2004 - MA009 
KOBON TRIANGLE SOLUTION
Fred Louis Iacoletti
Robinson Secondary School, Fairfax, Virginia, United States of America

The Kobon Triangle Solution was a successful experiment that proved that the Kobon Triangle Sequence was not sporadic and that it corresponded with an x-y Cartesian coordinate function. Kobon Triangles are non-overlapping triangles that can be pieced together by an "n" number of straight-line segments. For example, three line segments can form one triangle at a maximum. Four line segments can form at most two triangles. Five line segments cannot form more than five triangles.<br><br>A pattern had to be found and then placed into a function to prove the Kobon Triangle Sequence was not sporadic. Because one is required to use the scientific method, many “trial and error” experiments were used. There were many different types of equations each tested multiple times to achieve accurate equations. The experiment also required the integrated use of computational methods (i.e. calculators and computers). There were a TI-82 graphing calculator, a Windows XP Calculator, and a TI-85 Computer Graphing Program.<br><br>Ultimately, three simple equations were developed. Both involve trigonometry, quadratics, as well as operations that involve number theory (advanced arithmetic). The input is the “n” number of line segments that make up the output, or the maximum number of non-overlapping triangles formed by the line segments on a non-Euclidian plane.<br><br>This experiment demonstrates that the Kobon Triangle Sequence has a pattern. It is also possible to place the Kobon Triangle Sequence into a function, hence proving it is periodic. 
________________________________________
2009 - MA009 
THE COMPUTER SIMULATION OF AVASCULAR TUMOR GROWTH WITH SURGICAL THERAPY
Martin Panko
Gymnazium Exnarova 10, Kosice, SLOVAKIA

This interdisciplinary study presents discrete model of dynamics of malignant biological tissue at cellular and extracellular level. The purpose of my model is the investigation of growth of the planar, "in vitro" tumor at qualitative and quantitative level as well.<br><br>The computer simulation performed for given model mimics spatio-temporal patterns which are observable in real tumor growth conditions. The presented form of model is based on the stochastic variant of cellular automaton updated in a way known as Monte Carlo method. The technological conception of my model allows per cell manipulation that provides the ability of simulating the surgical therapy on tumor. There are several physiological processes implemented in this model and cells are distinguished into several populations. The model encompass quantitative descriptions of several phenomenons which affect tumor growth.<br><br>The results indicate layered structure of virtual multicellular spheroid that is composed of necrotic core surrounded by a ring of quiescent cells and proliferative cells residing on tumor periphery. Simulations provides quantitative phenomenological description of spatio-temporal dynamics of avascular, planar tumor. My findings are consistent with experimental biological growth data confirming well known Gomprerzian macroscopic law. My model also indicates that invasion of the tumor depends on the ballance between the mechanisms of cell proliferation and migration. 
________________________________________
2010 - MA009 
EXTREMAL PROPERTIES OF L.C.M. AND G.C.D. FOR SEQUENCES
Volha U. Shumskaya
Gymnazium #56, Minsk, BELARUS

Some tasks of P. Erdos are the basis for this research. The tasks present two problems investigated and generalized in our project. <br><br>Various sequences of s different numbers are chosen from the integers belonging to a segment [1, T]. Let denote such sequences by As. The main purposes of investigation are to find out or evaluate the following values. <br><br>1) Maximum of minimum values of the least common multiple (l.c.m.) of m integers, where the minimum is taken among all sets of m numbers chosen from above sequences and maximum is chosen among all such minima calculated for any sequences As. <br><br>2) Minimum of maximum values of the greatest common divisor (g.c.d.) of m numbers, where the maximum is taken among all sets of m numbers chosen from above sequences and minimum is chosen among all such maxima calculated for any sequences As.<br><br>For investigations methods of number theory, analysis and particular results of other similar tasks are used.<br><br>The main results of the project are the following: the first problem has been fully solved for various s and for m = 2. Also, it has been fully solved for any m and T = 2s. For the second problem some interesting algorithm and results are obtained. 
________________________________________
2006 - MA009 
SOME TOPOLOGICAL PROPERTIES OF A FAULT-TOLERANT DESIGN
Po-Chun Kuo
National Hsinchu Senior High School, Hsinchu City, Taiwan (R.O.C.)

Recently Kao and Hsu studied an interesting family {BT(n)} of 3-regular planar bipartite hamiltonian graphs, having the property that each member remains hamiltonian when a pair of nodes, one from each partite set, is deleted. It can therefore be viewed as a fault-tolerant design for token ring. Moreover, BT(n) is a 3-regular planar bipartite graph of 6*2^n-4 nodes attaining the minimum diameter of 2n+1. Our investigation consists of three parts:<br><br>Part 1: Connectivity<br><br>Let x and y belong to different partite sets. Then<br><br>1) There exist three internally-disjoint spanning paths joining x and y.<br><br>2) There exists a hamiltonian path joining x and y.<br><br>Part 2: Fault- tolerability<br><br>Let x and y belong to the same partite set. Then<br><br>1) There exist three internally-disjoint spanning paths of BT(n)\{z} joining x and y for any z belong to the same partite set.<br><br>2) There exists a hamiltonian path of BT(n)\{z} joining x and y for any z belong to the different partite set.<br><br>Part 3: Global Extendibility<br><br>There exists a hamiltonian cycle passing through any three given edges in BT(n).<br><br> 

Awards won at the 2006 ISEF
Fourth Award of $500 - Mathematics - Presented by Lucent Technologies
________________________________________
2005 - MA009 
A NEW ALGORITHM FOR DIGITAL SIGNATURES
Gerold Gruenauer
Kepler Gymnasium, Weiden / Bavaria, Germany

Signatures are an essential part of our daily life. Whether they are used for simple postcards or important business letters, they guarantee the authenticity of the signed document.<br><br><br>Unfortunately there is no such security on the internet. A main reason for that is that the existing algorithms for digital signatures are so complicated that they cannot be used by an inexperienced programmer without a big amount of work. Therefore in many software programs, they're left out completely.<br><br><br>In this project a new fast and easy algorithm for digital signatures, the TurboSign algorithm, will be presented. TurboSign is - unlike the well-known digital signature schemes - not based on a number theoretical problem but on a very hard, well researched problem in lattices, namely the closest vector problem (i.e. find the closest integer vector of a given point in a given lattice). Due to this structure the new signature scheme doesn't need any complicated operations and can for that reason be implemented in software programs even by amateur programmers. <br><br><br>The TurboSign algorithm is therefore a good solution for all small software programs, where the implementation of an alternative digital signature scheme is either too complicated or too expensive. As a result the TurboSign scheme can be used to increase the security of the internet. 

Awards won at the 2005 ISEF
Fourth Award of $500 - Mathematics - Presented by Science News
________________________________________
2009 - MA010 
COMPLETE DESCRIPTION OF RECTILINEAR STEINER MINIMAL TREES AND THEIR BIFURCATIONS IN THE CASE OF FOUR-POINTS BOUNDARIES
Antonina V. Fabianskaya
Lyceum #1511 at MEPhI, Moscow, RUSSIA

In the present work we investigate Steiner Minimal Trees on Manhattan planes. They arise in chip design. In particular, we describe bifurcations of such networks, i.e., their spasmodic changes under small perturbations of boundary sets. The obtained results enable to characterize areas of stability of each concrete network form and to control possible catastrophic transformations of the forms under various changes of boundary conditions. The latter is extremely important for constructing reliable models. In the present work the following methods were used:<br><br>1) Graph theory<br><br>2) Combinatorics<br><br>3) Algebra<br><br>4) Extreme networks theory<br><br>5) Bifurcation theory<br><br>Analysis of the obtained results<br><br> Steiner Minimal Trees on Manhattan planes joining four-points boundary sets are completely described. The atlas showing dependence of structure and length of such networks on their boundary sets is constructed. Stability domains of structures of such networks are studied. All possible bifurcations of such networks under deformation of their boundary sets are described. This description is visualized in the form of bifurcation graph. Value for scientifically-practical use: the present work can be considered as one more important step for understanding of the global structure of Steiner Minimal Trees family on Manhattan planes; in practice the results of this type can be used, for example, for constructing more reliable models of chips optimized in speed and power inputs. 
________________________________________
2006 - MA010 
STRENGTH OR ENDURANCE
Kevin Michael Meigh
Wheeling Park HIgh School, Wheeling, WV , United States

<br><br>The purpose of my project is to find the connection between the ratio of radii of an ellipse and its mechanical advantage on specified angle of rotation intervals. Besides looking for correspondence, I am also trying to make the job of workers in the distribution industry easier by finding the shape of an elliptical body that will produce the highest average ideal mechanical advantage as it is rolled. I hypothesize that an elliptical body with a 3/5 radii ratio would yield the highest average mechanical advantage. I reasoned that since the ratio was slightly over 1/2, the worker would gain leverage and not lose a significant amount at particular angle intervals. In the experiment, I used a coordinate plane to model the path of the ellipse as it rolled and integration techniques of Simpson’s and the Trapezoidal Rule to determine the distance of resistance that was traveled in response to the effort exerted. After modeling the elliptical movement, I calculated the corresponding distances of effort by distance formulas. The ratio of distance of effort to distance of resistance would serve as the mechanical advantage. After finishing the collection of data, my hypothesis was proven wrong. The ratio of 1/5 yielded the highest average mechanical advantage of 2.8911, multiplying the effort force by 43 at its maximum. Which elliptical box will help workers most depends on muscular strength and endurance. If you never want to have a disadvantage with effort force and want maximum advantage on the 0º to 30º and 330º to 360º rotation intervals, the 3/5 ratio would probably be best.<br><br> 
________________________________________
2004 - MA010 
AN ALTERNATE KNIGHTS MOVE THEOREM PROJECTED IN THREE DIMENSIONS
Christopher James Harwood
Greybull High, Greybull Wyoming, USA

The overall purpose for this project was to try to determine whether the knights move theorem could be transposed into 3 dimensions by simply cubing the formula. The knights move theorem is also known as the knights tour or knights circuit. These two names are derived from Hamiltonian paths and Hamiltonian circuits, which say that a path can be traced along any regular polygon touching every node only once. The hypothesis stated that the proposed idea would work due to the fact that magic squares, cubed, become magic cubes.<br><br> The project branched out and covered numerous other theories since the knights move theorem actually has no known formula, due to the sheer number of possibilities. The project took the direction of trying to determine whether the knights move had any relations to other theorems, such as the Fibonacci sequence. It was theorized that the knights move is actually a better form of the Fibonacci spiral due to the fact that over specialization is slow death and therefore, it was thought that the Fibonacci spiral/triangle breaks down after a certain amount of integers are presented. In contrast, the less perfect knights move moves across two axis; one square in 1 direction and 2 squares in another. <br><br> The knights tour relates to the magic cube and magic square in that with some perfect tours, the knights tour creates a magic square or magic cube. A magic square or magic cube is when you can add up the numbers in any direction and come up with the same answer. In a cubed format, this is true also for the space diagonal. <br><br>In conclusion, the knights move theorem directly relates to many other theorems and could be a better solution to some. 
________________________________________
2008 - MA010 
ON THE REDUCIBLE QUINTIC COMPLETE BASE POLYNOMIALS
Alex Hao Chen
York High School, Yorktown, VA

Let B(x) = x^m + ••• + b_1x + b_0 be a polynomial with integer coefficients. If every element in Z[x]/(B(x)Z[x]) has a polynomial representative with coefficients in S = {0, 1, 2, …, |b_0| - 1} then B(x) is called a complete base polynomial. We prove that if B(x) is a completely reducible quintic polynomial with five distinct integer roots less than -1, then B is a complete base polynomial. Meanwhile, we provide a Mathematica program for determining whether a given polynomial B(x) is a complete base polynomial or not. The program enables us to experiment with various polynomial examples, to decide if the potential result points in the desired direction and to formulate credible conjectures. 

Awards won at the 2008 ISEF
Third Award of $250 - American Mathematical Society
Third Award of $1,000 - Mathematical Sciences - Presented by
________________________________________
2005 - MA011 
TRIANGLES: BEYOND SIERPINSKI
Neysha Marie Burgos - Vázquez
Carmen Belén Veiga -Juana Díaz, Puerto Rico

<br><br>When the even numbers on the Pascal Triangle are painted, the Sierpinski Triangle is obtained. Another similar fractal was created when the multiples of the prime numbers 3, 5, and 7, were painted. <br><br>Those new fractals have patterns. In a way they behave like people. Because the cells interact and form tissue, tissues interacts and forms organs and so the organs form systems, the systems form people and when people interact societies are formed.This<br><br>way, patterns in one level interact to form the next level. <br><br>The way in which the sums were done have some shared characteristics with the Punnet square, including that the table and what it forms have symmetry. When the patterns were observed, some similarities were found that could be compared with the ones found in nature; so the next step was to try and find a fractal that appears in nature. Similar steps and techniques used in the Pascal Triangle to obtain the Sierpinski Triangle were used to prove this. Using the hexagonal numbers that appear in the Pascal Triangle a numeric beehive was done. So fractals in Nature could be studied as numbers. This investigation will continue to search other techniques to make imaginary fractals, or ones that appear in Nature, either authentic or non-authentic. In a future different fractals, static or dynamic, could be explained using this method and applying different theorems, theories and laws like Chaos, Newton's Theory or the Evolution as chains that connect the functioning of <br><br>different fractals. In this way different Nature's phenomena could be explained too.<br><br> 
________________________________________
2009 - MA011 
FORMULA "Z3"
Hector L. Maldonado-Perez
CROEM HS, Mayaguez, PUERTO RICO

Formula “Z3” which function is in a Fibonacci pattern is the base of this investigation. This formula implies that (t_x)^2+(t_x+1)^2 = t_2x+1. Implicating that by choosing two consecutive numbers from Fibonacci’s pattern {1,1,2,3,5,8,13,21,34,55,89…} and elevating them both to the second potent, then add them up, that value is the value of another number in a higher position in such pattern. How to seek the positions? Add the sub indexes in the formula, then substitute. This formula was affirmed by the trigonometry function of Math Induction. The formula is affirmed, but it is important to empathize that it is only applies for impair numbers in a higher position, dew to the fact that it is functioned by two consecutive numbers and when you add, the result will always be an impair positioned number. The pattern will be organized in a chart from term t_1 to term t_100. This quantity of positioned terms is enough to prove the efficiency of the formula. This formula is only effective for impair positioned numbers, but when reaching the end of the projection of the Investigation a formula that could function with pair positioned numbers was found. This formula will be Investigated in a posterior Investigation dew to the extension of the equation. In conclusion many people have recognized Fibonacci as a Puzzle solver, but he may have also been a puzzle maker in his work. Perhaps the Fibonacci Pattern is full of secret waiting to be discovered.<br><br>t_n means “t sub n” 
________________________________________
2009 - MA012 
GAM'S DETERMINANT
Gerardo Arroyo-Martinez
CROEM H.S., Mayaguez, PUERTO RICO

The Sarrus’ scheme is an alternative method which purpose is the finding determinants in 3x3 matrices. Said Sarrus, that if to a 3x3 matrix you add its first 2 columns its determinant can be calculated. The investigation seeks to alter that scheme so that an option is opened for the exposure of a new method called GAM’s Determinant which function is in a 4x4 matrices.<br><br>GAM’s Determinant consists on finding the common factor of the values in the row of a 4x4 matrix, and then the row and column on which this value is represented will be eliminated. That common factor is multiplied by the sum of the product of the three diagonals. Diagonals are seeked with the three remaining values in the cancellation of the row and column. The direction of the diagonals varies with the column on which the common factor is located. For columns 1&3 the diagonal is drawn to the right, meanwhile columns 2&4 are drawn to the left. This process is repeated for each of the values in the first row to seek the positive terms. Finding negative terms requires repeating the recently mentioned process but the diagonals are different: the diagonals that were to the right will now be to the left and vice versa. <br><br>By obtaining the determinant, the demonstration of altered methods for the finding of determinant in 4x4 methods is affirmed. Determinants are applied to almost all the math findings, this investigation is another method for seeking determinants in 4x4 matrices. 
________________________________________
2005 - MA012 
CALCULATING PRIME NUMBERS USING HEXAGONS
Andres O Fontanez
Ramon Power y Giralt High School, LasPiedras, Puerto Rico 

THIS INVESTIGATION IS ABOUT THE DISCOVERY OF TWO NEW MATHEMATICAL FORMULAS USED TO CALCULATE PRIME NUMBERS USING HEXAGONS. BY OBSERVING TRUCKS TRANPORTING CILINDRICAL TUBES A PATTERN OF PERFECT HEXAGONS CAN BE FOUND. UPON THE ADDITION OF THE CILINDERS, THE RESULT IS A TRIANGULAR NUMBER. BASED ON THIS FORMULA, THE INVESTIGATOR DISCOVERS THAT THE RESULTS ARE MOSTLY PRIME NUMBERS. AFTERWARDS, UPON OBSERVING THE PATTERNS AGAIN, THE INVESTIGATOR MAKES A SECOND DISCOVERY THAT SHEDS ANOTHER FORMULA THAT GENERATES EVEN MORE PRIME NUMBERS.<br><br>Formula #1<br><br>1+6( TN(TN+1)) =The result are Mostly prime number<br><br> --------<br><br> 2<br><br>Formula #2<br><br>6(n) = I (+/-) 1 = Prime Number<br><br>Variables:<br><br>n = Natural numbers<br><br>TN= Triangular numbers<br><br>I= Intermedial numbers <br><br><br> 
________________________________________
2006 - MA012 
THE USE OF MODULES FOR THE DEVELOPMENT OF PATTERNS WITH UNITARY FUNCTIONS
Diana María Rosario-Torres
Carmen Belén Veiga -Juana Díaz, Puerto Rico

The module is an auto controlled component of a system. It possesses a definite inter phase something is modular if it’s built in such a way that facilitates assembling, flexible transaction and repayment of components. The purpose is to study certain properties of some unitary functions through the analysis of the diagrams that mold them. Will there be symmetry in the diagrams that are molding the unitary functions and under what conditions? Will there be a relation between the numbers that signal other numbers with the module we are using? If exists symmetry on the diagrams that are molding the unitary functions then patterns can be established. If there exists relations between the numbers that ratlines other numbers with the module that are utilizing, then the conjectures will be established. Some patterns were found, the hypotheses were correct and some conjectures were established:<br><br>• The diagram of the function n -> n^2 where the module is prime equal to the diagram of the function n -> 2n where the module is 1 less than the module of the first function.<br><br>• If it’s a prime module then if subtract 1 of the module and divide it in 2 then the result of the division will be the quantity of deflected and none deflected numbers in the diagram.<br><br>(P-1)/2 = quantity the numbers wound and no wound in the <br><br> diagram.<br><br>• In the function n -> 2n (p-1) (mod p) the residue of cero will always be – 1. <br><br><br> 
________________________________________
2008 - MA012 
COUNTEREXAMPLE OF FERMAT'S LAST THEOREM
Edwin A. Rosado-Olivieri
Lysander Borrero Terry, Villalba, PUERTO RICO

This research pretends to evaluate the theorem and find a demonstration or a counterexample for it. The question: If the theorem exists, why didn’t have a demonstration? Is possible it have a counterexample? It was affirmated that exist a counterexample to the theorem. To prove it, the investigator evaluated for different entire positive numbers in x and y of the Fermat’s Last Theorem formula. Raised the numbers for n=3 thru n=50 in one part and the others three parts raised the numbers for n=3 thru n=30 in Microsoft Excel. In the theorem’s evaluation the investigator found that for the cases of:<br><br> <br><br> 1^18 + 6^18 , 1^20 + 4^20 , 2^22 + 5^22 and 1^23 + 4^23 <br><br>has entire positive numbers, finding various counterexamples in every parts of the methodology. Therefore, exists various counterexamples for the Fermat’s Last Theorem. The investigator find that the theorem can be true if enunciates that for the equation: <br><br> <br><br> x^n + y^n = z^n, <br><br>there no entire positives numbers if 2<n<18, change part of the enunciation of Fermat’s Last Theorem. Other researches to make with this project are to find a pattern making a method to determine others counterexamples for the theorem, waiting for in the future make a contribution in the Fermat’s Last Theorem for the mathematic history and open new fields for other researches related with it. 
________________________________________
2004 - MA012 
EXPANSION OF ALGEBRAIC STRUCTURES: THE COMPARATIVE ANALYSIS AND COMPUTER REALIZATION
Alexander Alexandrovich Siritsa
Municipal educational school #2, Lipetsk, Russia

The given work is devoted to research of expansion of the most known algebraic structures. Various modes of expansion, such as methods of pairs I and II types as also were investigated at expansion of a semiring of natural numbers to a field of rational numbers. In work the alternative expansions of a field of real numbers to a ring of double or dual numbers were considered. The outcomes of the given work can be applied in some areas of physics, such as screw calculus, and also at study of algebra, number theory and non-Euclidean geometry. 
________________________________________
2003 - MA012 
POLYNOMIAL MAPS FROM ZN TO ZN
Alexandr V. Medvedev
BSU Lyceum, Minsk, BELARUS 

See Abstract on File for correct formula notation. 4/24/03<br><br> The main aim of the given research is determination of all maps and substitutions in Zn_(residue class ring modulo n) which are represented as polynomials and also finding out the general form of all zero polynomials in Zn_.<br><br> It is well known that any map in Zp_ where p is prime number can be determined with polynomial. There are known also some particular decisions of the problem of zero polynomials. But I couldn’t find in the literature the general solution of this problem so as the problem of maps.<br><br> First of all it was proved that there is a bijective map between set of polynomial maps over Zmn_ (let us mark it as PF(Z{mn}_)) and PF(Zm_)xPF(Zn_) where m and n are mutually disjoint. The similar statement was proved for substitutions. That’s why studying of the polynomials over Zp_^k took the most part of my work.<br><br> I searched out the formula for the amount of polynomial maps and substitutions.There were also found some properties that can considerably decrease the amount of polynomial maps. A formula was deduced according to which all the map in Zp_^k and also its polynomial can be restored by its first k*p values. Using this algorithm it is rather easy now to determine whether the given map is polynomial or not.<br><br> I should mention that these results can be applied in cryptography. Many cryptographic transformations are considered as Zn_ permutations which can never be polynomial. <br><br> 

Awards won at the 2003 ISEF
Second Place Awards of $500 - American Mathematical Society
Third Award of $1,000 - Mathematics - Presented by Panasonic Consumer Electronics Company
________________________________________
2004 - MA013 
GRAPHS WHOSE MONOIDS OF ENDOMORPHISMS ARE REGULAR (IN SENSE OF VON NEUMANN)
Alexey Yu. Shubnikov
Mathematical Education Center, St. Peterburg, Russia

Let A be an assosiative monoid ring. An element a in A is called regular (in sense of von Neumann [1] ) if there is an element x in A such that axa=a. A is called regular if every element of A is regular.<br><br>Semigroups of endomorphisms carry important information about graphs. In [2] A. Ya. Aizenshtat described the ordered sets whose monoids of endomorphisms are regular. In [3] M. E. Adams and H. Gould described finite semillattices which satisfy this property. In the present paper we deal with graphs. The main results:<br><br>1. We gave the full (six types) description of the graphs whose partially monoids of endomorphisms are regular.<br><br>2. We gave the full (five types) descriptions of the reflexive and transitive graphs whose monoids of endomorphism are regular.<br><br>3. We gave the full (two types) descriptions of the reflexive and symmetric graphs without cycles whose monoids of endomorphism are regular.<br><br>Results and methods of the present paper can be used for the similar problems in various classes of graphs and semigroups.<br><br> 
________________________________________
2008 - MA013 
MULTIPLYING PERFECT SQUARES
Benjamin Gonzalez-Burgos
CROEM High School, Mayaguez, Puerto Rico, PUERTO RICO

The purpose of this investigation is to multiply perfect squares differently from the traditional way. The procedure, depending on the case, will be the following:<br><br> Perfect squares of two digits:<br><br> ab<br><br> x ab <br><br> ------<br><br>The first step is multiply units by units (b x b = b ^ 2 )<br><br> If b ^2 < or = 9, Then;<br><br>• When b ^ 2 < or = 9, no regrouping is taking place, in this case multiply tens a by unit b. The result is doubled. The result will be number d.<br><br> If b ^ 2 > 9, then;<br><br>• When b ^ 2 > 9 and regrouping is taking place, multiply a x b x 2 + c. Where c are the regrouped tens of the previous operation.<br><br>The next step is done in two different ways: when d, which is the result of the previous operation, is more than 9 and when it is less or equal to 9.<br><br>• Case 1: When d is < or = 9, multiply tens by tens.<br><br>• Case 2: When d > 9, multiply tens by tens and add k, where k is the regrouped number of the previous operation.<br><br>Also the researcher discusses how to multiply perfect squares with three digits number (abc) differently from the traditional way.In conclusion, there are other methods that can be used to obtain perfect squares through multiplication differently from the traditional way.This investigation plans to find other methods of multiplying perfect squares for numbers of four or more digits. 
________________________________________
2010 - MA013 
FINDING A MATHEMATICAL RELATIONSHIP IN HUMAN PROPORTIONS
Diana Judith Cardenas
The Good Hope School, Frederiksted, VIRGIN ISLANDS

The divine proportion, also known as the golden ratio and Phi (1.618), is encountered when describing ratios between distances (Weisstein, 2009). This ratio was and is often used in architecture to make structures more pleasing to the eye (Britton, 2009). The divine proportion has been studied repeatedly in history, notably by Leonardo Da Vinci in the 1500’s (Anderson, 1999). The purpose of this research was to disprove any differences regarding gender, age and ethnicity in finding the golden ratio in human proportions. The procedure included calculating two ratios: total body length divided by the distance from the navel to the floor, and total arm length divided by the distance from the elbow to the tip of the middle finger. The results of this study indicate that finding the golden ratio does not depend on gender and ethnicity, although subjects of the same ethnic group had uneven population sizes. A positive correlation was also found regarding age and the approximation to the golden ratio. In other words, human proportions are nearer to the golden ratio as the body develops into adulthood. Future studies may focus on how these ratios differ in an infant’s body and change throughout the life span. Further research should also investigate the golden ratio in the five established races (Tyler, 1902). 
________________________________________
2005 - MA013 
POLYHEDRONS, GRAPHS AND EULER
Diana María Rosario - Torres
Carmen Belén Veiga -Juana Díaz, Puerto Rico

This study investigates relations between faces, vertices, and edges in polyhedra. We also create graphs with geometric figures to find relations involving the number of vertices that they posses using edges to link them. Patterns that relate the graphs with the geometric figures are studied. If a relation exists between faces, vertexes and edges in polyhedra, one can generate a sequence of numbers satisfying some formula and if it relates the graphs and the geometrics figures, then one can find a pattern. Polyhedra are created, classified as regular, irregular or conjugate models, and then the models are described as how they were constituted in terms of figures. In the graphs, using the parity of the vertices the following conjectures were established: For every graph composed only by triangles, square, pentagons, hexagons and heptagons: with an even number of vertices, the path can always be made to finish in the last vertex of the graph, and with an odd number of vertices, the path can always be made to finish in the last even vertex of the graph. Therefore there exists a pattern that relates the graphs and the geometrical figures that make up the graph: C - A + V = W, where W, C, A and V are the number of polygons, faces, edges and vertices respectively. In the future, the investigation will demonstrate every conjecture established. Applications exist in science and technology where polyhedral structures appear, like the study of crystals and communication networks. 
________________________________________
2006 - MA013 
POLYGONAL NUMBERS
Wilson Felipe Alvarez
CROEM, Mayaguez, Puerto Rico, United States

This Project studies the numbers in series and sequences that can form regular polygons when they are expressed in points. The amount of points in the figure depends on the amount of points per side and the amount of sides in the polygon. The basic example is the triangular numbers. This numbers are the sum of all whole consecutive numbers from 1 to n. When any number of this sequence is expressed in points, arranged, it forms a triangle, for example:<br><br> .<br><br> . . .<br><br> . . . . . .<br><br>. . . . . . . . . . .<br><br> . . . . . . . . . . . . . . <br><br>1 3 6 10 15 = amount of points in the figure <br><br> <br><br>Each number in the series is the anterior number plus one. The equation for this series is one known, (n(n+1))/2. But, what is the equation for the sequence of the square numbers? or for the series of odd numbers?, or even for other polygonal sequences or series? This series are interesting. To understand them, first, the investigator had to observe and study how the figures where created to propose rules for their creation, so he could analyze them later with more details. After analyzing the sequences, series, and figures he found a general equation for polygons with n points per side and k sides.<br><br> 
________________________________________
2009 - MA013 
PEDALS TRIANGLES AND EULER'S LINE
Emmanuel Colon-Colon
Escuela Superior Lysander Borrero Terry, Villalba, PUERTO RICO

The famous Swedish mathematician Leonard Euler discovered that in a triangle, when you find the four points of intersection of the special segments, the barycenter, the orthocenter, and the circumcenter remain aligned in the Euler’s Line. The objective of this investigation was to demonstrate that there exist a triangle in which the incenter and the Gergonne’s point passes through the Euler’s Line and finds a pattern among the Pedals Triangles. By modeling different triangles with diverse measures of the sides and the angles in the program, it is demonstrated that there exist triangles in which the Euler’s Line passes through the incenter and the Gergonne’s point. Therefore, if a triangle is always isosceles or it is a scalene triangle with two similar sides; the incenter and the Gergonne’s point are going to be part of the Euler’s Line and the pedals triangles are going to be proportional among them. To continue this investigation the Euler’s Line in the quadrilaterals will be used. 
________________________________________
2009 - MA014 
LAGRANGE'S FOUR SQUARE THEOREM
Liann M. Marquez Fuentes
Escuela Francisco Morales, Naranjito, PUERTO RICO

This research studied and analyzed the Lagrange’s Four Squares Theorem. The objective is to identify the perceptible characteristics of this series of numbers clearly. The problem presented was: Will it be possible to express each positive whole number as the sum of four whole squares? To answer this question, the following hypothesis was presented: If N is a natural number 0<N<201, then it is possible to express it as a sum of four whole squares. To validate the hypothesis presented a resource (applet) was used from the cybernetic red to find the decomposition of any natural number bigger than 1 in a sum of squares. After analyzing the results, it could be interpreted that every whole number that is found between 1 and 200 can be expressed as the sum of the four squares at most (Graphic 1.1) also some of them are expressed in multiple ways. For example: 36=4^2+4^2+2^2, 36=3^2+3^2+3^2+3^2, 36=5^2+3^2+1^2+1^2 and 36=6^2. In addition, 8% of the numbers between 1 and 200 have a unique decomposition (EDQU) as the sum of squares (23 is the biggest) and the same one is the sum of odd powers of 2 or these same powers multiplied by a prime number. The findings validate the presented hypothesis. For a future investigation it is proposed to determine a generating algorithm of the values of N for the ones that v(N)=1 and verify if the decomposition is maintained in two or three squares for the EDQU. 
________________________________________
2006 - MA014 
FORMULA TO FOUND MAGIC SUM ON IMPAIR ORDER MAGIC SQUARES USING TRIANGULAR NUMBERS
Andres O Fontanez
Ramon Power Y Giralt, Las Piedras, Puerto Rico

<br><br> When a Magic Square is solved, the sum of all rows, columns or diagonals will be the same. In the investigation that sum is called Magic Sum, and it will be produced using a new and improved formula that can find the magic sum of an impair order magic square, using triangular numbers. Each order of the magic square requires a different triangular number, and when that is known, there’s only to continue the formula steps, on the end madding it to calculate the magic sum of an impair order magic square without filling the square, the only thing to know is, the order of the square, the smallest number used on the square and the sequence between the numbers (Ex: 1,3,5,7…). With that information it is possible to solve the formula K(X+4Tn)P-X(K(P-1))=SM on the one K represents the order of the square, X represents the smallest number, P represents the sequence and Tn will be the triangular number that works on concordance with the square. Tn is found by this steps: (K-1)/2. That result will be N. N will be used on the official triangular number formula, on the one [N(N+1)]/2. Now it is possible to change the rest of the formula such as K, P and X. When that step is done the result of the formula works on a 100% of the times, no matter the size of it, or what sequence is used. <br><br> 
________________________________________
2008 - MA014 
RATIO BETWEEN THE AREA OF A CIRCUMSCRIBED CIRCLE AND AN INSCRIBED CIRCLE IN REGULAR POLYGONS
Luis Yediel Negron
Francisco Morales, Naranjito, PUERTO RICO

The purpose of this research was to find and analyze the ratio between the area of the circumscribed circles and inscribed circles in regular polygons from three to ten sides. The research question was: How the ratio between the area of a circumscribed circle and an inscribed circle in a regular polygon is affected when the number of the polygon’s sides increase from three to ten? The hypothesis proposed was: If the number of sides of a regular polygon increases from three to ten, the ratio between the areas of the circumscribed circle and the inscribed circle in the polygon will decrease and approximate to one. As a procedure a value was assigned to the longitude of the regular polygons’ side. Then, the Hawkins and special triangles theorems and trigonometry were applied to find the ratio before explained. The results of the ratios for the regular polygons with three, four and six sides were the followings: triangle: 4:1; square: 2:1; hexagon: 4:3. For the regular polygons with five, seven, eight, nine and ten sides, the ratios were approximated to the closest thousand. The results were the followings: pentagon: 1.528; heptagon: 1.231; octagon: 1.170; nonagon: 1.132; decagon: 1.106. Also, after calculating the data of the graphs, it was found that the quadratic model (y=ax^2+bx+c) represents better the data. To conclude the data collected supported the hypothesis. In a future research will be interesting to investigate the concentric circumference succession defined in a recurrent form where the radiuses are smaller each time. 
________________________________________
2005 - MA014 
THE NTH TERM OF SUCCESSIONS AND SERIES WITH FINITE RATIOS
Nelson Morales- Lugo
Áurea E. Quiles Claudio High School Guánica, P.R. 00653

This investigation’s main purpose was to develop a new theory based on finite ratios that present a given type of succession and series in order to facilitate the acquirement of the nth term. The problem, “Given any succession or series with common ratios between it’s terms in the first, second or nth division: would it be possible to develop a new theory that helps determine the succession or series’ nth term based on this finite ratios?” If we study the patterns presented in geometric successions and the exponential functions, as well as the finite ratios technique, we will be able to develop it. The developed formulas were the following: <br><br> Successions<br><br>Common ratio on: Polynomial degree of General Form<br><br> the exponent of a: <br><br>First division 1 Ka^n <br><br>Second division 2 Ka^[(n^2+n)/2] <br><br> <br><br>Third division 3 Ka^[(n^3+3n^2+2n)/6] <br><br><br>Fourth division 4 Ka^[(n^4+6n^3-n^2+18n)/24]<br><br><br> Series<br><br>First division 1 K(1-a^n)/1-a<br><br><br>K € R a = the common ratio of the sequence (0<a<1,a>1) n=0, n>0 <br><br> <br><br> One important application of these formulas is to determine the perimeter of fractals. Finally, it was concluded that this new theory will be of great use in the field of mathematics. It is projected to develop the corresponding formulas of series. 
________________________________________
2010 - MA014 
ANALYSIS OF SINGLE-ELIMINATION TOURNAMENTS
Chi-Hua Wang
National Pingtung Senior High School, Pingtung, CHINESE TAIPEI

Single-elimination tournaments studied in this project are the knockout tournaments which satisfy the property that each game has a winner and a loser; a loser of a game is not involved in any further game. Let a, b denote the strength of contestants A, B. Then the winning probability of contestant A when playing against contestant B is expressed by the quantity a / (a + b). Along the same line, in order to formulate the winning probability of contestant A when reaching the nth round based on scheme T, we introduce the following concepts: "Imaginary Opponent", "Threshold of Threat", "Formula of Winning Probability", "Probability-to-Strength Ratio (PSR) ", "Rate of Scheme T ". These concepts enable us to study the influence of the variations in contestant's strength on winning probability (a) based on the traditional scheme and (b) based on the winning probability when the strength of the contestant is kept fixed. Results of this project clarify some myths in the single-elimination tournaments. We find it surprising that the conventional wisdom "Once the strength of our opponent increases, our winning probability will decrease" actually is false! This project may be of importance in explaining the fairness of single-elimination tournaments, as well as in providing a better understanding of single-elimination tournaments. 

Awards won at the 2010 ISEF
Third Award of $1,000 - Mathematical Sciences - Presented by Intel
________________________________________
2008 - MA015 
NOVEL FORMULA TO FIND THE BORDERS OF A TRIANGULAR NUMBERS
Adolfo Rodriguez-Velazquez
Santiago Torres, Las Piedras, PUERTO RICO

A formula to determine any triangular numbers was developed by Carl Friedrich Gauss in 1796. Since then, this formula have been used for many mathematicians to determine the quantity of triangular numbers for any given number, but a formula to find the borders of any triangular number given have not been developed up to this point. This formula is named “Adolfo’s Triangular Number”. The Adolfo’s formula is composed of a series of equations that calculate the exact quantity of point that each triangular border has. By finding the exact number of points needed in each border of the triangular numbers, it will simplify the construction of those borders. The application of this formula is unlimited, since it can be apply to disciplines such as Civil Engineering, Mathematics, Architecture, among others. For that reason, this research successfully established a formula where the borders of any triangular number can be obtained. First, a comparison is made between the graphical and algebraic properties of the triangular numbers, and the sum of three consecutive natural numbers. That consecutive sum is called: “the Adolfo Triangular Numbers”. Then a pattern is observed between the triangular numbers and the “Adolfo” numbers that gives origin to the three formulas that effectively determine the borders of any triangular number. These formulas are submitted to 50,000 applications using Microsoft Excel. After analyzing the results obtained of those applications, it is evidenced that the new formulas to identify the borders of any triangular number, using the “Adolfo” numbers, are completely effective. 
________________________________________
2003 - MA015 
THE N-SECTION OF A SEGMENT AND AREA RATIOS
Alexandra Jannet Gandara
Lysander Borrero Terry High School, Villalba, Puerto Rico 00766

The purpose of the research is to determine if there is any pattern when finding the ratio between the area of a triangle and the area of a polygon constructed inside the triangle. In order to construct the polygon inside the triangle, the intersection points of the segments from each vertex of the triangle up to a point of N-section in the opposite side are used as vertexes. The division of the sides of the triangle in N equal parts is obtained with a method using circles over a perpendicular to each side of the triangle. Afterwards, parallel lines are made and the intersection points with each side of the triangle are identified. The use of the CABRI II program is essential in the research.<br><br> The analysis of the areas obtained leads the researcher to make the following conjecture: the ratio of the area of the triangle to the area of the polygon keeps constant and maintains a pattern.<br><br> According to the pattern found, a formula was created to predict the ratio of the area of the triangle to the area of the polygon, given the number of points in which the sides of the triangle are divided. The formula is Tn=1 + 9n(n+1)/2, where Tn is the ratio of the areas, n=N/2 and N is the number of points used in the division of the segments, N even. As a future projection, the researcher expects to develop an algebraic demonstration for the formula.<br><br> <br><br><br><br><br><br> <br><br> <br><br> <br><br> 
________________________________________
2010 - MA015 
MAXIMIZING THE LION'S SHARE
Hsin-Po Wang
Taipei Municipal Jianguo High School, Taipei City, Taiwan (R.O.C.), Taipei City, Taiwan (R.O.C.), CHINESE TAIPEI

Captain Jack and his crew obtained as loot a number of chests containing fresh fruit. Each chest might contain a number of apples and a number of bananas. Captain Jack would like to get at least half of the apples and at least half of the bananas.<br><br> Before examining the chests, Captain Jack must announce the number of chests which he was taking for himself. Then he could examine the chests and made his choices. What was the minimum number of chests which Captain Jack must take in order to guarantee that he would get at least half of the apples and at least half of the bananas?<br><br> This problem may be approached using techniques from advanced mathematics. The aim of this paper is to tackle the problem entirely by elementary means. Upper bounds are obtained by explicit constructions while lower bounds are obtained by general arguments. The matching bounds will deliver the exact result. and as a bonus the method of selecting the chests.<br><br> This problem may be considered as a mathematical model on how to distribute relief material in disaster areas, how to assign indivisible resources such as expertise, how to design multi-purpose machinery, or how to assemble ingredients to prepare food which meets certain health standards. We hope that contribute to improvement of efficiency in various applications. 
________________________________________
2007 - MA015 
RECONNAISSANCE MECHANISM FOR THE POLYGONAL NUMBERS
Joel Antonio Morales-Rosado
Florencia Garcia, Las Piedras, Puerto Rico

The polygonal numbers have been a historic fascination since the Greeks and, recently, by their particular numeric properties and their presence on cryptography. This research pretends to evaluate if there exists any mechanism to recognize all the polygonal numbers. Optimistically, it was affirmed it exist. To prove this affirmation, all base equations of polygonal formulas (28) are reduced, to be applied, to their corresponding coefficients in a quadratic way. Later, about 28 discriminants are determined. Eventually, these are tested 65,000 times each, followed by an analysis of numerical behavior. This analysis demonstrates the infectivity of these discriminants in identifying each of the numbers in an effective way. In response to this failure, Reconnaissance Formulas are develop. These pretend to amend the error of the discriminants in identifying the polygonal numbers and to localize the position of the number in the correspondent succession. Then, they are tested equally as the discriminants, in 65,000 evaluations of each one, and a numerical behavior study, which totally prove its effectiveness. Furthermore, in response to the precision shown on these new formulas, the application P.I.P.I. (Prototype of Instant Polygonal Identification) is established in Microsoft Excel. Instantly, any given number can be known, if in fact, it is a polygonal number. If it is, then their exact location on the succession and the type of polygon is also known. Therefore, a mechanism to recognize and localize any real number with P.I.P.I., based on the 28 formulas found, can be established effectively and precisely. 

Awards won at the 2007 ISEF
Third Award of $1,000 - Mathematical Sciences - Presented by Acatel-Lucent
First Award of $3,000 - United States Air Force
________________________________________
2008 - MA016 
RESEARCH INTO THE ORDER 3 MAGIC HEXAGON: ITS PROPERTIES, CONSTRUCTION AND POSSIBLE APPLICATIONS
Fanxing Meng
Tianjin No.1 High School, Tianjin, CHINA

An order 3 magic hexagon resembles the shape of a 19-cell honeycomb, arranged in a 3 4 5 4 3 manner. The requirement is to fill the numbers 1-19 in the grids so that each row (15 in total) adds up to 38.<br><br> Previously invented methods aimed at solving this problem and proving its uniqueness were either not rigorous enough or too intricate. So by analyzing its properties, I wanted to find a combinatorial solution to its construction, prove its uniqueness, and investigate whether its mathematical principles can be used in real-world applications.<br><br> The difficulty depends on the viewpoint, so the first step was to label each grid in a convenient way. I chose to look at the magic hexagon as a network composed of a center and rings. Then the connections and restrictions of each number set could be found by formula derivation. In a similar fashion, symmetrical properties were also found. The next step was to analyze possible distributions of odd and even numbers. Out of the 9 configurations, only 1 proved to be usable. The final step was construction. With all the properties known, the few impossibilities were easily eliminated, and only one solution remained, thus proving its uniqueness.<br><br> The procedures used on the order 3 magic hexagon may be extended to those of higher orders, providing more ease in their construction. The unique properties of magic hexagons may be used in some fields of application, such as in password systems, missile defense systems, composite material, large-scale roof structure and many other fields. 
________________________________________
2007 - MA016 
BRAND NEW FORMULA FOR PASCAL TRIANGLE
Laury Delgado-Marquez
Florencia Garcia High School, Las Piedras, Puerto Rico

During centuries of study, the enigma surrounding the sequences patterns of the Pascal Triangle, have been studied widely. It is of common knowledge the existence of only one formula that find any term in the Triangle, the theorem ((n!)/(n!(n-p))). This requires the formatting of the triangle in the form of pyramid an inscribed in a table that names every coefficient of the Newton’s Binomial. <br><br> This investigation has the purpose to create an equation that satisfied the search for “n” term in the Pascal Triangle, expressed in a rectangular way, and presented by the hypothesis that it really can be created.<br><br> On first instance the Pascal Triangle was develop in the form of a rectangular triangle, rows and columns, and after an arduous analysis of the patterns found, within the sequences of the consecutive terms, it can be presented the next formula:<br><br> ((n + (c – 1))!) / (( n + (c – 1) – c) ! c !)<br><br> In this formula the value of the variable “n” represent the term requested vertically and “c” the column, where the term is found, on the rectangular form of the Pascal Triangle. <br><br> After evaluating the created formula for any given “n” and “c” value, and realizing an analysis of almost 1,000 evaluations, with equations of factorial basis; it can be concluded that it is completely effective on its function: Find what real number, is on the column desired. This validate the hypothesis of the pattern in the Pascal Triangle.<br><br> 
________________________________________
2003 - MA016 
POSITIVE INTEGERS N, N>1 AND THE ANTI-GIUGA CONJECTURE
David Colon-Cabrera
University Gardens High School, San Juan, Puerto Rico

Fifty-three years ago, the mathematician G. Giuga established his conjecture: If n>1 then n|1(n-1) + 2(n-1) +¡¬+ (n-1) (n-1) + 1 if and only if n is prime. Given the above conjecture, it was asked, what positive integers n, satisfy the new anti-Giuga conjecture n|1 (n+1) +2 (n+1) +¡¬+ (n+1) (n+1) - 1?<br><br>It was thought that if prime numbers, satisfied Giuga's conjecture, therefore composite numbers n>1, would satisfy the new Anti-Giuga expression. The expression 1 (n+1) +2 (n+1) +¡¬+ (n+1) (n+1) - 1, was evaluated with composite numbers n, 3<n<101. This expression was analyzed when n is odd or even and also when n is a perfect square or a cube. It was proved that:<br><br>i. For 3<n<101, the only composite numbers that satisfy the Anti-Giuga expression were the multiples of 4. <br><br>ii. The odd numbers that satisfy the Anti-Giuga expression had no relation to each other, except the 25 and 49 that are perfect squares. <br><br>iii. For 3<n<101, all perfect squares, except 9 satisfied the Anti-Giuga expression. <br><br>iv. For 3<n<101, all perfect cubes, except 27 satisfied the Anti-Giuga expression. <br><br>It could be interesting to observe the effect of using values of n, for n>100, to verify if the patterns already found, are satisfied.<br><br> 
________________________________________
2005 - MA016 
COMBATING GERRYMANDERING: A NOVEL MATHEMATICAL APPROACH
Daniel David Graves
Nicolet High School, Glendale, Wisconsin, USA

Gerrymandering is becoming more prevalent throughout the nation. Yet no standard exists to identify gerrymandering. This project develops a mathematical formula that can be used as a standard to identify this from of unfair redistricting.<br><br> An effective formula to identify gerrymandering must allow for the preservation of “communities of interest,” encourage compact districts over non-compact districts, allow for the preservation of counties, and not be excessively sensitive.<br><br> The formula derived in this project defines “population” as the people within a circumscribed geometric figure around the district. The people within the figure and inside the district represent one sample and the rest of the population represent a second sample. S is the difference between the two samples for any given criterion, such as race. Next, the circumscribed geometric figure is dissected by an algorithm that evaluates what the average difference between the two samples would have been if the district were drawn using North-South or East-West line partitions. The formula's result is found by dividing S by the result of the algorithm. A result of "x" means that the district separates two groups to the extent of "x" times the local standard.<br><br> This formula offers a new, effective way to both encourage compactness and locate unfair redistricting. The derived formula can be applied to legislative as well as congressional districts. If the input were modified to use previous studies on demographic modeling, this formula could become the basis for an effective law to destroy the practice of gerrymandering. 

Awards won at the 2005 ISEF
$2,500 Second Place Scholarship Award in Mathematics - Robert Luby, Jr. of IBM
________________________________________
2006 - MA016 
TILINGS ON A SQUARE STRIPE
Maxim A. Ulanov
Advanced Education and Science Centre of MSU, Moscow, Russia

Target setting.<br><br>A finite set of squares on the infinite square grid is called a tile if the plane can be covered by its translates without gaps and overlappings. The aim of the work is to find a criterion for such a set to be a tile.<br><br>The key idea.<br><br>Let us start with the one-dimensional case. Let F be the set of squares on the infinite square band. The criterion for this set to tile the band can be formulated in terms of a special polynomial. Let us number the squares of the band starting with the leftmost square of F by nonnegative integers. Let a0=0,a1,…,an be the numbers of squares in F. Consider the polynomial P(x)=x^(a0)+…+x^(an). It turns out that the properties of F can be expressed in terms of properties of P(x). In particular we can consider whether the set is a tile by analyzing its polynomial.<br><br>The main contents of the work.<br><br>The problem s solved on the squared band first. I obtain the tiling criterion for sets of a prime number of squares, and then I formulate the general tiling criterion on the band. Then I prove the following statement: any tiling on the squared grid can be constructed using tilings on bands. And then using this statement and the result on the band I formulate the general tiling criterion on the plane. 

Awards won at the 2006 ISEF
Fourth Award of $500 - Mathematics - Presented by Lucent Technologies
________________________________________
2005 - MA016 
COMBATING GERRYMANDERING: A NOVEL MATHEMATICAL APPROACH
Daniel David Graves
Nicolet High School, Glendale, Wisconsin, USA

Gerrymandering is becoming more prevalent throughout the nation. Yet no standard exists to identify gerrymandering. This project develops a mathematical formula that can be used as a standard to identify this from of unfair redistricting.<br><br> An effective formula to identify gerrymandering must allow for the preservation of “communities of interest,” encourage compact districts over non-compact districts, allow for the preservation of counties, and not be excessively sensitive.<br><br> The formula derived in this project defines “population” as the people within a circumscribed geometric figure around the district. The people within the figure and inside the district represent one sample and the rest of the population represent a second sample. S is the difference between the two samples for any given criterion, such as race. Next, the circumscribed geometric figure is dissected by an algorithm that evaluates what the average difference between the two samples would have been if the district were drawn using North-South or East-West line partitions. The formula's result is found by dividing S by the result of the algorithm. A result of "x" means that the district separates two groups to the extent of "x" times the local standard.<br><br> This formula offers a new, effective way to both encourage compactness and locate unfair redistricting. The derived formula can be applied to legislative as well as congressional districts. If the input were modified to use previous studies on demographic modeling, this formula could become the basis for an effective law to destroy the practice of gerrymandering. 

Awards won at the 2005 ISEF
Award of $3,000 in savings bonds, a certificate of achievement and a gold medalion - U.S. Army
________________________________________
2007 - MA017 
VERIFICATION OF NEW FORMULAS TO GENERATE MERSENNE PRIMES IN A GIVEN INTERVAL
Jennifer Pérez
Aurea E. Quiles Claudio, Guanica, Puerto Rico

Mersenne primes in a given interval. After the search of information about all Mersenne prime numbers known still today, the following problem was established: will be possible determinate the validity of the new formulas? It was infered that if the computer program Mathematica V. 5.0 was used to determinate the primality of the obtained numbers, it will be possible. Mersenne prime number endings, number of digits and the patterns observed through the prospective sum and substration triangles were studied. GIMPS Project with Curtis Cooper and Steven Boone found the biggest Mersenne prime number:(2^632,582,657) -1 with 9,808,358 digits.<br><br>The new developed formulas are:<br><br>1) Mm =( 2^12m-7) -1 2) Mm = (2^12m+1) -1 m≥1<br><br>for all possible prime numbers ended in 1, ≥ 31<br><br>3) Mm = (2^12m-1) -1 4) Mm = (2^12m-5) -1 m≥1<br><br>for all possible prime numbers ended in 7, ≥ 127<br><br>The computer program Mathematica V.5.0 was used to generate Mersenne primes and to determinate the primality of the obtained numbers through the new formulas in the interval [1, 2^3357]. It is concluded that the new formulas are an effective instrument to generate Mersenne primes, because they don’t depend on a list of prime numbers and also help to decrease the test values in a given interval. It is projected to continue the search of new Mersenne primes in others intervals. 
________________________________________
2004 - MA017 
THE DOZENAL SYSTEM: A BETTER NUMBER SYSTEM?
Marchany-Rivera, Angie 
Eugenio María de Hostos High School, Mayagüez, Puerto Rico

The Dozenal Societies of America and Great Britain promote that a dozenal system is better than the decimal system. It is believed that the observation of twelve appearances of the Moon in a year is the reason why this number is used universally regardless of culture. <br><br> The purpose of this investigation was to test if base twelve is a more practical base to use. The hypotheses were if there is less repeating dozenal representations of fractions than in base ten. Also, if dozenal representations of perfect squares and primes follow a pattern. Finally, if patterns in the Pascal’s triangle would be easily recognizable with base twelve.<br><br> To start this investigation it was read and analyzed different papers presented by the Dozenal Societies. Then various properties, suggested by those papers, were tested. Some calculations were made by hand and others in a spreadsheet. To produce and handle larger amounts of data, small conversion programs were developed and word tools were employed to convert number to different bases.<br><br> The hypotheses were in part confirmed. It was found that there is less repeating dozenal representations of fractions from 1/2 to 1/10 (10=12). Besides, square numbers last digit follows a simpler pattern than in base ten. Patterns in Pascal’s triangle were not any easier to visualize in this system, neither patterns in primes. This leads to the conclusion that these patterns reflect intrinsic properties of numbers and operations that manifest themselves regardless of the base number representation in use.<br><br> 
________________________________________
2006 - MA017 
ON EMBEDDABILITY OF CUBIC GRAPHS WITH ROTATIONS IN THE TORUS
Gleb A. Pogudin
Gymnasium 6, Novosibirsk, RUSSIA

The problem of embeddability of graphs (or graphs with an additional structure) into the plane or the torus (and other surfaces) is one of the most important in the topological graph theory. Such problems are important in dynamical systems theory. A nice form of a solution of this problem is to find a set of graphs such that a graph G would be realizable if and only if G does not contain any of these graphs. <br><br>A half-edge of the graph is a pair of an edge and its endpoint (or, informally, an end of an edge). A graph with orientable rotations (R-graph) is a graph for each of whose vertices a cyclic ordering of the edges issuing out of this vertex is fixed. An R-graph is R-embeddable into the torus if it can be drawn on the torus without self-intersections so that cyclic ordering of the edges issuing out of each vertex coincides with the prescribed. Two R-graphs are called R-homeomorphic, if one can be obtained from the other by subdivisions of edges and inverse operations. A graph is called cubic if from each its vertex there issues three edges.<br><br>Main theorem. A cubic R-graph is R-embeddable into the torus if and only if it does not contain any R-graphs R-homeomorphic to one of the R-graphs G1 … G9. <br><br><br>The problem for further research is to find criterion of embeddability for all R-graphs and I have very good progress in this way.<br><br> 

Awards won at the 2006 ISEF
Third Award of $250 - American Mathematical Society
________________________________________
2009 - MA017 
DIRICHLET PRIME MAGIC SQUARE
Sarah Lee Sellers
Hedgesville High School, Hedgesville, WV

The purpose of this experiment was to find out if a bordered, odd-order magic square could be made using only Dirichlet prime numbers. A bordered, odd-order magic square has magic squares inside of magic squares and has an odd number number of rows, columns, and main diagonals. A Dirichlet prime is in the form 3n+1 (let n be any integer), and if this formula creates a prime number then it is a Dirichlet prime. A Dirichlet prime was chosen as the variable n and all deltas (or the Dirichlet primes that were in arithmetic progression, with n being the second of the three numbers) were saved to potentially be used. <br><br> This experiment confirmed the hypothesis that it is possible to create a bordered, odd-order Dirichlet prime magic square with cryptology inside cryptology. This experiment has also proven that all bordered, odd-order prime magic squares must be either Dirichlet or Eisenstein with the exception of the prime number 3 and its corresponding prime.<br><br> A 223x223 bordered, odd-order Dirichlet prime magic square was made with 111 Dirichlet prime magic squares with one. The 223x223 bordered, odd-order Dirichlet prime magic square also has cryptology inside cryptology. The size of the bordered, odd-order Dirichlet prime magic square was chosen because the prime numbers 19, 23, 29, 31, 37, 41, and 43 are all in arithmetic progression, and their sum is 223 which is also a prime number. 

Awards won at the 2009 ISEF
Third Award of $250 - American Mathematical Society
________________________________________
2010 - MA017 
SIMULATION STUDY OF STABLE METRO BRAKING WITH REAL TIME CLOSED-LOOP SYSTEM BASED ON OPTIMIZATION MODEL
Jia Hao Xu
High School Attached to Fudan University, Shanghai, Shanghai, CHINA

The optimization of the metro train braking process affects the efficiency of braking and comfort of passengers. According to the investigation of train braking process in Shanghai Metro System, unstable braking often occurs, and the major factor affecting the stability is the changing rate of acceleration (jerk). Based on the related research findings and documents, this project is aimed to study the best balance point between efficiency of braking and the comfort of passengers by using the mathematical modeling.<br><br>1. Convert the sway angle of the self-made ball into acceleration and jerk for quantifying instability during metro breaking. After analyzing about 100 groups of data which were measured on ten different metro lines in Shanghai, it was found that the train was frequently unstable during braking. <br><br>2. Establish a tri-section metro braking optimization mathematical model, which can improve the train braking stability with the guaranty of the shortest braking time and distance. Especially in the final stage of the braking, the model can make train stable by maintaining the constant and acceptable jerk. <br><br>3. Develop a real time closed-loop system which can gather the train’s running data in every 0.1 second and eliminate the deviation of braking.<br><br>4. Make simulation experiments in LabView environment. Deviation generated by the computer randomly were tested in the experiments to prove that the real time closed-loop system is reasonable and feasible.<br><br>The study results indicate that the tri-section metro braking optimization mathematical model, closed-loop control system and computer simulation program can practically enhance the Metro train stability during braking. The real time closed-loop system can also be applied to the braking control of other vehicle. 

Awards won at the 2010 ISEF
First Award of $1,000 - Mu Alpha Theta, National High School and Two-Year College Mathematics Honor Society
________________________________________
2009 - MA017 
DIRICHLET PRIME MAGIC SQUARE
Sarah Lee Sellers
Hedgesville High School, Hedgesville, WV

The purpose of this experiment was to find out if a bordered, odd-order magic square could be made using only Dirichlet prime numbers. A bordered, odd-order magic square has magic squares inside of magic squares and has an odd number number of rows, columns, and main diagonals. A Dirichlet prime is in the form 3n+1 (let n be any integer), and if this formula creates a prime number then it is a Dirichlet prime. A Dirichlet prime was chosen as the variable n and all deltas (or the Dirichlet primes that were in arithmetic progression, with n being the second of the three numbers) were saved to potentially be used. <br><br> This experiment confirmed the hypothesis that it is possible to create a bordered, odd-order Dirichlet prime magic square with cryptology inside cryptology. This experiment has also proven that all bordered, odd-order prime magic squares must be either Dirichlet or Eisenstein with the exception of the prime number 3 and its corresponding prime.<br><br> A 223x223 bordered, odd-order Dirichlet prime magic square was made with 111 Dirichlet prime magic squares with one. The 223x223 bordered, odd-order Dirichlet prime magic square also has cryptology inside cryptology. The size of the bordered, odd-order Dirichlet prime magic square was chosen because the prime numbers 19, 23, 29, 31, 37, 41, and 43 are all in arithmetic progression, and their sum is 223 which is also a prime number. 

Awards won at the 2009 ISEF
First Award of $3,000 - Air Force Research Laboratory on behalf of the United States Air Force
________________________________________
2010 - MA018 
FORMULA TO FIND THE BORDERS OF SQUARE NUMBER WHICH ARE NAMED YANI’S NUMBERS
Yaniria Ayala-Lopez
Ramon Power y Giralt, Las Piedras, PUERTO RICO

Polygonal numbers have been studied since the time of the ancient Greek civilization. Currently representing an area of interest for its innovative devolopment in mathematics and other applications in society. The numbers can be represented as conglomerated clumping of points in a square. At present there is a formula that can calculate these numbers, but there hasn’t been created a formula to find the border of a square number. In conducting the investigation it was obseved that there was a relationship between square numbers and the amount of points that has the border. The following problem has been formulated: How to find the border of a square number?. The hypothesis is, the patrons of cuadrangulated numbers and apliying the formula of the same, it is wait to find a formula to find the border of any cuadrangulated number. The formula YANI;Y(n-1)= S(n) – S(n-2), where “ Y’’ signifies the YANI numbers this is the border of any cuadrangulated number. This formula is aplied in ingeeniering when it uses the equidistant points like a reference to ubicate the perfect amount of necessary support in the construction of columns or any tipe of edification of square form. The proyections of this investigation are to realize detailed analisis using the borders finded with the formula in a way that is possible to fabricate models of buildings in a scale, following the alinations of the obteined points with the formula. Finally, in this investigation the conclusion is that utilizing the formula YANI, it can obteined the border of any cuadrangulated number. 
________________________________________
2007 - MA018 
PAPIROFLEXIA AS A LEARNING TOOL OF MATHEMATICAL CONCEPTS
Michael Rosado
Francisco Morales High School, Naranjito, Puerto Rico

Papiroflexia (origami) is considered an artistic movement nowadays. It is an activity that requires patience and coordination. Papiroflexia consists of folding a piece of paper; mostly square, to obtain the shape of determined objects. Nevertheless, as a consequence of its evolution, different objects have been created using rectangular paper and other initial polygons. Considering the importance of papiroflexia in geometric applications, a study was designed to test the following hypothesis: if the use of papiroflexia is effective as a learning tool, then the students will demonstrate that they can better visualize and comprehend several mathematical concepts.<br><br> The methodology used in this investigation is focused on the modular papiroflexia. The study was divided into several phases: search of modules, papiroflexia workshop assembly and questionnaire administration. Twenty three (23) high school students were selected for the investigation. They were oriented on the papiroflexia technique and then they were asked to prepared different modules.<br><br> A ten (10) questions questionnaire was administered to the students. Each of them answered it before and after the papiroflexia workshop. The results revealed that most of the students didn’t know what papiroflexia was. In addition, they identified the connection of papiroflexia with math, science and arts as significant. They also indicated that papiroflexia helped them to visualize and comprehend mathematical concepts such as polygons and polyhedrons. In conclusion, the use of papiroflexia as a learning tool of mathematical concepts demonstrated that the students were capable of better visualizing, and comprehending concepts related to polygons and polyhedrons.<br><br> 
________________________________________
2005 - MA018 
INVESTIGATING THE CHANGES IN THE POINCARÉ ALGEBRA AND GROUP BY ENLARGING THE SPACE-TIME DIMENSIONS
Niket Ranjan Pandey
Bethel High School/New Horizons Governor's School, Hampton, Virginia, USA

In our physical universe, there are experimental observations on energy, momentum, angular momentum, etc., which obey conservation laws. These conservations laws are assumed to follow the symmetry of space-time. Symmetry is the property that a system has when transformations on the system do not change the mathematical appearance of the system. These transformations form a mathematical group and for these groups, there are corresponding group algebras, which are obeyed by the generators of the group. For physical space-time, there are three space dimensions and one time dimension. Recent theories such as String theory and M-theory suggest that there might be more than four dimensions of space-time. The symmetry group of space-time is the Poincaré Group. Therefore, the changes in the structure of Poincaré Algebra by changing the space-time dimensions were investigated and examined. It was hypothesized that increasing the space dimensions would not change the mathematical structure of the Algebra, but increasing the time dimensions would alter the Poincaré Algebra.<br><br> It was found that when there were 4 space dimensions that the Algebra did not change, but it did change when there were 2 time dimensions. A simple analog for a layman would be the investigation of the extension of Plane Trignometry to 3-dimensional. A variety of generators (rotational, boost, and translation) were incorporated to find whether or not the Algebra changed. Conclusively, the initial hypothesis was justified and correct. This project has an overarching relevance because all conservation laws in physics are based on the Algebra and symmetry of the group. 

Awards won at the 2005 ISEF
Honorable Mention Award - American Mathematical Society
Tuition Scholarship Award of $5,000 per year for 4 years for a total value of $20,000 - Indiana University
________________________________________
2004 - MA018 
PATTERNS FOR IRRATIONAL ROOTS 1<N< CONTINUED AS EXPRESSED>
Luciano Luis A.
Especializada University Gardens, San Juan, Puerto Rico

Does a pattern exist in the irrational roots of integers n, 1<n<100, expressed as continued fractions? A list was created with the irrational roots using a computer program. Through an intense analysis patterns were found which establish that the expansion of the continued fraction of the terms: (a^2+1)^1/2, (a^2+2)^1/2, (a^2+2a)^1/2, (a^2+a)^1/2, [a^2+(2a)/3]^1/2, [a^2+(2a)/k]^1/2 always will have a determined amount of terms of the form:[a;2a], [a;a,2a], [a;1,2a], [a;2,2a], {a;3,2a], [a;k,2a], respectively. The method that was used to prove these conjectures was through the use of the continued fractions properties. It was observed that using the right mathematical operations we could get a quadratic equation with three terms that was solved using the quadratic formula. This gave the basis to prove the conjectures. Then several theorems were established for the patterns that were just found. A larger list of irrational roots would help in the search of new patterns. This investigation is a contribution, in particular to Number Theory and cryptography. 

Awards won at the 2004 ISEF
90% paid tuition scholarship award - Instituto Tecnologico y de Estudios Superiores de Manterrey, Campus Guadalajara
________________________________________
2009 - MA018 
FORCING A DRAW IN K-IN-A-ROW GAMES
Sheng-Hao Chiang
National Experimental High School at Hsinchu Science Park, Hsinchu City, CHINESE TAIPEI

(k, p)-games studied in this project are k-in-a-row games with the additional rule that the players put p stones at a time. Here is the precise rule. For a fixed positive integer p, two players Black (always plays first) and White alternately place p stones of their color at empty intersections of an infinite Go-like board. The winner is the first player reaching k consecutive stones horizontally, vertically or diagonally. A player is said to have a winning strategy if there is a rule leading to his winning no matter what the opponent plays. The game is said to draw if neither player has a winning strategy.<br><br>Using a computer search algorithm, Allis proved that the most commonly played (5, 1)-game on a 15-by-15 board Black has a winning strategy. Borrowing arguments first proposed by John Nash, it can be proved that White has no winning strategy in all (k, p)-games. The surprising finding of this project shows that White can force a draw in infinitely many (k, p)-games!<br><br>Our main results are:<br><br>1. For the (11, 2)-game, White can force a draw. Consequently, White can force a draw over all (k, 2)-games, k > 2. This result is sharper than the optimal known record of having a draw in a (15, 2)-game. <br><br>2. For a fixed p = 3, ..., 999, if <br><br>k > 7+3p+3 ceiling(log_2((k+7)/24)), <br><br>then White can force a draw in the (k, p)-game. The lower bound expressed in this result is also optimal so far. 

Awards won at the 2009 ISEF
Second Award of $1,500 - Mathematical Sciences - Presented by Intel
________________________________________
2004 - MA018 
PATTERNS FOR IRRATIONAL ROOTS 1<N< CONTINUED AS EXPRESSED>
Luciano Luis A.
Especializada University Gardens, San Juan, Puerto Rico

Does a pattern exist in the irrational roots of integers n, 1<n<100, expressed as continued fractions? A list was created with the irrational roots using a computer program. Through an intense analysis patterns were found which establish that the expansion of the continued fraction of the terms: (a^2+1)^1/2, (a^2+2)^1/2, (a^2+2a)^1/2, (a^2+a)^1/2, [a^2+(2a)/3]^1/2, [a^2+(2a)/k]^1/2 always will have a determined amount of terms of the form:[a;2a], [a;a,2a], [a;1,2a], [a;2,2a], {a;3,2a], [a;k,2a], respectively. The method that was used to prove these conjectures was through the use of the continued fractions properties. It was observed that using the right mathematical operations we could get a quadratic equation with three terms that was solved using the quadratic formula. This gave the basis to prove the conjectures. Then several theorems were established for the patterns that were just found. A larger list of irrational roots would help in the search of new patterns. This investigation is a contribution, in particular to Number Theory and cryptography. 

Awards won at the 2004 ISEF
Second Awards of $1,500 - U.S. Air Force
________________________________________
2010 - MA019 
NOVEL RELATIONSHIP BETWEEN SQUARE SHAPES
Angel Rafael Agrinsoni-Santiago
Florencia Garcia High School, Las Piedras, PUERTO RICO

The main purpose is to state that each rectangle has a diagonal identical to the diagonal of an exclusive square. The theorem of Pythagoras establish that the hypotenuse (diagonal, c, d) is equal to the square root of the sum of each cathetus (c, b, l) raised to the second power for a right angle triangle. The relationship between both diagonal then is stated: given that<br><br>a^2+b^2=c^2, where a and b are the cathetus and c is the hypotenuse for a scalene right angle triangle. In the same analysis the sum of each side raised to the second power is equal to d^2, where l is any side of the isosceles right angle triangle. From this is obtained the following equation: <br><br> (2)^(1/2) (l)=d=(2)^(1/2) [((a+b)/2)^2+(|a-b|/2)^2 ]^(1/2)=c. From the derivation of this equation it is obtained that <br><br> l= [((a+b)/2)^2+(|a-b|/2)^2 ]^(1/2), <br><br>where l is a side of the isosceles right angle triangle which is the same value for the square sides. After this relationship, ((a+b)/2) is a constant and |a-b|/2 is the equation number. Further relationships between other geometrical shapes could be derived using this model and useful application designed for engineering and physics. 
________________________________________
2009 - MA019 
WEAK-SIGNAL-TRANSMITTING PROBLEM IN THE FRAMEWORK OF DYNAMIC GAME
Cheng Li
High School Affiliated to Fudan University, Shanghai, CHINA

A signal-transmitting problem is studied in the framework of the game theory. In the article, the concept of weak-signal-transmit is raised. Further it is pointed out that the use of some traditional methods like “forward induction” will result in the loss of information. First, a concrete game with two sub-games is presented. Both of two sub-games have three Nash equilibriums, and one is superior to another in the Pareto meaning. Player 2 can choose a subgame to enter. If a Pareto Superior subgame is chosen, it might have 0 payoff because of different equilibria forecast. While a Pareto Inferior sub-game is chosen, it will give player 1 a weak signal which induces a certain payoff. The article is talked about the transmitting problem in a more complex framework. Use the backward induction with sieve methods can compute these weak signals. This result can illustrate the phenomenon that players may prefer a certain payoff with less payoff rather than the uncertain one with more payoff. And player may induce the game into Parote Inferior because of the uncertainty in the game. This phenomenon may happen in some economy or sociology fields, with a wide range to apply. 
________________________________________
2004 - MA019 
THE MEMBERSHIP PROBLEM FOR IDEALS IN THE RING OF POLYNOMIALS OVER THE INTEGERS Z[X]
Arreche-Aguayo Carlos Eduardo
University Gardens High School, San Juan, Puerto Rico

It is known that there exists a feasible procedure to decide whether or not an arbitrary polynomial belongs to a given ideal in the ring of polynomials over the integers Z[x], if the ideal’s minimal basis is known. The problem this investigation addressed was: Given an ideal A and an arbitrary polynomial f(x) in Z[x], is there an effective procedure to decide whether or not f(x) belongs to A?. Three techniques that are useful to solve the membership problem in particular cases were discussed. The methodology consisted of considering specific cases in which it was known beforehand whether or not f(x) belonged to A and trying to find a pattern where these techniques were useful.<br><br>There already existed an effective procedure to find an ideal’s minimal basis, but it depended on being able to solve the membership problem. Therefore, a different approach to the problem was considered: Is there another effective procedure to find an ideal’s minimal basis that doesn’t depend on being able to decide if an arbitrary polynomial belongs to a given ideal? This investigation was successful in developing such a procedure. Hence, the membership problem for ideals in Z[x] was solved. For future investigations another effective procedure to solve the membership problem might be developed from any of the three techniques discussed here. This could be useful to solve the membership problem for ideals in the ring of polynomials with several variables and coefficients in the integers. 

Awards won at the 2004 ISEF
Honorable Mention Award - American Mathematical Society
________________________________________
2005 - MA019 
CAR PARKING MADE HARD!
Chun-Ju Lai
Taipei Municipal Chien Kuo Senior High School, Taipei, Taiwan, R.O.C.

A parking function is a sequence of numbers in [n] = {1, 2,..., n} of length n such that its rearrangement b in the ascending order satisfies the inequalities b(i) < i+1 for all i in [n], where b(i) is the ith term of b.<br><br> This definition leads us to the discovery of some interesting symmetric number patterns. In order to prove the symmetric property rigorously, we establish a combinatorial bijective correspondence between the set of all parking functions and the set of all labeled trees.<br><br> The bijection is equivalent, after some investigation, to the second bijection of Riordan and Foata between the set of all parking functions and the set of all acyclics. Our approach is much more succinct and clear and, moreover, makes the combinatorial proof of k-leading counting possible.<br><br> In this project, we also exhibit a bijective correspondence between the set of all parking functions and the seemingly unrelated "critical states of the chip-firing game".<br><br> The setting of the chip-firing game is then extended from a complete graph to a wheel graph and to a fan graph. To our surprise, the results are related to the classical Fibonacci and Lucas sequences.<br><br> 

Awards won at the 2005 ISEF
Second Award of $1,500 - Mathematics - Presented by Science News
________________________________________
2004 - MA019 
THE MEMBERSHIP PROBLEM FOR IDEALS IN THE RING OF POLYNOMIALS OVER THE INTEGERS Z[X]
Arreche-Aguayo Carlos Eduardo
University Gardens High School, San Juan, Puerto Rico

It is known that there exists a feasible procedure to decide whether or not an arbitrary polynomial belongs to a given ideal in the ring of polynomials over the integers Z[x], if the ideal’s minimal basis is known. The problem this investigation addressed was: Given an ideal A and an arbitrary polynomial f(x) in Z[x], is there an effective procedure to decide whether or not f(x) belongs to A?. Three techniques that are useful to solve the membership problem in particular cases were discussed. The methodology consisted of considering specific cases in which it was known beforehand whether or not f(x) belonged to A and trying to find a pattern where these techniques were useful.<br><br>There already existed an effective procedure to find an ideal’s minimal basis, but it depended on being able to solve the membership problem. Therefore, a different approach to the problem was considered: Is there another effective procedure to find an ideal’s minimal basis that doesn’t depend on being able to decide if an arbitrary polynomial belongs to a given ideal? This investigation was successful in developing such a procedure. Hence, the membership problem for ideals in Z[x] was solved. For future investigations another effective procedure to solve the membership problem might be developed from any of the three techniques discussed here. This could be useful to solve the membership problem for ideals in the ring of polynomials with several variables and coefficients in the integers. 

Awards won at the 2004 ISEF
Third Award of $1,000 - Mathematics - Presented by Panasonic Consumer Electronics Company
________________________________________
2005 - MA019 
CAR PARKING MADE HARD!
Chun-Ju Lai
Taipei Municipal Chien Kuo Senior High School, Taipei, Taiwan, R.O.C.

A parking function is a sequence of numbers in [n] = {1, 2,..., n} of length n such that its rearrangement b in the ascending order satisfies the inequalities b(i) < i+1 for all i in [n], where b(i) is the ith term of b.<br><br> This definition leads us to the discovery of some interesting symmetric number patterns. In order to prove the symmetric property rigorously, we establish a combinatorial bijective correspondence between the set of all parking functions and the set of all labeled trees.<br><br> The bijection is equivalent, after some investigation, to the second bijection of Riordan and Foata between the set of all parking functions and the set of all acyclics. Our approach is much more succinct and clear and, moreover, makes the combinatorial proof of k-leading counting possible.<br><br> In this project, we also exhibit a bijective correspondence between the set of all parking functions and the seemingly unrelated "critical states of the chip-firing game".<br><br> The setting of the chip-firing game is then extended from a complete graph to a wheel graph and to a fan graph. To our surprise, the results are related to the classical Fibonacci and Lucas sequences.<br><br> 

Awards won at the 2005 ISEF
Award of $1,000 - Mu Alpha Theta, National High School and Two-Year College Mathematics Honor Society
________________________________________
2009 - MA020 
ANTIRECESSION MARKETING IN ACTIVITY OF COMMERCIAL BANKS
Chingiz Zhumagulov
The Republican Specialised in Physics and Math School, Almaty city, KAZAKHSTAN

Scientific project is devoted to definition of present-day marketing role in crisis management, development of theoretical and practical mechanism and tools of antirecession marketing in the activity of commercial banks to overcome negative effects of world financial crisis. <br><br>Rapid formation of competitive national economy depends on efficient development of finance services market including commercial banks management in terms of marketing. At the present time banks experience grave crisis and principles and procedures of antirecession marketing need to be used to overcome negative effects of world financial crisis. <br><br>The project covers such issues as marketing management fundamentals of bank product, concept and nature of bank marketing, characteristics of antirecession marketing bank management. 
________________________________________
2010 - MA020 
ANALYZING THE FIBONACCI SEQUENCE, PHASE TWO
Hector Luis Maldonado-Perez
Croem High School, Mayaguez, Puerto Rico, PUERTO RICO

For many years, the Fibonacci sequence has been well studied by mathematicians. Many people have come to say that the Fibonacci sequence has many secrets hidden. Such sequence begins with a 1, 1; and each term is equal to the sum of the two preceding numbers. To seek numbers in a higher position, two formulas were created: Formula “Z3” & Formula “G7”. Although both formulas are based on algebraic calculations which lead to the seeking of numbers in a higher position, they are different. Formula “Z3” ((t_x)^2 + (t_x+1)^2) = t_2x+1 refers to that by choosing two consecutive numbers, raising them to the second power and adding them, will result in a value to be placed in a odd position. Formula “G7” ((t_x+2)^2 – (t_x)^2 = t_2x+2) means that by choosing two numbers, one is the first and the second is leaped by a number, raising the to the second power and then subtracting the first from the second will give a value which will be placed in an even position in the sequence. These formulas provide us with a whole new series of facts to be added to the knowledge of sequence subjects. They will be, from now on, part of the discoveries related to the Fibonacci sequence. Although many people think that in Fibonacci there is no place for creation there are still many topics that can be explored leading to helpful tools for Mathematics.<br><br>Note;<br><br>*t_x means t subscripted by x 
________________________________________
2004 - MA020 
TRIANGLES: FROM TARTAGLIA TO SIERPINSKI
Burgos-Vazquez Neysha Marie
Carmen Belen Veiga, Juana Diaz, Puerto Rico

Among the Tartaglia, Pascal and Sierpinski triangles, there exist interesting relationships. Both the Niccolo Tartaglia triangle and the Blaise Pascal triangle are obtained by adding previous line numbers which is one of the reasons why they have similar characteristics. The purpose of this investigation is to compare the horizontal, vertical and diagonal lines in the Tartaglia and Pascal triangles. Also, painting the prime numbers in an appropriate way, in the Pascal triangle and using modular arithmetic, some patterns were found. In particular the Sierpinski´s triangle is obtained with this process.<br><br>It is also shown that if a particular pattern can be established among the horizontal, vertical and diagonal lines in Tartaglia´s triangle, then a similar pattern is found in the Pascal triangle and viceversa. Also, a relationship between the Pascal triangle and the Cartesian plane was found. <br><br>This investigation will continue, finding more relationships between these triangles. The method developed in this investigation, which uses modular arithmetic, is an excellent way to obtain different fractals.<br><br> 
________________________________________
2007 - MA020 
CRYPTIC KEYS TO CORRESPONDENCE: FERMAT'S LITTLE THEOREM APPLIED TO CRYPTOGRAPHY
Rebecca Jean Rapf
Sheridan High School, Sheridan, Wyoming, USA

The purpose of the project was to understand Fermat’s Little Theorem, how it can be applied to cryptography, why algorithms can speed up the process, and, finally, to explain how one particular algorithm applies the theorem while using what would appear to be very different calculations.<br><br> It was predicted that the algorithm could be explained using the properties of modular arithmetic and binary numbers.<br><br> Research was done on the basic principles behind primality testing. The functionality of primality testing as applied to cryptography was investigated. Also, various algorithms were explored until one was selected based on its simplicity. The relation of the algorithm to the original theorem was then explained. <br><br> The result was that the algorithm provided a simplified and convenient way to determine probable primality for large integers based on the usage of the binary number system and modular arithmetic, while yielding the same results as the original theorem.<br><br> 
________________________________________
2006 - MA020 
CHARACTER SUMS AND RAMSEY PROPERTIES OF GENERALIZED PALEY GRAPHS
Nicholas Michael Wage
Appleton East High School, Appleton, Wisconsin

In mathematics, a graph is a collection of points connected by lines (what most people would think of as a network). Graph theory has applications in many fields, from internet searching and neural networks to social sciences and terrorist networks. <br><br><br> In this work I studied the limiting and sub-graph properties of Paley graphs and their generalizations, an important and widely studied class of graphs. The results partially validated an old conjecture of Paul Erdös and relate to a classic paper of Evans, Pulham, and Sheenan. The proofs used tools from number theory, including Weil's deep theorem on the Riemann Hypothesis for finite fields.<br><br><br> Given a prime p such that 4 divides p-1, we obtain a Paley graph by taking as vertices the integers {0, 1, ... , p-1}, with an edge between x and y if and only if x - y is a square modulo p. In 1962 Erdös made a conjecture about the limiting properties of graphs concerning the number of complete and empty subgraphs they contain. Although this conjecture was shown in 1989 to be false in general, I proved that it does indeed hold when we restrict our attention to Paley graphs. The results were also extended to generalized Paley-type graphs. In addition, I used special character sum evaluations to determine exact values for questions analogous to those studied by Evans, Pulham, and Sheenan. <br><br> 

Awards won at the 2006 ISEF
Third Award of $250 - American Mathematical Society
Second Award of $500 U.S. Savings Bond - Ashtavadhani Vidwan Ambati Subbaraya Chetty (AVASC) Foundation
UTC Stock with an approximate value of $2000. - United Technologies Corporation
________________________________________
2010 - MA021 
THE MEMBERSHIP PROBLEM IN Z[X]
Axel Antonio Garcia-Burgos
Croem High School, Mayaguez, PUERTO RICO

The main purpose is to determine if a polynomial P with integer coefficients belongs to the ideal generated by a set of polynomials. If the given polynomial P and the given set of polynomials (P_1, P_2, P_3,..., P_n) belong to the set Z [x] (polynomials with integer coefficients), there is a procedure to find Q_1, Q_2, Q_3,..., Q_n when it belongs to Z [x] so that P= P_1Q_1+P_2Q_2+...+ P_nQ_n. If not, it becomes necessary to determine whether such polynomials do not exist. It was hypothesized that there was a method to determine whether the polynomials (Q_1, Q_2, Q_3,..., Q_n) are in Z [x]. The methods used were: matching coefficients, Cramer's rule, Bezout theorem, Euclidean algorithm, and Euclid's extended theorem. Through mathematical observation, it was determined that polynomial P would belong to this set, but this result depends on the coefficients in the polynomials given, and their degrees. This is possible because a relationship, between the greatest common divisor of the coefficients of polynomials (P_1, P_2, P_3,…, P_n ) and the coefficients of the polynomial P, can be found. Significant applications of this research would be used in Computational and Abstract Algebras. Such investigation will have a progress trough out time, furthermore and emphasis will be aspired.<br><br>NOTE:<br><br>*Q_n means Q subscripted by n.<br><br>*P_n means P subscripted by n. 
________________________________________
2009 - MA021 
ON REDUCING POLYNOMIALS OVER THE FIELD OF RATIONAL NUMBERS
Gabit Gabdullin
Kazakh- Turkish School, Astana city, Astana city, KAZAKHSTAN

The aim of the study: Creation of the final algorithm for discovery all rational radicals of a polynomial and the final algorithm determining whether a polynomial is reducible over Q.<br><br> Hypothesis:<br><br>1. There is an ultimate algorithm for discovery all rational radicals of the polynomial f(x).<br><br>2. There is an ultimate algorithm, which determines whether f(x) is reducible or non-reducible over Q.<br><br>3. There is an ultimate algorithm for factorization of f(x) in non-reducible factors over Q.<br><br> The novelty of the study: we give positive answers to the questions (R), (Ir), (F)<br><br>(R) Do we have an ultimate algorithm for discovering all rational radicals of the f (x) polynomial?<br><br>(Ir) Do we have an ultimate algorithm defining the reducibility or non-reducibility of f (x) over Q? <br><br>(F) Do we have an ultimate algorithm for the factorization of the polynomial f (x), i.e. is it possible to factorize f (x) in non-reducible factors over Q?<br><br> The methods used: the author used the Polynomial Divisibility Theory, the Euclidean algorithm and the Polynomial Factorization Theory.<br><br> Conclusions and applications: The results of the study can be used in research of polynomial radicals’ properties in the Number Theory and in solving real tasks. 
________________________________________
2004 - MA022 
MXN ADMISSIBLE BOARDS
Huan-Chun Yeh, Taipei Municipal Shyr Jainn Junior High School
No 67, Shinhai Rd,. Sec. 7, Wensh Chiu, Taipei, Taiwan 116, R. O. C

<br><br> In an mxn board, we assign each square a real number so that, for each square, the sum of its neighboring squares is equal to 1. Here the neighboring squares are those sharing a common side with the given square. An mxn board capable of such assignment is called an admissible board, and the sum of the mn assigned numbers is denoted by f(m,n). The process of our prove incates that f(m,n) is indeed well-defined.<br><br> In this project, we first prove that mxm admissible board exists if and only if m is even and in which case f(m,m)= m(m+2)/4. By studying mxn variable boards, we propose an algorithm to find all mxn admissible boards, and to calculate the value of f(m,n) if it is defined. We also prove that every mxn admissible board is 2m+2-column-cyclic and f(m,2m+2) =[(m+1)^2/2], where [.] stands for the Gaussian notation. This column-cyclicity enables a fast calculation of f(m,n), especially when n is large. We also characterize all the mxn admissible boards: an mxn board fails to be admissible if and only if m = 2^(a-1)b-1,and, n = m+2a^q or m-2a^q where q, a are positive integers and b is an odd integer.<br><br> 

Awards won at the 2004 ISEF
Third Award of $250 - American Mathematical Society
________________________________________
2010 - MA022 
EGYPTIAN FRACTIONS IN LIGHT OF ALGORITHMS
Sebastian Sueiras
Colegio San Ignacio de Loyola, San Juan, PUERTO RICO

The goal designed for this investigation was, given the viability of the iterative processes, or algorithms, for expanding a common fraction, to expose Egyptian fractions in terms of these algorithms. Thas is, to present Egyptian fractions as a subsequent theme to the application of the repetitive methods. With the purpose of expanding the scope of the research, two generalizations, originally for the mathematic community to develop to their liking, were proposed; (1) if by implementing the identity 1/n=1/(n+1),from 1/n = 1/n, common fractions can be decomposed into a collective sum, or Egyptian fraction, and (2) the ‘factorial algorithm’ as ¿effective? and ¿efficient?, both posed obviously as questions. In the first, the basic identity for Egyptian fractions results in a trivial case for the expansion of Egyptian fractions, following the logic of having x times 1/y, where x is the numerator of the common fraction to be worked with, x/y. For the second, the algorithm’s quality of effectiveness is reaffirmed, since it offers a short expansion of unit fractions with manageable integers; however, it was discarded as efficient for having been based on a tedious and unnecessarily repetitive process, even for the most simple of cases. By adding the resurgence and explanation of the Fibonacci-Sylvester algorithm, the extent of applications for Egyptian fractions is limited to the use of algorithms for their development. Given this, it is imperative to introduce once again the purpose of the investigation as its conclusion. This is only a sample of the inherent potential of algorithms for Egyptian fractions. Much can be done to develop even this and other investigations, as to explore new twists and possibilities within the scope of work for the study of Egyptian fractions. 

Awards won at the 2010 ISEF
Scholarship Award of $12,500 per year, renewable annually - Florida Institute of Technology
________________________________________
2004 - MA022 
MXN ADMISSIBLE BOARDS
Huan-Chun Yeh, Taipei Municipal Shyr Jainn Junior High School
No 67, Shinhai Rd,. Sec. 7, Wensh Chiu, Taipei, Taiwan 116, R. O. C

<br><br> In an mxn board, we assign each square a real number so that, for each square, the sum of its neighboring squares is equal to 1. Here the neighboring squares are those sharing a common side with the given square. An mxn board capable of such assignment is called an admissible board, and the sum of the mn assigned numbers is denoted by f(m,n). The process of our prove incates that f(m,n) is indeed well-defined.<br><br> In this project, we first prove that mxm admissible board exists if and only if m is even and in which case f(m,m)= m(m+2)/4. By studying mxn variable boards, we propose an algorithm to find all mxn admissible boards, and to calculate the value of f(m,n) if it is defined. We also prove that every mxn admissible board is 2m+2-column-cyclic and f(m,2m+2) =[(m+1)^2/2], where [.] stands for the Gaussian notation. This column-cyclicity enables a fast calculation of f(m,n), especially when n is large. We also characterize all the mxn admissible boards: an mxn board fails to be admissible if and only if m = 2^(a-1)b-1,and, n = m+2a^q or m-2a^q where q, a are positive integers and b is an odd integer.<br><br> 

Awards won at the 2004 ISEF
Fourth Award of $500 - Mathematics - Presented by Panasonic Consumer Electronics Company
________________________________________
2004 - MA023 
THE SUM OF THE THIRD POWER OF THREE CONSECUTIVE NATURAL NUMBERS
Luis Angel Alvarado 
Escuela Manuel Martin Monserrate, Santa Isabel, Puerto Rico, USA

The purpose of this research was to demonstrate what happens with the sum of the third power of three consecutive natural numbers. The addition of the third power of three consecutive natural numbers is always a multiple of nine.<br><br>Three consecutive natural numbers were selected and were expressed with variables. After that, the third power of each natural number was calculated, and then expressed using variables. The total was divided by nine to see if the quotient was a whole number without residue. If that happens, the sum of the third power of the three consecutive natural numbers is always a multiple of nine. When the variables are substituted by consecutive natural numbers the sum is always a multiple of nine.<br><br>The result of this research brings the opportunity to know this special characteristic of the sum of the third power of three consecutive natural numbers.<br><br> 
________________________________________
2003 - MA023 
INSTRUCTIONAL MODULE TO REVIEW THE COLLEGE BOARD IN THE AREA OF MATH
Jose E. Morales
Aurea E. Quiles Claudio High School, Guanica P.R.

The purpose of this investigation was to make available to the students and to candidates for Admission and Evaluation Tests of College Board, a review of concepts and skills in the area of mathematics. After collecting the statistic data to justify this work, the problem established was that it would be possible to develop an instructional module to review the College Board in the area of math using the Microsoft Power Point program. It was inferred that this would be possible if the command and options of the program and the related math theory were studied. Finally, the instructional module was constructed by developing skills and concepts in the areas of Arithmetic, Algebra, Geometry, Statistics and Probability and integrating different multimedia. Also, this module includes a practice test, in which the student can check the time he needs to make it, and the number of correct answers. It is projected to divulge this module and to continue with the review and actualization of the module. 
________________________________________
2005 - MA023 
IDENTIFYING TRENDS AND ANOMALIES IN PUGET SOUND MARINE TEMPERATURE DATA
Benjamin Luke Small
Bellarmine Preparatory School, Tacoma, Washington

Existing data on water temperature gathered from two sites in Puget Sound from 1932-1975 were examined to search for statistical trends and anomalies. The data was imported into Excel and cleaned up for analysis, and surface-level measurements were discarded because they were influenced too much by external forces such as wind and weather. The data was normalized to remove the normal sinusoidal fluctuations in temperature during the course of the year that would otherwise skew the data. After a thorough study of the data, it was concluded that it was not possible to identify trends or anomalies in the data, nor was it possible to identify any upward trends in the water temperature data that may have been due to global warming. Possible reasons for this result include: insufficient data; one or more countervailing causes that may have offset any warming in water temperature; or global warming did not occur in the waters of Puget Sound prior to 1975. 
________________________________________
2007 - MA023 
FUNCTION POOLS THE MATHEMATICS OF DATA BACKUP
Hagai Helman
Reut School, Jerusalem, Israel

Suppose we have 3 disks we would like to backup (the numbers in this example are, of course, arbitrary). The natural way to do it is to copy each disk. This method, however, is not efficient. If an original disk and its copy are lost, the information they contained is also lost, even though we still have four disks undamaged. As an alternative, we can make a set of four disks, any three of which can be used to recreate the original disks.<br><br><br> The question I inquired in my project was: can we make more than four disks that still have this quality? How many more?<br><br><br> First, I generalized this question and rephrased it in mathematical terms.<br><br><br> I made a literature research. Apparently, and no one inquired this problem before. I also found relations between special cases of my problem and some solved and unsolved problems in combinatorics. <br><br><br> I proved many new theorems related to this subject. I solved several important special cases completely, and fount partial solution for the general case.<br><br><br> Except my theoretical results, I also developed an applicative technique to build virtually unlimited number of backup-data-units, in order to backup real digital data-units. I developed it using properties of finite fields, and linear algebra. This technique can be used not only to backup disks, but also for other uses. for example, it can be used to make streaming video transmission on-line faster.<br><br> 

Awards won at the 2007 ISEF
Third Award of $250 - American Mathematical Society
________________________________________
2006 - MA023 
KEPLER'S QUEST - ISOPERIMETRIC BIOGEOMETRY
Daniel Karoly Bezdek
Father Lacombe High School, Calgary, Canada

My project centres around three major topics: (1) In connection with the classical and well-known Kepler Problem that was just recently solved, I study a quite recent modern version of it called the Discrete Kepler Problem and solve it for small number of congruent spheres as well as for convex configurations of arbitrary number of spheres. (2) I prove the Steiner Conjecture within the class of convex higher order deltahedra by showing that the regular icosahedron has the smallest isoperimetric quotient among all convex higher order deltahedra. (3) In connection with a well-known problem of Poincare on existence of simple closed geodesics on the boundary of convex bodies, I classify all possible simple closed geodesics on the boundary of convex higher order deltahedra. All this creates a theory for convex higher order deltahedra that led me to introduce a new protein folding model. 

Awards won at the 2006 ISEF
Third Award of $1,000 - Mathematics - Presented by Lucent Technologies
________________________________________
2007 - MA023 
FUNCTION POOLS THE MATHEMATICS OF DATA BACKUP
Hagai Helman
Reut School, Jerusalem, Israel

Suppose we have 3 disks we would like to backup (the numbers in this example are, of course, arbitrary). The natural way to do it is to copy each disk. This method, however, is not efficient. If an original disk and its copy are lost, the information they contained is also lost, even though we still have four disks undamaged. As an alternative, we can make a set of four disks, any three of which can be used to recreate the original disks.<br><br><br> The question I inquired in my project was: can we make more than four disks that still have this quality? How many more?<br><br><br> First, I generalized this question and rephrased it in mathematical terms.<br><br><br> I made a literature research. Apparently, and no one inquired this problem before. I also found relations between special cases of my problem and some solved and unsolved problems in combinatorics. <br><br><br> I proved many new theorems related to this subject. I solved several important special cases completely, and fount partial solution for the general case.<br><br><br> Except my theoretical results, I also developed an applicative technique to build virtually unlimited number of backup-data-units, in order to backup real digital data-units. I developed it using properties of finite fields, and linear algebra. This technique can be used not only to backup disks, but also for other uses. for example, it can be used to make streaming video transmission on-line faster.<br><br> 

Awards won at the 2007 ISEF
Third Award of $1,000 - Mathematical Sciences - Presented by Acatel-Lucent
________________________________________
2007 - MA024 
INVESTIGATION AND SOLUTION OF THE EQUATION IN GROUPOID
Olga Meshcheryakova
High school #69, Lipetsk, Russia

One of the major algebra’s reverse problems is investigation and solution of the equations. Binary ratio of equality = should be set for record of the equation in set S. Binary algebraic operation — should be set above elements of set. Such structure is groupoid Ã = <S, 0>. A reverse problem of algebra in groupoid consists in finding such element x, that Ax=B (sign on operation is letting down) and in the investigation and solution of this equation. The realization of some properties for parameters A and B, presence of the certain elements accompanying them are necessary for this purpose.<br><br>The purpose of the given work is development of a method of investigation and the solution of the equations in groupoids without performance of laws of operations.<br><br>The method of the solution of the equations suggested in work consists in the task of ratio for parameters and their accompanying elements, an establishment of criteria of resolvability, introduction of accompanying operations. The reliability of the suggested method is illustrated in the given work.<br><br>During scientific work the method of investigation and the solution unilateral (linear) have been developed, and also the bilateral equations, investigation of the equations in set groupoids is executed, solutions of these equations are found. The suggested method is possible for applying in systems with an artificial intelligence as many laws there can be broken. Results of the lead investigations can be applied also in such area of a science, as cryptography. 
________________________________________
2008 - MA024 
AN APPROACH TO SOLVING THE M&M'S CONJECTURE
Tzu-Yu Lin
National Tainan Girls' Senior High School, Tainan, Taiwan R.O.C., CHINESE TAIPEI

For a list [a,b,c] of three real numbers, there is a unique number d such that the mean of the appended list [a,b,c,d] is the same as the median of [a,b,c]. If this process is repeated to the list [a,b,c,d], a new list [a,b,c,d,e] is formed. Keep repeating this process for various initial lists, Harris S. Schultz and Ray C. Shiflett have observed that the lists will be eventually constant. Although this phenomenon has been observed numerically, it has not been rigorously proved. <br><br> In this project, we have discovered an interesting pattern: if the initial list takes the form [-x,1,x] with x no less than a certain constant 41.625, then after repeating the process 70 times all appended terms will be equal to 41.625. Similar phenomenon is also observed: when x lies in certain intervals, the resulting lists will also become constant but the location of x appears to bear no relationship to the number of steps required to reach the constant. Our result generalizes the previously known result of the above Schultz and Shiflett. 
________________________________________
2008 - MA025 
MAGICIAN'S PREDICTION OF CARD SUITS
Sheng-Hao Chiang
National Experimental High School at Hsinchu Science Park, Hsinchu City, CHINESE TAIPEI

This project is motivated by the following problem given at the International Mathematics Tournament of the Towns 2004: "The audience shuffles a deck of 36 cards, containing 9 cards in each of the suits: spade, heart, diamond and club. A magician predicts the suit of the cards, one at a time, starting with the uppermost one in the face-down deck. The design on the back of each card is an arrow. An assistant examines the deck without changing the order of the cards, and points the arrow on the back of each card either towards or away from the magician, according to some system agreed upon in advance with the magician. Is there such a system which enables the magician to guarantee the correct prediction of the suit of at least (a) 19 cards; (b) 20 cards?" <br><br> Attacking this problem, the following results are obtained:<br><br>(1) We sharpen the result by designing a system which guarantees the correct prediction rate of 26 cards. <br><br>(2) We also obtain an asymptotic value of 81.07% correct prediction rate when the number of cards is sufficiently large. 
________________________________________
2006 - MA025 
ILLUST-LOGIC WILL SAVE THE EARTH
Saori Koshino
Nanzan Girls' High School, Nagoya-shi, Aichi, Japan

As a method to deduce the real image from its data, imaging is an important key in science as to make invisible things visible to unlock the mystery of nature.<br><br>Illust-Logic is a puzzle game popular in Japan. The objective is to discover a hidden figure in a square from given numbers on its edges, which is parallel to the imaging. In most cases, Illust-Logic problems have one good answer. I deduced a formula to solve them. I also found there are problems which have two or many figures as its answer. I can see whether the problem is good or not.<br><br>In order to apply these findings as a new method for imaging to many fields in science, I introduced a new Illust-Logic game.<br><br>To make sure this new game is effective and practical, I made experiments using collision of variety of balls, representing atoms or electrons. I arranged balls in grooves on a base and rolled other balls in grooves on a slope attached to the base. I measured the time from release of the first ball to the fall of the last ball. I could detect small single collision, which gives the first number in Illust-Logic and big collision in chain, which gives the second and third number. I could transform these numbers into figures by new Illust-Logic technique and positions of balls were determined. <br><br>From above, I believe the new game with my technique can be a powerful tool for imaging to promote science.<br><br> 
________________________________________
2003 - MA025 
DEVELOPMENT OF NEW FORMULAS TO GENERATE MERSENNE PRIMES
Maria C. Bintron
Aurea E. Quiles Claudio High School, Guanica P.R.

The purpose of this investigation was to study general properties of the Primes of Mersenne and the formula which generates them. The established problem is the following: given the formula of Mersenne Mn = 2^(n) -1, where n and 2^(n) -1 are prime numbers; will be possible to develop new formulas that make easy the search of Mersenne Primes numbers in a given interval? Using the facts that prime numbers greater or equal to 5 are part of two arithmetical sequences: (6m-1) and (6m+1), (m > 1) and that the Mersenne prime numbers known end in 1 and 7, then it was possible to develop some new formulas. Some properties of the Mersenne prime numbers were analyzed: the number of digits and different patterns were observed, in particular the last digit of the numbers.<br><br>The following formulas were developed:<br><br>1) Mm = 2^(12m-7) -1 2) Mm = 2^(12m+1) -1 m > 1<br><br>All possible prime numbers ended in 1, > 31.<br><br>3) Mm = 2^(12m-1) -1 4) Mm =2 ^(12m-5) -1 m > 1<br><br>All possible prime numbers that ended in 7, > 127.<br><br>These formulas have the benefit that they don’t depend on a list of prime numbers and they also help to decrease the test values in a given interval. All the formulas were validated through the computer program for mathematical computers V. 2.1 in the interval [1, 2^(3607)]. It is projected to continue analyzing the properties and patterns of these numbers in order to generate new Mersenne prime numbers. 
________________________________________
2004 - MA025 
RESEARCH ON THE NUMBER-REASONING PROBLEM
Ning Zhang
The high school affiliated to Fudan University, Shanghai, China

This paper concerns a number-reasoning problem. A professor writes a positive integer on each of n students' foreheads and divides the students into two groups. There are m (m>=n/2) students in one group and the rest are in the other group. The sum of numbers in one group is equal to that of the other. The students know the size of the two groups, but they do not know to which group they belong. Each student can see the numbers on the others' foreheads, but he cannot see the number on his own. The professor asks each of the students one by one whether he knows his number till someone replies with a correct answer. The question is whether someone is able to answer his number after several rounds of questioning and in which round this occurs.<br><br> This problem is a typical case of nested reasoning, a basic problem in symbolic logic. Only special cases can be solved using conventional methods, and the complexity of the solution increases exponentially when the number gets larger. We establish a series of definitions, based on which the problem description is translated into mathematical representation and the essential part of the problem is analyzed using algebraic methods. After several important propositions were proved, we were able to systematically analyze the problem. Then the problem is solved perfectly. By employing the new method, the efficiency of the solution is greatly improved. The method can also be effectively applied to other problems of nested reasoning. 

Awards won at the 2004 ISEF
Third Award of $250 - American Mathematical Society
________________________________________
2007 - MA025 
ENDLESS PROPAGATION--THE ARITHMETIC RULES OF REGULAR PENTAGONS
Albert Chiehyang Liu
Kaohsiung Municipal Kaohsiung Senior High School, Kaohsiung City, Taiwan (ROC)

This study originated from a puzzle game in which people were asked to construct a regular pentagon from an unlimited supply of two kinds of golden triangles of fixed sizes: acute golden triangle A and obtuse golden triangle B. When completing such an exercise, it is natural to ask:<br><br>1. What sizes of regular pentagons are constructible?<br><br>2. Identify the irreducible pentagons in this construction: pentagons containing no sub-pieces which themselves can construct smaller pentagons.<br><br>Our main results are as follows:<br><br>1. We find that the side lengths of the constructible regular pentagons are exactly the integral linear combinations of 1 and the reciprocal of the golden ratio. We also proved that the numbers of A and B are expressible as quadratic forms of the coefficients.<br><br>2. The sides of the irreducible objects can be characterized by a weighted linear combination of two successive terms of Fibonacci numbers. We also established two equations between prime pentagons expressing multiplicities of the decomposition. <br><br>3. We establish the "prime factorization of the pentagons": every constructible regular pentagon can be decomposed into either one or two kinds of successive prime pentagons. <br><br>Finally, we show that the constructible pentagons may be classified according to the prime factorization, and such a classification can be illustrated by an interesting planar pattern. A complete description of all prime factorizations can then be achieved from such patterns. 

Awards won at the 2007 ISEF
Third Award of $250 - American Mathematical Society
________________________________________
2005 - MA025 
GENERALIZATIONS OF KOCH CURVE AND ITS APPLICATIONS
Zhu XinRan
Shanghai Southwest Weiyu Middle School, Shanghai, China

Fractal geometry is one of the prevailing branches in modern mathematics. Koch curve, discovered by Sweden mathematician Von Koch in 1904, is an important curve in Fractal geometry. Formed in a close way, it shapes like snowflake, and has finite area, though infinite length.<br><br>In this paper, some generalizations are made by changing either the initiator or the generator in the formation of Koch curve. The initial figure will be taken mainly as a regular m-gon, while a variety of generators will be built on it, including squares, isoceles triangle, etc. For each case considered I proved the limiting curve has infinite length and determine explicitly the exact area value of the region surrounded by the limiting curve. Furthermore I also consider a generalization in the three-dimensional space with regular polyhedrons as initiators, and regular tetrahedrons or cubes as generators added on each face of the polyhedron. The resulting sets now are closed surfaces and it has been proved that they have infinite areas and determine explicitly their exact volumes of the surrounded bodies. After theoretical work, a mathematic model related with Koch curve and its generalized figures has been built up with computing technology. <br><br>After improving the simulation program by using OpenGL in Visual C++, a software package with dynamic interface has been developed for the purpose of providing a visualizing way for teaching fractal. When loading different parameters which affect the shape of figures and conduct the similar transforms like Koch curve, a great many geometry figures can be obtained. Plane figures are very functional and practical in art, and can be widely applied in popular craft, pattern design, printing and dyeing, etc. 3-D polyhedrons are expected to apply widely in crystal cutting, architecture, etc. Also, some applications of Koch curve in the fields like bio-chemistry have been developed. 

Awards won at the 2005 ISEF
Third Award of $1,000 - Mathematics - Presented by Science News
________________________________________
2007 - MA025 
ENDLESS PROPAGATION--THE ARITHMETIC RULES OF REGULAR PENTAGONS
Albert Chiehyang Liu
Kaohsiung Municipal Kaohsiung Senior High School, Kaohsiung City, Taiwan (ROC)

This study originated from a puzzle game in which people were asked to construct a regular pentagon from an unlimited supply of two kinds of golden triangles of fixed sizes: acute golden triangle A and obtuse golden triangle B. When completing such an exercise, it is natural to ask:<br><br>1. What sizes of regular pentagons are constructible?<br><br>2. Identify the irreducible pentagons in this construction: pentagons containing no sub-pieces which themselves can construct smaller pentagons.<br><br>Our main results are as follows:<br><br>1. We find that the side lengths of the constructible regular pentagons are exactly the integral linear combinations of 1 and the reciprocal of the golden ratio. We also proved that the numbers of A and B are expressible as quadratic forms of the coefficients.<br><br>2. The sides of the irreducible objects can be characterized by a weighted linear combination of two successive terms of Fibonacci numbers. We also established two equations between prime pentagons expressing multiplicities of the decomposition. <br><br>3. We establish the "prime factorization of the pentagons": every constructible regular pentagon can be decomposed into either one or two kinds of successive prime pentagons. <br><br>Finally, we show that the constructible pentagons may be classified according to the prime factorization, and such a classification can be illustrated by an interesting planar pattern. A complete description of all prime factorizations can then be achieved from such patterns. 

Awards won at the 2007 ISEF
Fourth Award of $500 - Mathematical Sciences - Presented by Acatel-Lucent
________________________________________
2003 - MA026 
DEEP IMPACT OF NUMBER ELEVEN OVER THEORY OF NUMBERS
Jose Joel Soto
Ramón J. Davila, Coamo, Puerto Rico 

This research deals with the Deep Impact of Number eleven in the Theory of Numbers. The idea emerges from Pascal Triangle. The researcher studied sixty-six rows of Pascal. Those rows exhibit fifty-five multiples of eleven in its initial stage. Those quantities of multiples were reduced in arithmetical progression up to zero considering ten rows. Observing Pascal, a new row of multiples of (eleven) is generated. In this case (two) triangles containing fifty-five multiples of (eleven) are generated, having the same properties as the first. The procedure of obtaining triangles that are multiples of eleven increases arithmetically, each time it reaches ten rows. <br><br> This procedure is applied to all prime numbers. With this procedure, it is noticed that the adjacent areas corresponding with those expansions represent a perfect summary of facts generated initially. <br><br> Inspired with a mathematical problem that required to find all the numbers of two digits that totaled eleven so that when interchanged generated a square, it was decided to explore with numbers of three digits applying the same procedure. Also the researcher applied that procedure with multiples of eleven that generated squares, finding some important sets of number with specific characteristics. For example a new succession combining it with the Fibonacci succession was created combining it with the Fibonacci sucession. Also, the researcher found a Magic Square, with a specific set. The elements of this set in coordination with the (3x+1) problem is essential to explain the square generated when numbers of this set are interchanged. <br><br> 
________________________________________
2007 - MA026 
EASE POLYGONS ARE NOT EASY
Cheng-Tao Chung
Taipei Municipal Jianguo High School, Taipei, Taiwan

Equilateral and semi-equiangular (abbreviated EASE) polygons are defined to be polygons with sides all of the same unit length that are connected with a fixed positive or negative angle. In this research, the number of EASE polygons corresponding to a given number of sides and a fixed angle will be discussed. Each EASE polygon can be expressed by several sequences, based on its characteristics such as angles, vectors, and partitions. Converting EASE polygons into mathematical sequences simplify the problem. Rough upper bound and lower bound are obtained by analyzing the sequences via methods of permutation. In order to make the bounds more precise, the many-to-one nature of the sequences has been eliminated. In other words, a EASE polygon can be expressed by more than one sequence, due to the symmetric nature of the configuration. Ways to create more EASE polygons from given ones are mentioned which in turns provide us useful information. Although not quantitative, the qualitative information gives us a better idea of the actual number of EASE polygons. The "flip" and "insert" operations are introduced. It follows from the definition that when a EASE polygon is flipped, a special part of the whole sequence is rearranged; when a EASE polygon is inserted, fragments of short sequences are inserted into the original one. In future, we intend to find other applications to the methods developed here and to find a better estimation of the number of EASE polygons. 

Awards won at the 2007 ISEF
Second Award of $500 - American Mathematical Society
Third Award of $1,000 - Mathematical Sciences - Presented by Acatel-Lucent
________________________________________
2007 - MA027 
INTERACTING WHIT SIERPINSKI
Sugei Margarita Vargas Martinez
Praparatoria Iztapalapa 1, Colonia Lomas de Zaragosa, MEXICO

This project started with a bibliographical research on the construction of fractales, specially on the so-called Sierpinski' s triangle. Subsquently, the almost magical connections that this mathematical element maintains whit concepts such as multiples and divisors, raising to the power of two, the second modulus, Pascal' s triangle, Newton' s binomial, and Pythagoras theorem, among others, werechecked. Finally, someof the properties of this fractal and triangles in general, were also revised. A document which contains a series of mathematical practices was written after checking all the bibliographical material in this doament math laboratory is simulated, providing the student with the chance of learning and such as : multiples, fractions, area calculation, base two second modulus, and even limits. In these practices every theme is introduced with help of Sierpinski' s triangle, its construction and its relation with Newton's binomial, Pascal' s triangle and other conceptes previously mentioned. Tha project also includes all the solutions as well as some other recomendations for the teacher, since it is intended to be a didactic support for students as well as for teachers at this educational level. Now the proyect is being optimized upon the judge' s observations. 
________________________________________
2004 - MA027 
PATTERN RECOGNITION USING COMPUTER PRECISION ERRORS IN THE ESTIMATION OF FRACTAL DIMENSION
Igor Kreimerman
Israel Arts and Science Academy, Jerusalem, Israel

Many objects around us such as trees, mountains and rivers have a fractal nature, namely, small portions of them are similar in shape to the whole. Different classes of objects tend to have different fractal dimensions. An interesting question is whether the fractal dimension of shapes can be used for pattern recognition. The naive answer is negative, since different shapes may have equal dimensions. However, a well-known disadvantage of computers – their finite precision – turns out to be a helpful tool in this task. In computer analysis, the fractal dimension is often estimated from the relation between the minimal number of boxes of side R needed to cover the object and the length of R. Stretching or skewing a fractal does not change its dimension. Moreover, applying a stretching transformation to a fractal is equivalent to applying the inverse transformation to the boxes. Therefore, the fractal dimension is not expected to change when using rectangles rather than squares. Indeed, for the skyline of a mountain ridge, both methods result in the same dimension. For trees, however, different results are obtained. I show that these deviations are due to the finite resolution of computer images and investigate the general conditions under which they occur.<br><br><br>To sum up: The use of the absolute fractal dimension together with the deviations that arise from different methods of estimating provides a novel tool for pattern classification. In addition, the insensitivity of the fractal dimension to transformations such as rotations and translations makes it a useful tool for invariant object recognition. 

Awards won at the 2004 ISEF
Fourth Award of $500 - Mathematics - Presented by Panasonic Consumer Electronics Company
________________________________________
2004 - MA028 
FRACTAL CLASSIFICATION OF URBAN TREES
Sebastián Luis Cabrera
Escuela de Ciencias “Profesor Jorge A. Sabato”, Rojas - Buenos Aires, Argentina

The general purpose of this project was to explore the presence of fractal characteristics in trees and to obtain a simple tree classification based upon the fractal features found.<br><br>The analyzed trees were chosen among species common in the urban area of Rojas, Buenos Aires, Argentina, since both quantity and location would favor their later study. <br><br>This tree selection work includes: visual and photographic investigation, botanic categorization, preliminary detection of fractal geometry (self-similarity and scaling) by simple observation, samples selection of trees of different species, search for self-recurrent geometrical patterns, and optimum choice among equal species entities.<br><br>The analysis of the samples’ fractal features was done by modeling in two dimensions based on the self-recurrent patterns obtained. The modeling was fulfilled through the use of an informatic tool which is both easy to use and obtain, since it is part of a popular commercial packet: Microsoft Encarta Encyclopedia. The modeling quality is certified by a qualitative comparison procedure between the real image and the obtained model.<br><br>The definitive self-recurrent patterns which resulted from the modeling process were described through simple geometric features, later normalized and subsequently tabulated.<br><br>The analysis was completed by exploring the concept of fractal dimension in trees. The samples fractal dimension was obtained by processing the models using the box counting method. Finally, the trees were classified according to the analysis process results. <br><br>The outcome of this work may be used in later studies in the field of urban or agricultural forestation. 
________________________________________
2008 - MA028 
SYMMETRIC ALGEBRAS: THE NEW APPROACH
Mikhail S. Shkolnikov
Center of Mathematical Education, Saint-Petersburg, Saint-Petersburg, RUSSIA

In this project it has been proved, that the algebra is symmetric iff there can be entered the additional coalgebra structure on it, with a coproduct which would be homomorphism of bimodules, a product - a homomorphism of cobimodules, and two diagrams of symmetry would be comutative. I gave category self-dual redefinition of symmetric algebra. It has been proved that such a structure is unique in terms of some restrictions. Thanks to such a redefinition it became possible to receive a number of fundamental results. Using offered methods, it has been constructed an injective bar-resolvent for symmetric algebra. <br><br>The injective cover of regular bimodule over symmetric algebra has been found. It has been given as a narrowing of the coproduction received earlier on some set. Since that, it became possible to carry out successful study of the set of all symmetric non-degenerate bilinear forms for the fixed algebra. It has been shown that an action of the group of all reducible central elements of symmetric algebra can be entered on the set of all specified above forms and the set of all coproducts, and it has been proved, that these two sets with group action are isomorphic to the group over itself. <br><br> <br><br>I’ve managed to generalize many previously received results on Frobenius algebras, but the category self-duality does not take place any more for them; nevertheless there is a "symmetry" for them in the other sense. And I've also found the criterion of the symmetry in terms of automorphisms. 

Awards won at the 2008 ISEF
Third Award of $1,000 - Mathematical Sciences - Presented by
________________________________________
2008 - MA029 
COMBINATORIAL NULLSTELLENSATZ AND ITS APPLICATIONS
Aisha Baisalova
The Republican Specialised in Physics and Math School, Almaty, KAZAKHSTAN

The Combinatorial Nullstellensatz is a theorem on roots of a polynomial in many variables which is a multi-dimensional analogue of the following well-known algebraic fact: a number of roots of a non-zero polynomial in one variable doesn’t exceed its degree. The theorem turned out to be useful in Combinatorial Analysis. In my project I concentrated on its applications in Graph Theory. Namely, I investigated the hypothesis of existence almost regular subgraphs for a class of scattered graphs (a weak version of Berge’s hypothesis).<br><br>Along with Combinatorial Nullstellensatz I used the classical “surgical” method of Graph Theory, i.e. one of adding/deleting vertices and edges.<br><br>I obtained the following new results. <br><br>1. Let p be prime. A finite graph with average degree not less than 2(p-1) and maximum degree at most 2p-1 is p-scattered if it has no p-regular subgraph.<br><br>EXAMPLE. For p=3, a 3-scattered graph with 9 vertices was constructed. <br><br>COROLLARY. There exists a 3-scattered graph with n vertices for any n>8. <br><br>2. Let m be a non-zero integer. A graph G is almost p-regular if all vertices, except for one, say v, have degree p; if deg(v)=p+m, then we say that G has a m-defect in the vertex v.<br><br>THEOREM. Let G be a p-scattered graph and let v be a vertex of G with deg(v)<2p-1. Then G has an almost p-regular subgraph H with the vertex v such that H has a (-1)-defect in v. 
________________________________________
2006 - MA029 
THE FORCE OF MORSE: A MATHEMATICAL ANALYSIS OF THE FORCES IN A CRYSTAL
Nikhil Eshwar Chelliah
Charlottesville High School, Charlottesville, VA, U.S.

This simulation can model the forces acting upon atoms or molecules in simulated crystals. The analysis used knowledge of the shear forces from the physical experiment and determined the separation distances using the computational model. When it used the correct values for the atomic properties, including the dissociation energies, normal separation distance, and material properties of carbon atoms, it could identify the separation distances between molecules in the crystal.<br><br> The physical component consisted of an experimental setup that tested the maximum amounts of shear force resisted by rods of pencil graphite of various diameters. The graphite was held by a fixed clamp and a free moving one that was attached to counterweights that applied the force. The resisted force was a direct variation of the area of each graphite rod—F=S[pi]r^2, where F is the resisted force (N), r is the radius of each rod (mm), and S is the shear stress of 0.4 N/mm^2.<br><br> The computational component comprised the Morse potential, a mathematical model of gravity, written as a Matlab program. The nested loops computed 715,275 interactions between the atoms across the shear in a crystal, including the shear force and stress required to break the crystal. A modified version of the program used the experimental shear stress to identify the separation distance of 10.7 angstroms.<br><br> This experiment provides a practical way to evaluate atomic properties of materials without using any expensive equipment. The entire project was small, cheap, and efficient, yet it can be applied to any material. 
________________________________________
2007 - MA029 
MATHEMATICAL MODELING OF THE SPEED OF EVOLUTION IN ASEXUAL POPULATIONS
John Imbrie-Moore
Charlottesville High School, Charlottesville, VA, USA

My project involves the design of computer simulations of evolutionary biology models. The project centers on a Java program which simulates two problems: First, given mutation rate, selective advantage, and population size in an asexual population, the program computes how long is required for a single mutant to overrun a population. Second, when sequential mutations are allowed, the program determines how fast the average fitness of the population increases and how spread out the fitness levels of the population become over time.<br><br> The idea for this project arose from a paper on the speed of evolution and the maintenance of variation in asexual populations (Fisher, Desai, & Murray, 2006). The speed of evolution was well understood in situations where single mutations take over the population. There was controversy, however, on how a population evolves when multiple different mutants are active at the same time. Some postulated that the speed of evolution would be independent of N, the population size, see for example (Gerrish & Lenski, 1998). Desai, Fisher, and Murray argued otherwise. In their paper they proposed that the speed of evolution and the spread of fitnesses in the population vary logarithmically with N. They demonstrated this effect in a real-life lab setting with a vat of yeast left to evolve over several hundred generations. My simulation confirmed many of the predictions of Desai, Fisher, and Murray. I also propose a modified theory which fits my data over a broader range of parameters.<br><br> 

Awards won at the 2007 ISEF
Fourth Award of $500 - Mathematical Sciences - Presented by Acatel-Lucent
________________________________________
2009 - MA029 
APPROXIMATION OF THE SIZE OF DISTORTED SPHERICAL OBJECTS, AND A NEW ALGORITHM FOR PRECISELY ESTIMATING THE SIZE OF SPHERICAL FULLERENE MOLECULES
Jun Sup Lee
Langley High School, McLean, VA

The initial purpose of this project was to mathematically identify the relationship between the areas of a spherical cap and the base of the spherical cap, by algebraically defining the ratio of the two areas. I thought that this ratio could be applied to calculate the size of spherical objects, simply by looking at a small portion of the object.<br><br>Using the elements of algebra, geometry, trigonometry and basic integral calculus, I concluded that this areal ratio R can be defined as R=2r/(2r-h), where r is the radius of the sphere and h is the height of the spherical cap. From this ratio I was able to derive a convenient formula for calculating the volume, surface area, and the radius of a sphere from only the areas of a spherical cap and its base.<br><br>My formula proved to yield significantly less error than the conventional way of calculating the size of objects that are “roughly” spherical. My new formula, which takes into consideration that nothing in the world is truly perfectly spherical, is applicable to distorted spherical objects. The most potential application of this formula proved to be in the fields of molecular science, as I developed a new algorithm for calculating the size of spherical fullerene molecules by combining my formula and the modern technology of SPM (scanning probe microscopy). 

Awards won at the 2009 ISEF
Fourth Award of $500 - Mathematical Sciences - Presented by Intel
________________________________________
2007 - MA029 
MATHEMATICAL MODELING OF THE SPEED OF EVOLUTION IN ASEXUAL POPULATIONS
John Imbrie-Moore
Charlottesville High School, Charlottesville, VA, USA

My project involves the design of computer simulations of evolutionary biology models. The project centers on a Java program which simulates two problems: First, given mutation rate, selective advantage, and population size in an asexual population, the program computes how long is required for a single mutant to overrun a population. Second, when sequential mutations are allowed, the program determines how fast the average fitness of the population increases and how spread out the fitness levels of the population become over time.<br><br> The idea for this project arose from a paper on the speed of evolution and the maintenance of variation in asexual populations (Fisher, Desai, & Murray, 2006). The speed of evolution was well understood in situations where single mutations take over the population. There was controversy, however, on how a population evolves when multiple different mutants are active at the same time. Some postulated that the speed of evolution would be independent of N, the population size, see for example (Gerrish & Lenski, 1998). Desai, Fisher, and Murray argued otherwise. In their paper they proposed that the speed of evolution and the spread of fitnesses in the population vary logarithmically with N. They demonstrated this effect in a real-life lab setting with a vat of yeast left to evolve over several hundred generations. My simulation confirmed many of the predictions of Desai, Fisher, and Murray. I also propose a modified theory which fits my data over a broader range of parameters.<br><br> 

Awards won at the 2007 ISEF
Award of $1,000 - Mu Alpha Theta, National High School and Two-Year College Mathematics Honor Society
________________________________________
2010 - MA030 
INVOLUTIONS ON COHOMOLOGY ALGEBRAS
Andrey A. Gorshkov
Center of Mathematical Education, Saint-Petersburg, Saint-Petersburg, RUSSIA

Nowadays different cohomology groups are becoming more common in different areas of algebra, geometry and topology. For example, they are employed in the deformation theory, algebraic geometry, noncommutative geometry, representation theory, string topology and other theories. All of them also use some additional structures on suitable cohomology groups such as the cup product, the Gerstenhaber bracket and different differentials. Specifically, a natural involution on the cohomology algebra appears. It is helpful in calculations, but it does not have any formal description. <br><br>The value of the involution can be understood on the example of complex numbers and quaternions. Everyone who examines these algebras knows how important the operation of conjugation on them is. This operation satisfies the following conditions: 1*=1, x**=x and (xy)*=y*x*. The operation defined on an algebra, which satisfies these conditions, is called an involution. As with quaternions and complex numbers, it is quite useful in work.<br><br>The main purpose of this work is to give a natural definition of the involution on the cohomology algebras. We produce the general construction of the involution on the Ext-algebra of an object from an abelian category. The produced construction applies to two types of algebras – the group cohomology algebra and Hochschild cohomology algebra of a symmetric algebra. We also explore the case of defining an involution on the Hochschild cohomology algebra of a group algebra in a different way, than in case of symmetric algebra, so that it is conformed to the embedding of the group cohomology algebra into the Hochschild cohomology algebra of a group algebra. 
________________________________________
2007 - MA030 
SHORT BILLIARDS
Daniel Karoly Bezdek
Notre Dame High School, Calgary, Alberta, Canada 

Billiards form a fundamental concept in mechanics, optics and mathematics. My project is centered around periodic billiards which belong to the core part of the classical theory of billiards. The major results of my project can be summarized in four new theorems. These are partly Euclidean (Theorem 1 and 2) and partly spherical (Theorem 3 and 4). Birkhoff’s theorem (1917) guarantees the existence of periodic billiards for smooth and strictly convex billiard tables. In connection with this I prove in Theorem 1 that for a large class of smooth and strictly convex billiard tables the shortest billiard trajectory is always a 2-periodic one. This has the important corollary that any convex domain of constant width w is a translation cover for any closed curve of length 2w. This statement is of independent interest. Ernst Straus (1951) posed the following problem: Is there a(n) (interior) point of a given billiard table that can illuminate any other (interior) point? A construction due L. Penrose and R. Penrose (1958) shows that the answer is no to both questions. I extend this construction to the spherical plane in Theorem 4 which together with Theorem 2 shows that any billiard table can be illuminated by finitely many properly chosen interior points both in the Euclidean as well as in the spherical plane and one cannot hope for a better result. Finally, Theorem 3 solves the fundamental problem of billiards for bi-angles in spherical plane by analysing their periodic billiard trajectories. <br><br> <br><br><br><br> 

Awards won at the 2007 ISEF
Second Award of $500 - American Mathematical Society
________________________________________
2005 - MA030 
GRAPH ISOMORPHIC LATTICE PATHS
Paul Francis Jacobs
Good Hope School, St. Croix, United States Virgin Islands

This study is in the field of graph and lattice theory. The paper presents a direct one-to-one correspondence, isomorphism, between various classes of graphs and that of lattice paths in n-space. The properties and uses of such an isomorphism are then investigated and applied from lattice paths to graphs. Special emphasis is given to the solution of the isomorphism problem as described in graph theory. The end result of such work would be the production of a polynomial-time algorithm based upon a graph invariant that takes advantage of the correspondence between graphs and lattice paths to prove isomorphism among graphs. 

Awards won at the 2005 ISEF
Honorable Mention Award - American Mathematical Society
________________________________________
2008 - MA030 
PROBLEM OF RAMSEY THEORY
Nurlan Taiganov
High School, Ekibastuz, Pavlodar, KAZAKHSTAN

Science novelty and the bound, which was found, differs from Bcrlekamp's<br><br>degree of one; this work makes deeper investigation, which had<br><br>independence; been made in works "Problems Of Sets' Partitions",<br><br>"Theoretical And Number Properties Of Partitions Of Sets Of Integers", "Properties Of Partitions Of Sets Of Integers".<br><br>Results of work and the author formulated and proved 6 theorems, 2 lemmas,<br><br>conclusion; 1 consequence, 2 algorithms are created to find quantity<br><br>of subsets, which contain at least one fc-term arithmetic progression of any or a definite differ, bounds on these numberSj generalisation of van der Waerden's theorem, the bound on van der Waerden's value.<br><br>Areas of practical received results are of a significant contribution in the application: development of combinatorial number theory. 

Awards won at the 2008 ISEF
Honorable Mention Award - American Mathematical Society
________________________________________
2007 - MA030 
SHORT BILLIARDS
Daniel Karoly Bezdek
Notre Dame High School, Calgary, Alberta, Canada 

Billiards form a fundamental concept in mechanics, optics and mathematics. My project is centered around periodic billiards which belong to the core part of the classical theory of billiards. The major results of my project can be summarized in four new theorems. These are partly Euclidean (Theorem 1 and 2) and partly spherical (Theorem 3 and 4). Birkhoff’s theorem (1917) guarantees the existence of periodic billiards for smooth and strictly convex billiard tables. In connection with this I prove in Theorem 1 that for a large class of smooth and strictly convex billiard tables the shortest billiard trajectory is always a 2-periodic one. This has the important corollary that any convex domain of constant width w is a translation cover for any closed curve of length 2w. This statement is of independent interest. Ernst Straus (1951) posed the following problem: Is there a(n) (interior) point of a given billiard table that can illuminate any other (interior) point? A construction due L. Penrose and R. Penrose (1958) shows that the answer is no to both questions. I extend this construction to the spherical plane in Theorem 4 which together with Theorem 2 shows that any billiard table can be illuminated by finitely many properly chosen interior points both in the Euclidean as well as in the spherical plane and one cannot hope for a better result. Finally, Theorem 3 solves the fundamental problem of billiards for bi-angles in spherical plane by analysing their periodic billiard trajectories. <br><br> <br><br><br><br> 

Awards won at the 2007 ISEF
Second Award of $1,500 - Mathematical Sciences - Presented by Acatel-Lucent
________________________________________
2005 - MA030 
GRAPH ISOMORPHIC LATTICE PATHS
Paul Francis Jacobs
Good Hope School, St. Croix, United States Virgin Islands

This study is in the field of graph and lattice theory. The paper presents a direct one-to-one correspondence, isomorphism, between various classes of graphs and that of lattice paths in n-space. The properties and uses of such an isomorphism are then investigated and applied from lattice paths to graphs. Special emphasis is given to the solution of the isomorphism problem as described in graph theory. The end result of such work would be the production of a polynomial-time algorithm based upon a graph invariant that takes advantage of the correspondence between graphs and lattice paths to prove isomorphism among graphs. 

Awards won at the 2005 ISEF
Fourth Award of $500 - Mathematics - Presented by Science News
________________________________________
2005 - MA031 
MONIDS OF ENDOMORPHISMS
Alexey Yu. Shubnikov
Mathematical education centre, St. Petersburg, Russia

In this paper I’ve found effective algorithms with polynomial complexity for solving two famous algorithmic problems: von Neumann regularity and description of hard graphs.<br><br>Let A be an associative monoid ring. An element a in A is called regular (in sense of von Neumann) if there is an element x in A such that axa=a. A is called regular if every element of A is regular. Monoid of endomorphisms is called hard if there are no endomorphisms in it but identical endomorphism.<br><br>Results and methods of the present paper can be used for the similar problems in various classes of graphs, rings and semigroups.<br><br> 

Awards won at the 2005 ISEF
Third Award of $1,000 - Mathematics - Presented by Science News
________________________________________
2006 - MA031 
A NEW ALGORITHM FOR CONSTRUCTING A PRIME LIST BY TWIN PRIMES
Yujing Wang
High School of Peking University, Beijing, CHINA

It is well known that no polynomial formula can be used to generate the primes without finding any composite numbers. There is limitation in sieve of Eratosthenes, which is commonly used. In this paper, we introduce a general formula and prove that every formula attach to it can generate the same set of primes. Then we propose a new algorithm for constructing a list of primes by the twin primes. This algorithm is based on the conjecture that every prime and the square of prime can be expressed as a sum of two numbers, namely, a twin prime, and the product of two successive integers (This conjecture is proposed in the first time and is verified within 10^9). The primes are categorized into different sequences according to the twin prime M. The value of M in each sequence can be calculated in terms of M=n^2-n+N, where N is the twin prime in the preceding sequence. In this way, we set up a new relationship between the primes and the twin primes. By calculating the integers in each sequence and deleting the composite numbers among them, we can obtain a precise list of twin primes and hence the primes. Compared with the sieve of Earatosthenes, this algorithm is different in the order to find the primes and the twin primes, and allows us to construct a list of primes by a series of polynomial formulae. Furthermore, the regularity of the distribution of all the primes can be seen more apparently from a new perspective. In this paper, we also suggest a new probable way to prove the Twin Prime Conjecture. 

Awards won at the 2006 ISEF
Fourth Award of $500 - Mathematics - Presented by Lucent Technologies
________________________________________
2008 - MA031 
DESCRIPTION OF THE IDENTITY BASIS OF INDICATOR BERNSIDE SEMIGROUPS
Sergey V. Bakulin
Centre of Mathematical Education (LCME), Saint-Petersburg, RUSSIA

I study varieties of semigroups related to completely 0-simple semigroup. The main fact that is know about 0-simple semigroups, is that variety generated by all completely 0-simple semigroups has finite an identity basis. The semigroup variety defined by given a finite set of identities is the Rees-Sushkevich variety if and only if it containsnone of the indicator Bernside semigroups. But complexity of algorithm checking has a exponential complexity. I present here an algorithmic description of the theese varieties in terms of identities. When some algorithmic problems is decidable, the computational complexity of this problem becomes very important. Open problems: it`s the algorithm of regognizing Rees-Sushkevich verieties and exact varieties has a polynormal complexity.I proved that description has a polynominal complexity. Also, i showed that an algorithm of recognizing exact varieties has a polynomial complexity too. I used these results for constructing computer program which regognizing Rees-Sushkevich varieties and exact for given finite set of identites (in real time). Now, it became known that many varieties are Rees-Sushkevich varieties. For example, it`s finitely approximate varieries, variety <br><br>V=[x=x^n+1; xy=yx]. 

Awards won at the 2008 ISEF
Fourth Award of $500 - Mathematical Sciences - Presented by
________________________________________
2010 - MA031 
BARYCENTRIC COORDINATES AND THEIR APPLICATIONS
Bayram Safa Cicek
Ankara Fen Lisesi, Ankara, TURKEY

The aim of this project is to show the known features in triangle geometry in easier ways and to produce new geometric results by using Barycentric Coordinates.<br><br> The definition of Barycentric Coordinates is as follows; Let P be a given point on the plane of triangle ABC. If the area of triangle PBC, PCA and PAB are A(PBC), A(PCA), A(PAB) respectively, then Barycentric Coordinates of point P are;<br><br> P= (A(PBC).A + A(PCA).B + A(PAB).C)/A(ABC)<br><br>The main aim of this project is checking the following formulas used in triangle geometry: calculating of length and area values, proving linearity of points in the same plane, testing intersection of lines in one point and their parallelism, which are derived from using the definition of Barycentric Coordinates. <br><br> In this the project, known and derived formulas of Barycentric Coordinates which can be used for triangle geometry are used as a method. As a result, known features in triangle geometry are proven by new and easy methods. Also unknown 10 results in triangular geometry are obtained by using Barycentric Coordinates. At the end of the project, the planned result of project is ensured by obtaining useful applications of Barycentric Coordinates.<br><br> Advance applications of Barycentric Coordinates in geometry and physics can be derived. Therefore, Barycentric Coordinates which have many properties must be evaluated and work carried out on this subject must be researched. 

Awards won at the 2010 ISEF
Fourth Award of $500 - Mathematical Sciences - Presented by Intel
________________________________________
2007 - MA032 
A NOVEL APPROACH TO INTEGER FACTORIZATION
Gregory Jan Malysa
Roanoke Valley Governor's School, Roanoke Virginia, USA

The purpose of this project was to study the fundamental theorem of arithmetic and, in particular, the problem of decomposing an integer into two integral factors. This project was selected because the problem of factorization is of great importance to the field of cryptography. Multiplication is generally considered to be difficult to invert, which makes it a suitable base for several encryption schemes. Several factorization algorithms already exist, but this project was concerned with a more deterministic and a simpler solution than the GNFS or the QS.<br><br> This project finally resulted in a transformation of the problem. The fundamental challenge of the GNFS, the QS, and all other methods based on Fermat’s Method is the search for a congruence of squares. By instead representing both the number to be factored and its factors in binary, the fundamental problem has been altered to be a system of modular equations, which may or may not be more difficult to solve. Modular equations are in general impossible to solve when they relate two or more variables, but the domain restrictions associated with binary (0s and 1s only) allow for several axioms or lemmas describing equivalence or equality relationships between bits based on their arrangement.<br><br> It is suspected that it is possible to reduce the system of equations to a single unknown, which may then be arbitrarily assigned one of two values in order to test, removing the need for any source of randomness or any stochastic process within the algorithm itself.<br><br> 
________________________________________
2008 - MA032 
ANALOGUE OF THE POPOVICIU'S INEQUALITY
Artem A. Timoshenko
Murmansk Polytechnic Lyceum, Murmansk, RUSSIA

The Popoviciu’s inequality is one of the inequalities that connects arithmetical and geometrical means of the numbers : , where , , , . According to the correlation between arithmetical, geometrical and harmonic means of the numbers : , it is possible to put forward a hypothesis about the justice of a new inequality: , where , , .<br><br>1) Verification the proposed inequality for 3, 4, 5, and 6 variables;<br><br>2) Demonstration of the hypothesis for n variables.<br><br>Demonstration of the inequality for (n + 1) variables<br><br>Let’s consider the inequality (1) for (n + 1) variables, then record it in extended form, and make its equivalent adequate transformations: <br><br> <br><br> <br><br> <br><br> <br><br> . Let’s divide both sides of the inequality by the expression (provided it can be positive only), and make a substitution:<br><br> ( ). With it all, we get the following inequality: . Let’s introduce a function , defining by the left side of the inequality. Its derivative function is: . Let’s note that this derivative function is negative on the set of positive numbers provided , and positive provided . That is, point is the only minimum point of the function on the set of positive numbers. Therefore, the inequality is true; the inequality (1) is true too.<br><br><br>In progress of the research, it was proved that the inequality, analogous to the Popoviciu’s inequality: is true. 

Awards won at the 2008 ISEF
Honorable Mention Award - American Mathematical Society
________________________________________
2008 - MA033 
SHORTEST BILLIARD TRAJECTORIES
Daniel Karoly Bezdek
Notre Dame High School, Calgary, Alberta, CANADA

Billiards form a fundamental concept in mechanics and optics and have been investigated for a long time. My project studies the mathematics of billiards, in particular the fast developing and fundamentally important area of geometric billiards. My project is a continuation of my last year’s work. As such, first of all it extends the major result from last year, moreover it proves three additional new results. All this can be summarized in four new theorems. Theorem 1 states that any of the shortest generalized billiard trajectories of an arbitrary moderately fat disk-polygon is a 2-periodic one. As the family of moderately fat disk-polygons contains the family of fat disk-polygons, Theorem 1 is an extension of last year's theorem of mine on fat disk-polygons. All this is part of my general program to characterize those disk-polygons in which all the shortest generalized billiard trajectories are 2-periodic. Theorem 2 makes the first step to extend the above program to spherical plane. This is a non-trivial program to execute, mainly because the spherical plane has no vector space structure. More concretely Theorem 2 proves that any of the shortest generalized billiard trajectories in an arbitrary fat spherically convex polygon is a 2-periodic one. Theorem 3 states that in any convex body of the d-dimensional Euclidean space, all the shortest generalized billiard trajectories have period at most d+1. Finally Theorem 4 states that in Euclidean 3-space any of the shortest generalized billiard trajectories of a fat ball-polyhedron is a 2-periodic one. 
________________________________________
2009 - MA033 
DEMOGRAPHIC PROJECTIONS IN A DISCRETE TIME USING A MATHEMATICAL MODEL
Wilfredo Ortiz
Guamani Private School, Guayama, PUERTO RICO

Overpopulation is a concern for all international communities. Many analysts and mathematicians have developed very complicated models to study population changes. The question was: is it possible to construct a simple mathematical model that can predict the changes in population in the future. To answer that question a hypothesis was established. If a mathematical model is constructed by using a matrix, then it is possible to predict population changes in the future. A very simple and easy to apply model was constructed based on the Leslie Matrix, which depends on the nativity and mortality rate of the female population. A qualitative analysis will be done using the characteristic polynomial of the matrix, which obtains the eigenvalues of the matrix that predicts the development of population over a discrete time. The eigenvalues were calculated to analyze the population. Real data about plants population, human population and animals population were used to test the model. The results obtained using the constructed model were compared with population projections for plants, humans and animals obtained from research literature and the results were almost the same. Therefore it can be concluded that the hypothesis is accepted. At this moment the model will be applied to two types of populations: bacterial and dynamical population combining the model with graph theory. 
________________________________________
2006 - MA033 
ON THE EXTREME PROBLEMS
Paata Ivanishvili
The 1st Secondary School, Rustavi, Georgia Republic 

Abstract. In the present work we present some kinds of problems which are well-known in the modern mathematics. They are:<br><br>(1) The simplest problems of classical variational calculus <br><br>(2) The Boltz Problem <br><br>(3) Problems with Free Ends<br><br><br> Such kind of problems are well investigated by the Euler equation. The solution of this equation needs some supplementary conditions.<br><br> The Euler equation is, as a rule, the second order differential equation. Its solution is sometimes connected with great difficulties. Instead of the above-mentioned Euler’s equation we investigate the given problems by means of the Hölder and Minkowski’s integral inequalities, which allow one to establish for the given functionals their absolute or local minimal or maximal values and hence to write the first order differential equation which is easily solvable and provides us with the required functional for which we can achieve the extremal value.<br><br> 
________________________________________
2009 - MA034 
THE FOUR NUMBER THEOREM AND ITS APPLICATIONS
Ozan Ozdenizci
Istanbul Mef Schools, Istanbul, Istanbul, TURKEY

There are many specialised solutions to Number Theory problems involving Divisibility properties and Diophantine equations. Sometimes these solutions are not easily obtained. Can we find a solution technique for a certain class of these problems? We set out to search for an answer to this question. At the end of our studies we managed to find a common general approach for a number of the problems mentioned above.<br><br>Our project is based on a special case of Paul Erdos' Four Number Theorem. <br><br>The original theorem states:<br><br>Four Number Theorem <br><br>If a and c are positive real numbers, and if b and d are positive integers such that;<br><br> a.b = c.d<br><br>then there exists a positive real number r and positive integers s , t and u satisfying<br><br> a = r.s , b = t.u , c = r.t , d = s.u<br><br>The aims of the project are as listed below:<br><br>i. To provide a simple proof for a special case of Paul Erdos' Four Number Theorem. <br><br>ii. To apply this result in solving fundamental divisibility theorems from Number Theory. <br><br>iii. To find an original way of producing Pythagorean Triples.<br><br>iv. To produce Pythagorean Triples in an original way using the Four Number Theorem,<br><br>v. To show that our work can be applied to simplifying the solution techniques of certain Diophantine Equations.<br><br>In preparing this project we have made use of the fundamental properties of division and factorising, simple properties of arithmetic, and the Four Number Theorem. <br><br>“To what other types of problem can we apply the special case of the Four Number Theorem in order to simplify their solutions?” and “To what types of problem can we apply the general form of the Four Number Theorem in order to simplify their solutions?” are also the questions that are worthy of discussion and will be the subject of our future research in this field. 
________________________________________
2005 - MA034 
DISCREPANCY OF PLANAR PARALLELELIPIPEDAL MESHES
Valentina N. Dobrovolskaya
Advanced Education and Science Center of Moscow State Univercity, Moscow, Russia

Target setting.<br><br>The aim of my research is to get the exact formula for discrepancy of planar parallelepipedal meshes.<br><br>Scientific achievement.<br><br>We try to express the discrepancy of planar parallelepipedal meshes through the sum of fractional parts of linear function with rational coefficient. Thus the value of discrepancy of the mesh is expressed through numerators and denominators of convergent of a continued fractions of this coefficient.<br><br>Practical value.<br><br>Discrepancy of planar parallelepipedal meshes is used for approximate calculating linear integral. The result achieved allows to create one of the fastest algorithms for approximate calculating the integral.<br><br>Key concept.<br><br>Discrepancy of planar parallelepipedal meshes is one of the measures of proportional distribution of mesh points in the unit square.<br><br>Mesh is a closed set of points in the unit regular hexahedron n-space.<br><br>Analysis of results.<br><br>The achieved result allows to evaluate the value of local discrepancy during O(ln N) arithmetical operation, discrepancy of the mesh - during O(N2 ln N) arithmetical operation if we use expanded memory of correlation O(ln N) byte. The value of discrepancy of the mesh is expressed through numerators and denominators of convergent of a continued fractions of this coefficient.<br><br> 

Awards won at the 2005 ISEF
Honorable Mention Award - American Mathematical Society
________________________________________
2007 - MA034 
INFINITE PRODUCT EXPANSIONS OF THE N-TH ROOT
Ardit Kroni
Synge Street CBS, Secondary School, Dublin 8, Ireland

This project focuses on methods of obtaining rational approximations of the n-th root of a natural number using rapidly converging infinite product expansions (IPEs) of the form [1+b(1)/a(1)][1+b(2)/a(2)][1+b(3)/a(3)]… where all a’s and b’s are integers.<br><br>A “simple infinite product expansion” is defined here as one in which b(n) is constant for all n greater than or equal to 2.<br><br>New non-simple IPEs for the n-th root related to the binomial series, the Newton-Raphson, Householder and similar algorithms are obtained. Recurrence relations are proved that allow these IPEs to be defined independently of the algorithms from which they were originally derived. These recurrence relations motivate the discovery of a new algorithm that generates simple IPEs with quadratic convergence for all surds. A simple numerical measure is defined that can be used to demonstrate that this new algorithm out-performs the Newton-Raphson algorithm – without imposing extra computational cost, each successive fractional convergent achieves accuracy comparable to the Newton-Raphson algorithm but with much smaller numerator and denominator.<br><br>The IPEs related to the Newton-Raphson algorithm are generally non-simple. But, for quadratic surds, certain choices of the initial estimate generate simple IPEs. An investigation of why this occurs leads to the discovery that there are very close links between the numerators and denominators of the convergents of the Newton-Raphson, Halley and Householder algorithms, Pell’s equation and simple IPEs of quadratic surds. The connections between non-simple IPEs and generalised Pell equations are also considered. These investigations lead to several new IPE algorithms. 

Awards won at the 2007 ISEF
Honorable Mention Award - American Mathematical Society
________________________________________
2005 - MA034 
DISCREPANCY OF PLANAR PARALLELELIPIPEDAL MESHES
Valentina N. Dobrovolskaya
Advanced Education and Science Center of Moscow State Univercity, Moscow, Russia

Target setting.<br><br>The aim of my research is to get the exact formula for discrepancy of planar parallelepipedal meshes.<br><br>Scientific achievement.<br><br>We try to express the discrepancy of planar parallelepipedal meshes through the sum of fractional parts of linear function with rational coefficient. Thus the value of discrepancy of the mesh is expressed through numerators and denominators of convergent of a continued fractions of this coefficient.<br><br>Practical value.<br><br>Discrepancy of planar parallelepipedal meshes is used for approximate calculating linear integral. The result achieved allows to create one of the fastest algorithms for approximate calculating the integral.<br><br>Key concept.<br><br>Discrepancy of planar parallelepipedal meshes is one of the measures of proportional distribution of mesh points in the unit square.<br><br>Mesh is a closed set of points in the unit regular hexahedron n-space.<br><br>Analysis of results.<br><br>The achieved result allows to evaluate the value of local discrepancy during O(ln N) arithmetical operation, discrepancy of the mesh - during O(N2 ln N) arithmetical operation if we use expanded memory of correlation O(ln N) byte. The value of discrepancy of the mesh is expressed through numerators and denominators of convergent of a continued fractions of this coefficient.<br><br> 

Awards won at the 2005 ISEF
Award of $5,000 - Intel Foundation Achievement Awards
________________________________________
2007 - MA034 
INFINITE PRODUCT EXPANSIONS OF THE N-TH ROOT
Ardit Kroni
Synge Street CBS, Secondary School, Dublin 8, Ireland

This project focuses on methods of obtaining rational approximations of the n-th root of a natural number using rapidly converging infinite product expansions (IPEs) of the form [1+b(1)/a(1)][1+b(2)/a(2)][1+b(3)/a(3)]… where all a’s and b’s are integers.<br><br>A “simple infinite product expansion” is defined here as one in which b(n) is constant for all n greater than or equal to 2.<br><br>New non-simple IPEs for the n-th root related to the binomial series, the Newton-Raphson, Householder and similar algorithms are obtained. Recurrence relations are proved that allow these IPEs to be defined independently of the algorithms from which they were originally derived. These recurrence relations motivate the discovery of a new algorithm that generates simple IPEs with quadratic convergence for all surds. A simple numerical measure is defined that can be used to demonstrate that this new algorithm out-performs the Newton-Raphson algorithm – without imposing extra computational cost, each successive fractional convergent achieves accuracy comparable to the Newton-Raphson algorithm but with much smaller numerator and denominator.<br><br>The IPEs related to the Newton-Raphson algorithm are generally non-simple. But, for quadratic surds, certain choices of the initial estimate generate simple IPEs. An investigation of why this occurs leads to the discovery that there are very close links between the numerators and denominators of the convergents of the Newton-Raphson, Halley and Householder algorithms, Pell’s equation and simple IPEs of quadratic surds. The connections between non-simple IPEs and generalised Pell equations are also considered. These investigations lead to several new IPE algorithms. 

Awards won at the 2007 ISEF
Second Award of $1,500 - Mathematical Sciences - Presented by Acatel-Lucent
________________________________________
2008 - MA034 
THE BLASCHKE-LEBESGUE PROBLEM REVISITED
Mate Jozsef Bezdek
Notre Dame High School, Calgary, Alberta, CANADA

Area and volume are among the most important concepts of mathematics. They have been studied from the dawn of this branch of science, and even today continue to play a central and influential role in pure as well as applied mathematics. Perhaps the best known open problem on computing / estimating volume is the Blaschke – Lebesgue problem, which has been raised about 100 years ago. The major goal of my project is to propose a new approach towards investigating the Blaschke – Lebesgue problem, by proving a significant new extension (Theorems 1, 2) of the classical Blaschke – Lebesgue theorem in the plane, and by proving another new theorem (Theorem 3) on a generalization of this problem in Euclidean 3 - space. More concretely, in the plane, we look at disk polygons with centre parameter d, meaning that they have the property that the distances between centres of the generating disks are at most d, where d is a given real number. Then, we look for the smallest possible area within the family of disk polygons, with given centre parameter d. Depending on the value of d, Theorem 1 finds the disk polygon with the smallest area, whereas Theorem 2 finds the greatest lower bound for the area of the disk polygons investigated. We extend this concept to Euclidean 3 - space, in particular, for a specific range of the distance parameter d, Theorem 3 finds the smallest volume member of ball polyhedra with centre parameter d. 

Awards won at the 2008 ISEF
Second Award of $1,500 - Mathematical Sciences - Presented by
________________________________________
2006 - MA035 
FACTORIZATION BY USING THE DIFFERENCE OF PERFEC SQUARES
George Kartozia
Georgian-American High School, Tbilisi ,GEORGIA REPUBLIC

There are a lot of mysteries connected with the name of well-known mathematician Pierre Fermat. Our project is about one of those mysteries that were solved by Fermat. He was sent a letter with 12 digit number. Fermat solved task and proved that it was in modern language RSA number, and was a multiple of two 6 digit prime numbers.<br><br> RSA numbers are composite numbers that have exactly two prime factors (i.e. so-called semiprimes) that have been listed in the factoring challenges of RSA security and have been particularly chosen to be difficult to factor. <br><br> On most computer systems, 8 digit primes can be obtained quiet quickly. In prime number theory developed the task of finding the largest prime number. The largest currently known prime is Mersenne’s prime which was discovered in December 2005 and has 9152052 digits, but this doesn’t mean that every prime that are before that number are known. If it was like that there wouldn’t be any RSA challenges, we would find 617 digit semiprime by multiplying two primes that would give in product number with 617 digits. The largest known RSA contains 200 digits.<br><br> While RSA numbers are much smaller then the largest known primes, their factorization is significant because of the curious property of numbers that proving or disproving a number to be prime (“primality testing”) seems to be much easier then actually identifying the factors of a number (“prime factorization”).<br><br> By using modern computers we are able to find quiet large RSA numbers, but how could Fermat solve task isn’t known. In our project we give the new way of factorization which we called “Factorization by using the difference of perfect squares”. With this algorithm we can factorize number that was given to Fermat without any calculating machine. We think of using this algorithm for larger numbers. <br><br><br> <br><br> 
________________________________________
2005 - MA035 
A CLASSIFICATION OF MAPS FROM GRAPHS TO THE PLANE WITH DISJOINT IMAGES
Nikita Yu. Odnobokov
Advanced Education and Science Center of Moscow State Univercity, Moscow, Russia

Let K and L be connected graphs. A link map is a pair (f,g) of continuous maps from graphs K and L to the plane such that f(K)g(L)=.<br><br>Two link maps (f0,g0) and (f1,g1) are called equivalent if they could be joined by a continuous family (ft,gt) of link maps.<br><br>The problem of classification of the link maps was raised by J. Milnor in [1]. This problem and its generalizations (e.g. doodle theory) are widely investigated in topology and theoretical physics. Some results were obtained for the high dimensional cases [2]. The main result of the present work is classification of link maps of graphs to the plane.<br><br>Denote by V and E the numbers of vertices and edges of graph K, respectively.<br><br> Denote k=E-V+1. Analogously we denote the number l for the graph L.<br><br>Main theorem. Denote by X the set of equivalence classes of link maps of graphs K and L in the plane. Denote by Y the set of sequences of k+l integers such that either the first k integers are zeroes or the last l integers are zeroes. Then X is in 1-1 correspondence with Y.<br><br>References<br><br>[1]J. Milnor, Link groups, Ann. Math., (1957).<br><br>[2]A. Skopenkov On the generalized Massey--Rolfsen invariant for link maps, Fund. Math., 165 (2000), 1--15.<br><br> 
________________________________________
2008 - MA036 
ON LEHMER-TYPE QUESTIONS FOR SPECIAL CLASSES OF ARITHMETIC FUNCTIONS
Matthew Michael Wage
Appleton East High School, Appleton, WI

This project is motivated by Lehmer’s 1947 conjecture on the Ramanujan tau-function. The tau-function has been studied for 100 years and is central to the theory of modular forms. (The coefficients of the Fourier series of the prototypical modular form Delta are given by the tau-function.) Lehmer’s seemingly simple conjecture remains open despite amazing advances in the field, including the work of several Fields Medalists.<br><br> <br><br> Lehmer conjectured that the Ramanujan tau-function never vanishes (i.e., that tau(n) does not equal zero for all positive integers n). Generalizing this, a Lehmer style problem would be to demonstrate, for any fixed alpha, that tau(n) = alpha for at most finitely many n. In a classical paper, Murty, Murty, and Shorey proved this for any odd alpha. Here we generalize these results to a wider class of arithmetic functions – a class that includes all of the functions given by the coefficients of the Eigenforms. We also give similar results for alpha in residue classes modulo 2, 3, 5, and 7 in the case of Ramanujan's tau-function. This includes the result of Murty, Murty, and Shorey. 

Awards won at the 2008 ISEF
Third Award of $250 - American Mathematical Society
________________________________________
2005 - MA036 
AN EXPLORATION OF ARCHIMEDES' STOMACHION
Kaitlyn Tuley
Mathematics and Science High School at Clover Hill, Midlothian Virginia, USA

In the essay Stomachion, Archimedes describes a unique tangram of fourteen irregular polygons called the Stomachion puzzle. Assuming Archimedes was interested in the number of ways the pieces could be arranged into a square, it could be an ancient beginning in Combinatorics theory. Previous research has investigated the number of solutions and devised a method for interrelating most of these solutions. This paper explores additional properties of the Stomachion puzzle and its pieces, and a new method for solving the puzzle. Two topics explored are centroids and combinations of several pieces into polygons that are regular, symmetric or congruent. An interesting property of combinations of triangles and their centroids is discussed. 

Awards won at the 2005 ISEF
Scholarship Award of $10,000 per year, renewable annually - Florida Institute of Technology
________________________________________
2005 - MA037 
DURER'S UNSOLVED GEOMETRY PROBLEM
Daniel Bezdek
St. Brigid Elementary and Junior High School, Calgary, Alberta, Canada 

One of the oldest unsolved problems of mathematics (geometry and art) was raised by Dürer, circa 1500: Determine whether every polyhedron can be cut along its edges and unfolded flat in one piece without overlap, that is into a polygon. Using present day terminology: determine whether every convex polyhedron has an edge-unfolding. I give a detailed description of the problem and introduce a new family of convex polyhedra namely, the family of convex higher order deltahedra. This family is an extension of the family of convex deltahedra that have been classified by Freudental and van der Waerden in 1947. After describing some geometric properties of convex higher order deltahedra, as my major result, I show that every convex higher order deltahedron has an edge-unfolding. Also, I connect this new family of convex polyhedra to the discrete Kepler problem by solving it for the convex case. Quite surprisingly then I can turn to another important application of triangle lattices, namely to the protein folding problem. This is one of the major unsolved problems of computational biology. With the help of the family of convex higher order deltahedra, I manage to propose a new HP-type model: the folded triangular lattice model to study this problem. 

Awards won at the 2005 ISEF
Third Award of $1,000 - Mathematics - Presented by Science News
________________________________________
2010 - MA038 
ANALYSIS OF EVACUATION FROM CLOSED SPACES
Jan Hoffmann
Gymnazium Alejova 1, Kosice, SLOVAKIA

Evacuation of people in emergencies can cause a number of problems. People in panic run to exits trying to escape which can lead to injuries. The objective of this project is to find the best mathematic/physics solution to this problem which can be used in real life. Stemming from experiments with balls, people and computer simulation the author tries to discover the most suitable plan of evacuation ways. Each hypothesis has been verified three times, in computer simulation, with steel balls and with people. We reached the conclusion that the best option is to have the emergency exit not partitioned, in the middle of the wall and to have the evacuation way narrowed gradually. Thanks to this findings we have managed to lower the evacuation time by 50%. Based on the results of the experiment we have found a correlation between narrow steepness and the total evacuation time and that the best approximation of measured values is a linear function We also calculated Pearson's correlation coefficient which led us to a conclusion that the correlation between narrow steepness and the evacuation time is big and indirect, which means the steeper the narrowing, the faster the evacuation is. The outcome of the work can be helpful when designing evacuation plans for schools, hospitals, stadiums, cinemas, theatres, big shopping centers… 
________________________________________
2004 - MA038 
ON THE PROPERTIES OF JUMP POINTS IN THE GAME OF N-TIMES NIM
Brian Todd Rice
Marion Senior High School, Marion, VA, USA

The game of n-times nim (n a fixed real greater than 1) is a take-away game with two players in which each player may take from a single pile of counters any positive integer of counters no more than n times the number of counters taken by the previous player. The winner is the player to take the last counter. In seeking a winning strategy for the game, one finds that winning positions, which depend on n, have some very interesting patterns. Importantly, all winning positions for a given n are obtainable from an infinite sequence S(n) of integers, which can be itself be determined using a known recursion. In order to further study the nature of these sequences, I introduce a novel set of constants associated with the game of n-times nim: those real numbers q at which the sequence S(q) "jumps" from one sequence to another. That such "jump points" exist (in the sense that they are well defined, discrete points) is proven in this project. It is shown that all jump points are rational, and a useful condition for determining whether or not a given number is a jump point is given. Several other original theorems, regarding the existence of jump points with various fractional parts, are proven, culminating in the main result that, for any nonnegative rational number r < 1, there are infinitely many jump points with fractional part r. Ongoing research into the density of jump points is also touched on. 

Awards won at the 2004 ISEF
Second Award of $500 - American Mathematical Society
________________________________________
2005 - MA038 
A NEW LOOK AT CAYLEY’S PROBLEM INVESTIGATION OF THE CONVERGENCE OF THE NEWTON ALGORITHM USING QUATERNIONS
Elad Oster
Israel Art and Science Academy, Jerusalem, Israel 

The Newton-Raphson algorithm is an efficient tool for numerically locating roots of real and complex functions. Given an arbitrary starting point the algorithm converges to one of the roots. However, when applied to complex variable polynomials, convergence is not guaranteed. It is not possible to determine a-priori for which polynomials and for which starting points this will happen. This problem, formulated a hundred years ago by the British mathematician Cayley, is still unsolved. When applied in the complex plane, the algorithm is also a useful tool for creating fractal patterns. These fractals are generated by assigning a color to each starting point according to the root it converges to. Assigning black to those points from which the algorithm does not converge results in the appearance of ‘black holes’. I developed computer tools for analyzing the behavior of the algorithm in the complex plane, and used them to study the ‘black holes’. In addition, I investigated the behavior of the algorithm in space by formulating it in terms of quaternions. This is a novel approach to the subject and indeed led to some interesting new results. First, under certain conditions the algorithm is more efficient in space than in the plane. Second, I observed that most of the ‘black holes’ are concentrated around the Gauss plane. Starting the algorithm from a plane parallel to the Gauss plane and slightly away from it allows it to converge from almost any point. Improving the efficiency of the algorithm and characterizing its convergence has many practical applications for scientists and for industry. 

Awards won at the 2005 ISEF
Third Award of $250 - American Mathematical Society
________________________________________
2004 - MA038 
ON THE PROPERTIES OF JUMP POINTS IN THE GAME OF N-TIMES NIM
Brian Todd Rice
Marion Senior High School, Marion, VA, USA

The game of n-times nim (n a fixed real greater than 1) is a take-away game with two players in which each player may take from a single pile of counters any positive integer of counters no more than n times the number of counters taken by the previous player. The winner is the player to take the last counter. In seeking a winning strategy for the game, one finds that winning positions, which depend on n, have some very interesting patterns. Importantly, all winning positions for a given n are obtainable from an infinite sequence S(n) of integers, which can be itself be determined using a known recursion. In order to further study the nature of these sequences, I introduce a novel set of constants associated with the game of n-times nim: those real numbers q at which the sequence S(q) "jumps" from one sequence to another. That such "jump points" exist (in the sense that they are well defined, discrete points) is proven in this project. It is shown that all jump points are rational, and a useful condition for determining whether or not a given number is a jump point is given. Several other original theorems, regarding the existence of jump points with various fractional parts, are proven, culminating in the main result that, for any nonnegative rational number r < 1, there are infinitely many jump points with fractional part r. Ongoing research into the density of jump points is also touched on. 

Awards won at the 2004 ISEF
Intel ISEF Best of Category Award of $5,000 for Top First Place Winner - Mathematics - Presented by Panasonic Consumer Electronics Company
________________________________________
2005 - MA039 
UPON SPECIAL PARABOLIC FUNCTIONS
Jose Ronaldo Vazquez
Lysander Borrero Terry High School, Villalba, Puerto Rico

Considering several multiples of number eleven (11) with three digits, the researcher can create several parabolas when taking those digits as coefficients of the independent variable (x). In that way a parabolic equation can be obtained expresing those digits as coefficients. The factors of the arranged equation helped to form a parabolic equation having two kinds of zeros; the zeros of the parabola (obviously) and the “ONCENIC ZEROS” which represent the multiples of the original digits. When the researcher took both zeros, a parabola is obtained that has special conditions as the (y) intercepts. Also the researcher noted that the distance between the coordinates of those zeros tend to be the same multiple of eleven. Taking the Greatest Integer Function the researcher got the same number for all the examples analyzed. Another important aspect is noted when those multiples of eleven are less than two hundred (200). Taking the negative of the first two digits it was possible to get the “ONCENIC ZEROS” that satisfied the equation. There are other special equations formed by numbers bigger than (200) that satisfied the conditions mentioned before; but not for all those parabolic function. Furthermore, there is an important aspect that can be observed in the equation formed by the zeros of the parabola and the “ONCENIC”. It can be noticed that the numerical value of the line is the same as the (y) intercept. 
________________________________________
2010 - MA039 
THE CALCULATION METHODS FOR SUM OF K TO THE MTH POWER
Kazuki Hirakawa
Jonan, Tokushima, Tokushima, JAPAN

I made nine methods to derive summation formulas. By six methods, we can derive summation formulas of integer powers. By three methods, we can derive summation formulas of real powers. Seventh method and “Integer power calculation on paper” are very useful.<br><br> <br><br>First, Seventh method was made from my fourth method. The fourth method has Bernoulli numbers. However, using the fourth method we cannot derive summation formulas of real powers. So, I wanted to make another method. By the way, the fourth method has C function. I changed C function to a factorial to generalize it. As a result, I made a formula. It has a constant term of a Bernoulli number. So, I deleted that, and the seventh method was made. It is an approximate function. if we change value of two variable, the precision is changed.<br><br> <br><br>Second, “Integer power calculation on paper” was made using my third method. Using the third method, we can derive the term of summation formulas recursively. The third method has two C functions. They are calculated using Pascal’s triangle. I made calculation on paper from this recursive method. “Integer power calculation on paper” is very simple. So, even elementary school students will be able to derive summation formulas. Then, I improved this method using my sixth method. Last, I made a new calculation on paper to derive summation formulas of real powers.<br><br> <br><br>Moreover, I made software to derive summations formula and to draw calculations on paper. After this, I want to make better methods. 
________________________________________
2007 - MA039 
PRIME MAGIC SQUARE: POSSIBLE OR IMPOSSIBLE?
Sarah Lee Sellers
Hedgesville High School, Hedgesville, West Virginia, United States

The purpose of this experiment was to find out whether or not it was possible to create a magic square using only prime numbers and only using each number once. The experiment first involved finding a pattern or some correlation between the prime numbers and then writing those prime numbers down. Once the pattern was found, the experiment then involved choosing a starting prime number (which was called “n”) and subtracting twice "n" (or 2n) from every prime number smaller than “n”, and recording each of those prime numbers along with their difference form “n”. The last step involved making the magic square using only the primes numbers that were written down, and placing them in a pattern so that all the rows, columns, and diagonals of the prime magic square equal the same number. <br><br> The experiment confirmed the hypothesis that it is possible to create a prime magic square. It was discovered that the larger the prime number is for “n”, the more potential the prime magic square will have to be larger and the more borders it could potentially have. In this experiment the first 25 possible 3x3 prime magic squares were made. The 43x43 that was made during this experiment also makes a world record for the largest handmade prime magic square, and it also has added cryptology that spell out a single word using only palindromic prime numbers (or prime numbers that read the same forward and backward.<br><br> 

Awards won at the 2007 ISEF
Fourth Award of $500 - Mathematical Sciences - Presented by Acatel-Lucent
________________________________________
2010 - MA040 
PRIME NUMBERS AND NEW REGULARITIES
Rina Ismailati
Gjimnazi Ismail Qemali, Tirane, ALBANIA

This mathematical project is about prime numbers,it discovers new regularities in them.Except the algebraic proceedings, I’ve also built new,interesting and regular graphics for each prime number that I’ve studied.In fact,this project includes a wider specter than prime numbers,because the regularities discovered in them,are present even in some other numbers,which are made of prime numbers.For example,the nr 34.(17 x 2).The regularities discovered in the nr 17,are also present in the nr 34.I’ve studied only a few prime numbers,but this studying method can be applied to any other prime number.More details: this project has to do with the division of integers with prime numbers.The conclusion in this project is that we can find regularities in the numbers that come after the decimal point,they are not chaotic.Also,we can create graphics by linking the numbers of the periods of this divisions,with the remainders of the devisions.Each prime number has its own graphic.<br><br>For example: the number 7.Lets study: 1:7, 2:7, 3:7, 4:7, 5:7, 6:7.The period of these divisions is the same.What’s different about them is only the numbers that comes after the decimal point,so the first number of the period.We can find regularities even inside of one period,even if we put these 6 periods in a column.(these regularities are repeated in cyclic way for every 6 numbers that are divided by 7).<br><br>In the end,I've generalized some of the regularities discovered,so I've proved that they are true for all the numbers,not for only the numbers that I've studied. 
________________________________________
2010 - MA041 
PROPERTIES OF FAMILIES OF THE SIMSON LINES OF INSCRIBED POLYGONS
Yevgeniy Gruver
General Specialized Sanatorium Boarding School For Gifted Children "Erudit", Donetsk, UKRAINE

The research is devoted to the studying of properties of the Simson lines for inscribed polygons. The main aim of the research is to determine the shape of the envelope of a family of the Simson lines of an inscribed polygon corresponding points of its circumcircle. The research consists of three parts. The first part is devoted to the studying of properties of the Simson lines for quadrangle, a full criterion (if and only if) was established that the envelope of the Simson lines is an astroid. The Simson alpha-lines were defined for triangle in the second part. The shape of the envelope of a family of the Simson alpha-lines has been determined. The third part is devoted to some properties of the Simson lines for any inscribed polygons. There is hypothesis that for sufficiently large n the envelope of a family of the Simson lines for inscribed n-gon will be a correct hypocycloid with n vertices if and only if the n-gon is a correct one.<br><br>In proving the main results of the research methods of differential geometry and the mathematical analysis were used. Once the hypothesis was formulated, in order to ensure its correctness, different polygons were modeled in silico. In particular, such software products as Cinderella were used. Also, when computer analogues were done there appears a first idea how to prove an alleged fact. The result of the research can be applied in optics and mathematical physics. 
________________________________________
2007 - MA041 
CRYPTOLOGY USING THE "BUTTERFLY EQUATION"
Mark Krummelbein
Lemvig High School, Lemvig, Denmark

Personal data security is becoming an urgent topic because every day personal computers around the world get hacked. This is a major problem, because most home computers contain data about bank accounts, personal life and other sensitive information. <br><br> Today conventional systems used for data security can easily be broken by a powerful computer and often even by hand. This is because the key used for encrypting is made in a given length that is repeated again and again until the entire text is encrypted.<br><br> The main idea of this project is to avoid repeating the key. This is done by using a chaotic equation to generate an individual value for each letter in the text.<br><br> I chose the Lorentz Attractor for this task, an example of a chaos theory equation. <br><br> The values from the equation are used to generate the cipher using traditional transposition principles. The idea is to use this system for both text encryption, and encryption on a binary level.<br><br> This system can be used for personal security and data send between entrusted individuals within a group. The system is not applicable to email because the same key is used for both encrypting and decrypting.<br><br> The sensitive nature of the attractor makes it suitable for encryption, because even extremely small changes in the initial conditions of a nonlinear dynamic system will generate an entirely different result. This makes it is very difficult to crack the code and thus providing high security to the data.<br><br> 

Awards won at the 2007 ISEF
Fourth Award of $500 - Mathematical Sciences - Presented by Acatel-Lucent
________________________________________
2005 - MA041 
SINKING BATTLESHIP
Christopher Edward Tucker
Huntington High School, Huntington, West Virginia, USA

The purpose of my project is to demonstrate that mathematical-based methods could compete with humans in the game of Battleship. The object of Battleship is to locate unknown objects on an x-y plane with more efficiency than one’s opponent. Four coordinate patterns, known as the Pattern Series method, were developed to efficiently cover the x-y plane. My hypothesis is that the Pattern Series method will perform as well or better than human subjects. <br><br> To obtain data, I created four experiments to test each method in my self-written Battleship Simulator program. The methods tested were: “Pure” random, “Semi-“ random, Pattern Series, and the game play of human subjects. Ten different arrangements of five ships (unknown object locations) were utilized in each test. Results were recorded as the number of shots taken (coordinate guesses) to sink all five ships.<br><br> The human subjects performed the best overall compared to the computer-based methods. This is interpreted to be the human’s ability to recognize ship arrangement patterns and the flexibility of guessing during a game. The Pattern Series and the Semi-random methods fell short likely because of their limitations in adapting to obvious ship arrangement patterns. The pure random method proved to be completely inefficient probably because there is no recognition when a ship is hit. The other methods (except for human) employed a rigid pattern to sink the ships once they were located. While the experiments do not support my hypothesis, I am confidant the results apply beyond the game of Battleship.<br><br> 

Awards won at the 2005 ISEF
Second Award of $1,500 - U.S. Air Force
________________________________________
2008 - MA042 
EISENSTEIN PRIME MAGIC SQUARE
Sarah Lee Sellers
Hedgesville High School, Hedgesville, WV

The purpose of this experiment was to find out if a magic square could be made using only Eisenstein prime numbers. A magic square has numbers arranged in an order so that all the rows, columns, and main diagonals add up to equal the same number. An Eisenstein prime is in the form 3n-1 (let n be any integer), and if this formula creates a prime number then it an Eisenstein prime. An Eisenstein prime number was chosen at random and called n. Only the deltas that created Eisenstein primes were kept to potentially be used. <br><br> This experiment confirmed the hypothesis that it is possible to create an Eisenstein Prime Magic Square when an Eisenstein prime number is chosen for n. The experiment also led to the discovery that any odd-order prime magic square must be either an Eisenstein Prime Magic Square or a Dirichlet Prime Magic Square. It was also proven that every prime number ending in 9, would force every delta to end in 0, 2, or 8. The experiment also proved that p-1 or p+1 is a multiple of 6 when p is a prime number greater than 3. It was also proven that every entry in the deltas is a multiple of 6. <br><br> An 111x111 Eisenstein Prime Magic Square was made with 55 Eisenstein Prime Magic Squares all in one. The 111x111 Eisenstein Prime Magic Square also has a cryptic message in the top center. 

Awards won at the 2008 ISEF
Honorable Mention Award - American Mathematical Society
Third Award of $1,000 - Mathematical Sciences - Presented by
________________________________________
2009 - MA043 
INFINITE SUMS OF ZETA FUNCTIONS AND OTHER DIRICHLET SERIES
Andrei Triffo
Synge Street CBS Secondary School, Dublin, IRELAND

The aim of this project is to investigate systematically the conditions under which it is possible to obtain a closed formula for the limit of an infinite sum of zeta functions. It is shown that many new formulas involving infinite sums of zeta functions can be proved using relatively simple and elegant techniques.<br><br>New and surprising connections between infinite products and infinite sums of zeta functions are demonstrated. <br><br>Many infinite series for well-known classical constants such as Catalan’s constant converge very slowly. The existence of a new class of convergence acceleration formulas for Catalan’s constant involving infinite sums of zeta functions is demonstrated. Similar formulas for other constants can be obtained using the methods described here. <br><br>The zeta function series is a special case of a more general series known as a Dirichlet series. In a paper in the American Mathematical Monthly in December 2001, Balanzario discussed what he called “T-periodic” Dirichlet series and he proved that in certain cases it is possible to obtain a closed formula for the sum of the series. In this project methods of obtaining closed formulas for infinite sums of T-periodic Dirichlet series are investigated, leading to many new and beautiful formulas. <br><br>The evaluation of Euler-Zagier sums and Witten zeta functions has been intensively studied in recent years because they occur in quantum field theory. These series converge slowly. A new method for the evaluation of Witten zeta functions using rapidly converging infinite sums of Hurwitz zeta functions is described. 

Awards won at the 2009 ISEF
Second Award of $500 - American Mathematical Society
Second Award of $500 U.S. savings bond - Ashtavadhani Vidwan Ambati Subbaraya Chetty (AVASC) Foundation
Third Award of $1,000 - Mathematical Sciences - Presented by Intel
________________________________________
2006 - MA044 
APPLICATION OF THREE - PAGES APPROACH IN THE THEORY OF KNOTS FOR THE HYPOTHESIS
Yevgeniya Malitskaya
High School Ridder, KAZAKHSTAN

The object of the paper is to find a new invariant of a knot. The author put forward a hypothesis: if it is possible to expand the knot in three half planes and paint its three pages red, blue and black so that on every plane all three colors meet, it will mean that the given is non-trivial.<br><br>This paper is the continuation of research made in 2001-2004. Since that time the theoretical material connected with knots, topogical spaces, the theory of groups has been studied. Some Experiments have been taken researching unchangeable characteristics of the knots, and finding common regularities.<br><br>The student's own research began with the demonstration of the theorem of existing the knot a in the final half group of knots so that a2=a. This theorem is not described in any source.<br><br>In the latest paper the author put forward hypothesis of three-colored painting of a three-paged diagram. The demonstration of the hypothesis has been done by adaptation of a known theorem of three-colored paintings of a three-paged diagram.<br><br>The number of experiments have been made when a number of links regular knot diagram has been counted and certain regularity has been discovered. The student managed to find a new invariant of a knot connected with the division of the number of diagram's links by 3 and identical with the known invariant about a regular painting of a knot.<br><br>Thus, the work done by author is ultimately new and independent. The results have never been published before. They have purely theoretical meaning and can be used in the activities with students interested in abstract mathematics. 
________________________________________
2009 - MA044 
CONTROLLING HIV FROM TRANSFORMATION INTO AIDS: MATHEMATICAL MODELING OF HIV DYNAMICS
Sohini Sengupta
Ocean Lakes High School, Virginia Beach, VA

This research analyzed a differential equation model for HIV dynamics in the blood stream. The model used a system of nonlinear differential equations to represent the population of uninfected CD4+ T cells, infected CD4+ T cells, and free HIV virions. We adjusted the model to include the effect of two drugs treatments: Protease and Reverse Transcriptase inhibitors, and the efficiency value of each drug was methodically manipulated. The main goal was to see which combination of drugs produced the best results in stabilizing T cell levels and deterring the onset of AIDs. <br><br> We found that for each efficiency value, there were two possible equilibria. At or above a certain critical value for drug efficiency, the HIV afflicted system approached uninfected steady state values, mathematically “curing” HIV. At values less than this critical value, the other equilibrium point was the attractor, and infection remained endemic. The Protease inhibitor was more effective than the RT inhibitor in reducing viral load as was signified by a lower critical value. The HIV virion can be eradicated with drug effectiveness less than 80%, and an increase in number of drugs leads to a decrease in required efficiency. <br><br> This research is valuable in seeing the effect of certain drug therapies on HIV dynamics through mathematical modeling. Otherwise, it may not be viable to experimentally test how much efficiency is needed for a drug to significantly reduce virion levels in an HIV patient. For further research, the model could incorporate a more complex immune response. 

Awards won at the 2009 ISEF
Third Award of $250 - American Mathematical Society
________________________________________
2008 - MA044 
COMPUTING PERFECT NUMBERS: THE SEARCH FOR EVEN AND ODD PERFECT NUMBERS
Vanessa Rivera-Quinones
Escuela Especializada University Gardens, San Juan, PUERTO RICO

Since the beginning of number theory there have existed questions yet to be answered. The existence of odd perfect number has been one of the oldest questions in Number Theory. The problem that inspired this research was: Do even or odd perfect numbers exist in the predetermined interval of 10^ 300 and 10^ 600? If they do exist how many and which are they? The hypothesis to be tested was that there will exist one or more, even or odd, perfect numbers in the predetermined interval of 10^300 and 10^600.<br><br> This research project used the new technology to compute perfect numbers and investigated what is known about the subject. In the time period of 3 weeks, a computer code in C++ language with the Dev- C++ compiler was ran. This computer code checked each number in the chosen interval and printed the ones which were perfect. The perfect numbers found were 2^520(2^521-1) and 2^606(2^607-1). The hypothesis was true but these two numbers have been already discovered. No new ones were found for the chosen interval. Even though no odd perfect number was found, there are various reasons to explain this: maybe it doesn’t exist; the interval used is too small or the number is in the form (4n+1)^4k+1 b^2 given by Euler. Nevertheless, odd prime numbers, any power of prime numbers and the product of two prime numbers as possible odd perfect numbers were eliminated. 

Awards won at the 2008 ISEF
Fourth Award of $500 - Mathematical Sciences - Presented by
________________________________________
2006 - MA045 
CRITERIA OF REALIZATION OF BOUQUETS ON THE PLANE AND TORE
Bakhytzhan Baizhanov
High School Aktobe, West KAZAKHSTAN

We will express criterions with a help of forbidden bouquets: The bouquet unrealized it contains forbidden bouquets.<br><br>There were found forbidden bouquets for a plane. After we have used facts that if graph is realized on the plane, so it may be realized that in tore too. Then we cut down tore accordingly to combinations of rib bouquets, in the end of the result we have obtained plane which realization we must proof. There were found 4 solutions for forbidden bouquets with some help of partition version. 

Awards won at the 2006 ISEF
Honorable Mention Award - American Mathematical Society
________________________________________
2004 - MA046 
SELF-NUMBERS
Nurzhas Makishev
Specialize in Physics and Mathematics, Almaty, Kazakhstan

The given problem is from book of the remarkable American popular writer of mathematics of M.Gardner "Time travel and other mathematical biwilderments". In this book is told about self numbers, open by the Indian mathematician D.Kaprekar.We shall take any natural number n and we shall add to it the sum of the his digits S(n).The obtained nuber m=n+S(n) refers to as the generated number, and initial number the n is called a generator of number m.The self number is a natural which does not have generator. <br><br> The question of distribution of generated numbers and self-numbers is investigated in a number of natural numbers.The infinity of the set of self numbers is proved. Necessary conditions for generators of the least generated number , having r generators are found and on the basis of this result a new method of calculation of elements gr is developed. In the cases p=2,3,4,5,6,7,8,9 the first 100 numbers are investigated and elements are calculated.In the case p=10,i.e. in decimal notation,the first 120 numbers are investigated and first 62 numbers gr are found. For each r the formulae of are obtained in the cases of p=2 and p=3. The formulae of and of are found for arbitrary p, . Also, the formulae of , and of are conjectured for arbitrary p, .<br><br>All results of the project are new and can be applied in various areas of mathematics.<br><br> 

Awards won at the 2004 ISEF
Fourth Award of $500 - Mathematics - Presented by Panasonic Consumer Electronics Company
________________________________________
2008 - MA047 
MAXIMUM METRIC IN RXR
Mehmet Yilmaz
T.C Orhaneli Turkan-Sait Yilmaz Anadolu Lisesi, Bursa, TURKEY

The purpose of the project is to study whether the function d from RxR to R taking the maximum of the absolute differences of the abcisses and the ordinates of two points of RxR is a metric or not. Our projects aims also that how the shapes at the conics of this metric are.<br><br>Firstly it is proved that the given function is a metric. After that the shapes of conics, i.e. the circle, the ellipse, the hyperbola and the parabola are examined and determined.<br><br>As a result it is found out that the circle of this metric is an Euclidean square of which sides are parallel to x-y axises, the ellipse of this metric is an Euclidean octagon. Moreover it is seen that the hyperbola and the parabola of this metric have very interesting shapes different from those of Euclidean metric. 
________________________________________
2008 - MA048 
CHAOTIC GAMES AND POLYGONAL FRACTALS
Mario Jardon Santos
Escuela Secundaria Anexa a la Normal Superior, Mexico City, MEXICO

The purpose of this project is to create the family of polygonal fractals using chaotic (random) games. I started with the chaotic game related to the well known Sierpinski Triangle. First, I draw the three vertexes of a triangle in the plane, and an arbirtriary point (a seed point). Then randomly, I choose one of the three vertexes, and draw the middle point between the seed point and the chosen vertex. From this middle point I repeat the same process choosing randomly a vertex and drawing the corresponding middle point. In the same way I analized the Sierpinski Carpet. <br><br>It was discovered the new family of polygonal fractals similar to the family of Sierpinski fractals, with their corresponding chaotic games. I constructed a system of functions which define the polygonal fractals and their associated chaotic games. <br><br>I showed that the game has necessary to be chaotic, I played with an ordered game and the points conveged to a finite orbit of fixed points but not a fractal at all. Continuing with the experiments in the computer, I discovered the antifractals. They use a different process, similar to the one of the fractals I made before, but with vertical and horizontal reflections. <br><br>This project does not work only as a research and experimetation, but as a way of learning and teaching mathematics. In the process I used the Open Office spreadsheet. 
________________________________________
2007 - MA048 
RATIO OF TOTAL EDGE LENGTHS OF TWO SIMPLICES, WHERE ONE CONTAINS ANOTHER
Nikita M. Savushkin
School #1134, Moscow, Russian Federation

Statement of problem <br><br>The goal of the project was investigation of the value area of P(V2)/P(V1) – ratio of total edge lengths of n-dimensional simplices V2 and V1, where V1 contains V2, and finding of the least upper bound <br><br>Ln=sup (p(V2)/p(V1)), V2 be a member of V1<br><br> <br><br>Results<br><br>• At the given space dimension n, the least upper bound Ln of the ratio P(V1)/P(V2) does exist and is equal to f(n)/n , where f(n) – maximal number of vertices in the section of n-dimensional simplex by a hyperplane. <br><br>• f(n) =[(n+1)/2]*[(n+1)/2] , where n – dimension of the space, [x] – integral value of x. <br><br>• Some examples and implications are considered:<br><br>– at n=2, L2=1 – follows from inequality of triangle sides;<br><br> at n=3, L3=3/4<br><br>– at n=4, L4=3/2;<br><br>– at n converging infinity, Ln is also converging infinity like n/4.<br><br>Two sets of vectors are given: a1, a2, a3, … ,an and b1, b2, b3, … ,bn, where sum of projection lengths of the vectors from the first set on any line does not exceed the sum of projection lengths of the vectors from the second set on the same line. In this case the sum of vector lengths from the first set does not exceed the sum of vector lengths from the second set.<br><br> 

Awards won at the 2007 ISEF
Third Award of $250 - American Mathematical Society
________________________________________
2008 - MA049 
AN EVALUATION OF POPULATION MODELS
Maria Teresa Bezem
Bergen Katedralskole, Bergen, Hordaland, NORWAY

I have investigated four population models: exponential growth, the Schaefer model, the Lotka-Volterra competitive species model and the Lotka-Volterra prey-predator model. The investigation has for its object to fit to the cod stock in the Barents Sea during the period 1962 to 1990. My research question is: Is it possible for me to fit the four above stated population models to the cod stock in the Barents Sea during the period from 1962 to 1990? <br><br>To find this out I first tried to understand how the models represent changes in population, each model having both strengths and weaknesses. I found out that exponential growth, being on the form of a geometric sequence: u(n)=u(0)*r^n, could represent populations having percentage increase or decrease. The Schaefer model represented populations that have a natural stability. It's expressed either as the logistic function: P(t)=c/(1+a*e^(-b*t)), a geometric sequence with linear growth/decrease: u(n+1)=(m*u(n)+p)*u(n) or a differential equation: du/dx=r*u*(1-u/k). At last the two Lotka-Volterra models are each given by a pair of differential equations: dx/dt=r*x*((k-x-ay)/k) and dy/dt=s*y*((c-y-bx)/c) for the competitive species model which can show that species can out compete each other and dx/dt=x(A-By) with dy/dt=-y(C-Dx) for the prey-predator model that can represent the cyclic changes in populations that are either eaten by other species or eat other species themselves.<br><br>Afterwards I tried to fit these models to the cod stock. I didn’t manage this for the competitive species model, whereas the three others could all show the general decrease, but not the short term fluctuations. The three models, that I was able to evaluate to the cod stock, all predicted different future outcomes. Exponential growth, by being a geometric sequence predicted that the cod stock would die out. The Schaefer model predicted that the cod stock would stabilize at an equilibrium size. And the prey-predator model predicted that the cod stock will always fluctuate and therefore increase to the early sizes again. 
________________________________________
2006 - MA049 
EFFICIENT PARAMETERISATION OF THE BINOMIAL OPTION-PRICING MODEL
Gohar Jehanzeb Abbasi
Synge Street CBS, Secondary School, Dublin 8, Ireland.

The global market for options is enormous. Contracts worth many trillions of dollars are traded every day.<br><br>The Black-Scholes option-pricing formula (1973) stimulated a rapid expansion of options markets. The binomial option-pricing model (1979) allows the pricing of American options on dividend-paying stocks.<br><br>Yet, in spite of the success of these and other models, research on option-pricing models continues because all current models have defects, for example, failure to deal with volatility smiles, transaction costs and illiquid markets.<br><br>This project describes algorithms that allow the parameters of the binomial option-pricing model to be inferred from the observed market prices of actively traded options, thereby dispensing with the assumption that the price process of the underlying is geometric Brownian motion.<br><br>The algorithms were programmed in C++. The user interface is simple and flexible; it allows rapid, accurate calculation of the model parameters and prediction of bid and ask prices. The algorithms were tested using the S&P 100 index option (American) and the S&P 500 index option (European). Previous research that attempted to extract model parameters from observed prices in the options market focused almost exclusively on European options.<br><br>The project proposes a novel approach to the difficult problem of incorporating transaction costs by inferring an “effective dividend yield” on the underlying from observed option prices in the market. This parameter is different for bid and ask prices.<br><br>The parameterisation technique described here augments the great strengths of the binomial option-pricing model, namely its simplicity and wide range of application. <br><br> 
________________________________________
2007 - MA049 
ON REALIZATION OF GRAPHS ON THE KLEIN BOTTLE
Alexey S. Telishev
Secondary school #77, Naberezhnye Cheilny, Russian Federation 

Problems of realizability (i.e. representability without self-intersections) of graphs (or graphs with an additional structure) on the plane, the Klein bottle (and other surfaces) are one of the most important in the topological graph theory and its applications [KPS, LZ, MT, P, Po, Sk', ST] (in particular to the dynamical systems theory [BFM]). In the present paper we give a criterion for realizability of graphs with an additional structure on the Klein bottle.<br><br>A half-edge is a pair (v, e), where e is an edge issuing out of vertex v.<br><br>A sketch is a graph with<br><br> for any vertex, an orientable cyclic ordering of half-edges issuing out of the vertex, and<br><br> a disposition of zeroes and units on its edges.<br><br>A sketch is called realizable on the Klein bottle if the sketch could be drawn with out self-intersections in the Klein bottle, and small oriented circles around could be added so that the following properties hold:<br><br> for any vertex the cyclic ordering of edges in the oriented circle around this vertex coincides with the fixed oriented cyclic ordering.<br><br> an edge contains zero if and only if the orientations of the circles around the ends of the edge agree along this edge.<br><br>Main theorem.<br><br>Let from each vertex of a conneted sketch issues out three edges. This sketch is realizable on the Klein bottle if and only if it doe snot contain subsketches homeomorphic to one of 10 sketches defined in the paper.<br><br> 

Awards won at the 2007 ISEF
Honorable Mention Award - American Mathematical Society
________________________________________
2007 - MA050 
INVERSE PROBLEMS OF TRIANGLE GEOMETRY
Valeriya V. Petkieva
Advanced Education and Science Center of MSU, Moscow, Russian Federation

In the paper, the following new generalized inverse problems of triangle and tetrahedron geometry are considered:<br><br>To find the sets of all the points in the plane (in space) that can be vertices of triangles (tetrahedrons) such that two given points are the pair of specified remarkable points of the triangle (or tetrahedron).<br><br>We consider the following pairs of points (where we denote by G the centroid, by H the orthocenter, by I the incenter, and by Hm the antiorthocenter):<br><br>(A) G and H (in the plane for triangles and in space for orthocentric tetrahedrons);<br><br>(B) G and I (in the plane for triangles)<br><br>(C) G and Hm (in the plane for triangles)<br><br>The main tool in this research is the theorems about Euler, Nagel, and Lemoine.<br><br>In all the cases mentioned above, compass-and-ruler constructions of the triangles and tetrahedrons in question are found.<br><br>Keywords: Incenter, centroid, orthocenter, antiorthocenter, Euler line, Nagel line, Lemoine line.<br><br> 
________________________________________
2010 - MA051 
AN INNOVATIVE INSTRUMENT FOR 'N'-SECTING AN ANGLE PERFORMS CLASSICAL CONSTRUCTIONS AND GOES BEYONG TO PLOT N-TH ROOT OF UNITY
Abhiroop Lahiri
South Point High School, Kolkata, West Bengal, INDIA

An innovative instrument has been designed, which can construct certain numbers in addition to those constructible by straightedge and compass, thus going beyond classical constructibility. <br><br><br>It consists of a disc, and an adjustable rod is attached to the centre of the disc. The other end of the rod is fixed to a needle such that it can move in all direction pivoted at a point. The disc is inclined to the vertical at a certain angle so that when it is rotated on its axis, the centre of the disc also traverses a circular path with the point of attachment of the rod, as the centre (a gyroscope). <br><br><br>An instrument has also been designed for plotting sine-cosine curves, and tan-cot curves, and another one for plotting sinh and cosh (sine and cosine hyperbolic) functions. <br><br><br>Out of all these instruments designed by me the one for n-secting an angle has opened up possibilities for construction of numbers which were never constructed earlier. <br><br> <br><br><br>It can 'n'-sect an angle, i.e.,it can not only trisect an angle, but also it can construct any surd multiple of an angle. It can construct some irrational non-quadratic points, and thus can locate 'n'th roots of unity in the Argand Plane. Hence a new family of numbers can be obtained, which are non-constructible by compass and straightedge. <br><br><br>Using certain steps of transformations (specified rotations), it can also do every construction (along with straightedge) which a compass and straightedge can do. Thus it can add,substract,multiply,divide and obtain square root and hence it is capable of constructing every number of the surd field. 
________________________________________
2008 - MA052 
MATH GAME, PRACTICE WHAT YOU LEARN WHILE PLAYING
Mounir Tarek Nasser
Dhahran Ahliyyah School, Dhahran, SAUDI ARABIA

Math Game project is about creating educational tool for children to practice the main mathematical operations (addition, subtraction, multiplication and division).<br><br>The Math Game is a two player board game that uses checkers like ground with numbers from 1 to 100. Each player has a piece to hold position on the board.<br><br>The Math Game consists of two levels. First Level "Start Spinning". The level requires that one of the players spins a wheel to select the math operation that will be used during the game. The rest of the players draw number cards and perform the math operation on them to move their playing pegs. This is useful to train the mind on one operation repeatedly. Second level "Strategy" allows one player to spin the wheel while the other choose two cards to enforce the first player to use them and perform the math operation. This adds excitement and requires thinking how to strategically choose cards to make it difficult for your opponent to win.<br><br>The Math Game is built on the following principles:<br><br>- Math skills are better developed with repetition. Players only move their pieces when the result of basic math operations on their cards are larger than the number they are currently holding by their pieces<br><br>- Timed games encourage fast prompt response. Each player is allowed certain time to complete all four math basic operations on the two cards withdrawn by him or loses turn to the other player<br><br>Math Game opts to provide a fun environment to help kids practice their mathematical skills and sharpen their quick computing capabilities. 
________________________________________
2010 - MA052 
CONSTRUCTING INVERSE LIMITS WITH UPPER SEMI-CONTINUOUS FUNCTIONS
Yong Zhi Zhou
Pinetree Secondary School, Coquitlam, BC, CANADA

In Continuum Theory, every continuous function f from X:[0,1] to Y:[0,1] has an inverse limit. The inverse limit of a function is the union of all dimensions of f (f^1, f^2, ...) embedded in an infinite dimension cube. The inverse limit of any continuous function is connected, planar (can be compressed to a plane), and irreducible (does not contain a circle or a branching point). Inverse limits of upper semi-continuous (usc) functions (multi-valued maps), however, do not necessarily reflect these properties. The goal of this project was to find counterexamples to these properties and to give conditions that these properties will and will not apply in inverse limits of usc functions. Specifically, I proved several key necessary and sufficient conditions for connected inverse limits, and generalized methods for constructing totally disconnected inverse limits. I also developed conditions and counterexamples to reducible inverse limits. These results will shed light on the construction of new topological structures. 

Awards won at the 2010 ISEF
Third Award of $1,000 - Mathematical Sciences - Presented by Intel
________________________________________
2010 - MA053 
SIX LINES PASSING THROUGH FEUERBACH POINT
Fedor A. Ivlev
Kolmogorov's Spesialized Educational Scientific Center of MSU, Moscow, n/a, RUSSIA

Given a triangle ABC. Let’s consider for it two cevian triangles of arbitrary points in the plane. Denote by A', B', C' the intersection points of the respective sides of these cevian triangles. The triangle A'B'C' is called “pole triangle”.<br><br> The purpose of the investigation was to study some properties of pole triangle respective to Gergonne point and mass center of triangle. <br><br> The working procedure is absolutely geometrical.<br><br>We consider a triangle T such that its orthic triangle is the median triangle of the initial triangle. Next properties (probably unknown) of the triangle T are proved. 1) Its vertices lie on the sides of the pole triangle under consideration. 2) The midpoints of its sides, the touching points of the incircle with the sides of initial triangle and the vertices of the mentioned pole triangle lie on certain three lines.<br><br>The next Lemma was proved: given two triangles such that: 1) vertices are lie on the sides of the triangle; 2) Their vertices are symmetric with respect to midpoints of the triangle sides. Then the perspectors of both pairs “triangle - its pole triangle” are congruent.<br><br> Next result was proved: three lines passing through vertices of this pole triangle and the respecting vertices of Gergonne triangle pass through Feuerbach point. The Lemma allows to obtain this result also if in condition we substitute Gergonne triangle into Nagel triangle that is the main result of the work. 

Awards won at the 2010 ISEF
Fourth Award of $500 - Mathematical Sciences - Presented by Intel
________________________________________
2010 - MA054 
HYPERBOLIC TRIANGLES OF THE MAXIMUM AREA AND TWO FIXED SIDES
Evgeniia Iskanderovna Alekseeva
GOU Lyceum "Vtoraiia shkola", Moscow, RUSSIA

The aim of the work is to consider the Lobachevskii geometry analogue of a well-known Euclidian problem; namely: to construct and describe a triangle with two sides of given length and the maximum area. We shall call it the maximum area triangle.<br><br> This problem is much more difficult and interesting than its analogue in Euclidian geometry, because in Lobachevskii geometry the area of a triangle depends on its angles and methods of Euclidian solution can not be applied to solve the Lobachevskii geometry problem.<br><br> To construct the maximum area triangle we will consider Poincare disk model of Lobachevskii geometry. In this model the area of a triangle can be expressed only through one angle. The area of a triangle is maximum if and only if this angle is maximum. Using this proposition we can construct the maximum area triangle with the help of compass and ruler.<br><br> The main result of the work is constructing the maximum area triangle and studying its properties. It appears that a lot of famous properties of Euclidian right-angled triangle have analogues for maximum area triangle in Lobachevskii geometry. For example, the centre of the circumcircle coincides with the middle of the triangle side, and there exists an analogue of the well-known Pythagorean theorem. So it is a maximum area triangle that should be considered analogous with a Euclidian right-angled triangle.<br><br> The maximum area triangle is fundamental and can be used to solve such difficult problems in Lobachevskii geometry as isoperimetric problem and Zenodorus problem. 

Awards won at the 2010 ISEF
Third Award of $250 - American Mathematical Society
Third Award of $1,000 - Mathematical Sciences - Presented by Intel
________________________________________
2006 - MA301 
ABOUT ONE-DIMENSIONAL COMPACT TOPOLOGICAL SEMIGROUPS
Alexey Dmitriev, Anton Kurbanov, Vadim Raskumandrin
Centre of Mathematical Education, Saint-Petersburg, Russia

Is it possible to build the operation above any segment of material line, which is rational, continuous and associative? In other words, to build examples of one-dimensional compact topological semigroups. Semigroup is topological if the operation in it is determined above topological by space and is continuous. Topological semigroup is called compact, if the topological space above which semigroup is built is compact. Compact topological groups and semigroups were studied by many well-known mathematicians; however, they examined the objects above the multidimensional spaces, while there were not nontrivial examples of groups and semigroups above the one-dimensional spaces. It is not difficult to devise the rational operation, which is locked in the section. But it is entirely not trivial, if we require that the operation should be associative. We succeeded in building such examples. <br><br> As a result an infinite series of one-dimensional compact topological semigroups were built for the first time, their properties were studied; moreover this series were nontrivial, because among the given examples of semigroups there are nonisomorphic. And, which not is unimportant, it was proven that above any segment of material line it is possible to build the semigroup, in which the operation is rational and continuous.<br><br> 
________________________________________
2010 - MA301 
MATHEMATICAL MODELING IN ANALYSIS OF FARMING SYSTEMS OF BROILER CHICKEN
Sergio Luiz Back, Edson Granemann dos Passos, 
Instituto Federal de Educacao, Ciencia e Tecnologia Catarinense, Rio do Sul, Santa Catarina, BRASIL

The broiler chicken production has always been searching for higher yield in shorter time. To achieve that farmers need to have a better control over the chicken stocks and modeling can help in doing that. This work aimed to analyze through mathematical modeling the zoothecnical results of poultry housed in semi-automatic and automatic sheds observing the Indexes of Efficiency and Productivity. Another aim was to find a mathematical model capable to show if the chicken could be taken to production at a younger age. Data concerning to weight, ration intake and mortality were collected at school during one year. It was possible to established curves of adjustment resulting in mathematical models relating the chicken’s weight and their age. Three models were used in this approach, one using statistic method and two with software methods. The polinomial model provided local adjustments to the data while the logistic model was more efficient for the analyzes of animal growth. It was established a correlation between the age and the weight of the animals througt the coefficient of determination and it was observed through mathematical modeling that the animals wich were set in the authomatic aviary had better performance allowing analyzis and projection of future growth rates. 
________________________________________
2003 - MA301 
GENERALIZATION OF THE KURATOVSKY'S PROBLEM
Evgeny A. Amosov, Artem G. Viktorov
Continuous Math Education Center, St. Petersburg, Russia

Abstract is not transferable to PDF. Abstract on file, too many special characters. 03/26/03 JI<br><br>The known problem of the Polish mathematician Kuratovsky considers properties of the operations: int- taking interior and cl – closing and asserts that by means of these operations it is possible to obtain only 7 different sets from A set (subject to A set). For instance, at A={0}U [1.2)U (2,3)U ((4,5) Q)) and X = R, all these sets are different. Hence only 7 sets are possible. Let’s do this problem if it is allowed to apply the operation conv (A) – taking of convex hull of A set. We have demonstrated the following theorems:<br><br>THEOREM 1: For arbitrary A cl(conv(cl(A))) = cl(conv(A)).<br><br>THEOREM 2: If B is a convex set with nonvacuous interior, U is open and B U 0 then there is an open set W at which WCU B.<br><br>DEFINITION: Let’s say that B interiorly everywhere dense in V, if int(B) is everywhere dense in V or in other words for any open U at which U V is not empty then int(U V) is not empty.<br><br>Conclusion 1. If B is a convex set with nonvacuous interior then B is interiorly everywhere dense in cl(B).<br><br>THEOREM 3: If B is convex and interiorly everywhere dense in open U then U B.<br><br>THEOREM 4: If B is a convex set with nonvacuous interior then int(cl(B))=int(B).<br><br>THEOREM 5: If B is a convex set with nonvacuous interior then cl(int(B))=cl(B).<br><br>Conclusion 2. If int(conv(A)) 0 then int(conv(A))=int(conv(cl(A))).<br><br>Following these theorems and conclusions we have disclosed that if int(conv(A)) 0 then only the 17 sets can be different.<br><br>For each case we demonstrate examples for which this assertion is correct.<br><br> 

Awards won at the 2003 ISEF
Honorable Mention Awards - American Mathematical Society
Second Award of $1,500 - Team Projects - Presented by Science News
________________________________________
2004 - MA301 
THE SUFFICIENT CONDITIONS FOR RIESZ BASES
Danila Alexandrovich Fandeev, Daniil Victorovich Hritoshin, Stepan Alexandrovich Ismakaev
Center of Mathematical Education, St.Petersburg, Russia

Let S be at most countable set of subscripts; B – Banach space. – system of vectors in space. H – Hilbert space. In this work 8 sufficient conditions related to Riesz bases are established.<br><br>Following theorems represent the most important of them.<br><br>THEOREM 2. If system of vectors is topologically free and series with , then system of vectors is Riesz basis.<br><br>THEOREM 3. If system of vectors satisfies the following three conditions: (i) – is topologically free; (ii) – is complete; (iii) there exists such p > 1 that for any , the next estimate is correct: , where A(p) is small enough, and , then is Riesz basis in Hilbert space H.<br><br>THEOREM 4. If system of vectors X of Hilbert space H is complete and there exists such , that the series is convergent and , then system of vectors X can be turned into Riesz basis by throwing out only finite number of elements.<br><br> 

Awards won at the 2004 ISEF
Fourth Award of $500 - Team Projects - Presented by Science News
________________________________________
2007 - MA302 
REPRESENTATION OF MODULAR LATTICES
Alexander Yurievitch Neshitov, Kirill Olegovitch Batalkin
Centre of Mathematical Education, Saint-Petersburg, Russia

Our work deals with the lattices theory. The basic fact , that is known about distributive lattices,<br><br>is that every distributive lattice can be put in subsets lattice of some set.<br><br>Taking into account this fact it’s evident that every finite distributive lattice can be put in submodule<br><br>lattice of module H (where H is prime order cyclic group in n grade). Some wider class of lattices was investigated in this work.<br><br>We have generalized these results for finite modular lattices. It was proved that every modular lattice can be put in submodule lattice of module H. Submodule lattice of module H is modular. So every sublattice of this lattice is modular.<br><br>Theorem: The finite lattice is modular if and only if it can be put in submodule lattice of module H.<br><br>Modular lattices have been chosen as the subject of inquiry, because the majority of lattices naturally arising (normal subgroup lattices of groups, submodules lattices of modules, subspace lattices of vector spaces, etc...) are modular. <br><br>Thus, results, achieved in this work allow to investigate modular lattices as sublattices of well-researched submodule lattice of module H. <br><br> Also we have found algorithm of inductive construction of subgroup lattice of every finite abelian group by representation them as composition of primary abelian groups.<br><br> According to this algorithm ,we can construct submodule lattices of any H.<br><br>Construction of submodule lattice of H coupled with Theorem about modular lattices give fully characterization of all finite modular lattices .<br><br> 
________________________________________
2010 - MA302 
SPECIAL TRANSFORMATIONS OF VECTORS AND MATRICES
Yauhen S. Babakhin, Aliaksandr Yeutukhovich, 
State educational institution "Gymnasia #13", Minsk, BELARUS

Consider a set of n-vectors, all elements of which are natural numbers, with the following operations: 1) the subtraction of n-vectors of a special kind p=(b, …,b)’, b is a natural number (if it doesn’t go above the limit of this set); 2) element-wise multiplication by the vector of a special kind m(i)=(1, … 1,a,1 …1)’a is a natural number, i is a number of position where a is located. Natural numbers a (a>1) and b are fixed. If we use the above set of operations and can get a vector e from a vector d and vice versa, we mean that the vector d is an equivalent to the vector e (d ~ e). Thus, the above set is divided into equivalence classes. The main problem is to determine the number of equivalence classes with the specific values of a and b.<br><br>It was determined:<br><br>- A number of equivalence classes, when b=1, a and n are any natural numbers.<br><br>- A number of equivalence classes for vectors when n=2 and a=cb+1, c is a natural number.<br><br>The original problem was generalized to the case of m×n dimension matrices. The main idea of the solution is that we reduce the problem for the matrices to the problem for the vectors. It was determined that all matrices are equivalent when a=2; b=1.<br><br>There are algorithms that allow transforming any vector (matrix) to the elementary representative of a corresponding equivalence class. <br><br>This research work is theoretically oriented and the received results can be applied in the sphere of linear algebra, algebra of structures and the theory of economic systems. 
________________________________________
2004 - MA302 
THE M - CONFIGURATION OF A TRIANGLE
Svetlana Yu. Arifulina, Anton I. Babich
Lyceum #1511 at Moscow Engineering Physical Institute, Moscow, Russia

Our report consists of two parts.<br><br>In the first part we study the new configuration associated with a triangle and called the M - Configuration. It includes points Aa, Ba, Ca on the sides of the triangle ABC such that the figure M pathing BCaAaBaC consists of four segments of equal length. <br><br>In the second part we take the point P inside the triangle and an angle ö. The triangle A'B'C' is the cevian triangle of the point P. Then we construct two isosceles triangles BAbA' and A'AcC with the base angle ö (pointing in the same direction) and the bases BA' and A'C. We take the line A'C. Lines BcBa and CaCb are constructed similarly. We study the triangle XYZ that encloses three lines AbAc, BcBa and CaCb.<br><br>We used the method of homogeneous barycentric coordinates and some classical geometrical theorems such as Euler line, Euler circle, Cheva and Menelay theorems. We also used some transformations of a plane as the isogonal and the isotomic conjugate.<br><br>In the first configuration we found many interesting and new theorems such as:<br><br>1) the construction of the M - Configuration by a ruler and a compass;<br><br>2) the relation to the Euler line and circle;<br><br>3) the 'generator' of new triangle centers. <br><br>In the second configuration we proved that:<br><br>1) the triangle XYZ is perspective to triangle ABC at the point Q and we also found coordinates of perspective;<br><br>2) for the fixed point P and the variable angle ö the point Q traverses a conic passing through A, B, C. If the point P is the orthocenter of the triangle ABC then this conic is the Kiepert hyperbola.<br><br> 
________________________________________
2006 - MA302 
APPLICATIONS OF MODULAR CRYPTO-FUNCTION IN THE SECRET CODE THEORY PHASE 2
Kevin Joel Rodriguez-Torres, Artemio J. Lopez-Gonzalez
Carme Belen Veiga High school,Juana Diaz, Puerto Rico

Cryptography is the method to encrypt messages or secret information. It had been progressing since the Braille code. Braille was a person that cannot see and even he had a code that was the first one created in the world. Then it was followed by the Morse code that was the second best code invented in the world. Codes had been the best invention for the security of the nations. Therefore, the investigator decided to do this investigation to keep creating a complicated code type that can be unbreakable. This year main porpouses is to prove the created theorems logically, to create a code more complicated and unbreakable and investigate for all the formulas and prime numbers that can be used for the function: F(x) =Rx (Px/p), where Rx is the residual of the polynomial Px and p it is a prime number. Also producing code systems replacing Px for an expression in the way a^x, where to it is a prime whose the last digit number it is not one and it is different from p and introduce it in a computer program. The obtain results of proved the theorems logically and use a computer program to encrypt. It was prove that it is truly unbreakable. For the future, it is expected to implant the code in a cryptographic system more unbreakable in the world, being the best advance system, unbreakable and superior that exists in mankind. 
________________________________________
2003 - MA302 
MULTIFRACTAL DIMENSION FUNCTIONS OF RN AND QP SPACE SUBSETS
Evgeniy E. Loharu, Sergey O. Ivanov
Continous Math. Education Center of St. Petersburg, St. Peterburg, Russia

Set density functions known earlier, such as porosity functions, Lebesgue density function provide a poor account of the struture of fractal sets. Thus, porosity functions are identically equal to zero for dense sets, and Lebesgue density function equals zero on sets of zero Lebesgue measure. The paper introduces a new concept – a concept of multifractal dimension function. If X is a metric space and A subset X, then the multifractal dimension function of A set is a function f:X->[0, dim H (X)], which correlates to every point of X space a Hausdorff dimension of A set in this point.The main results of the paper are the theorems characterizing multifractal dimension functions on Rn and Qp. Let X is a closed subspace Rn or Qp, E(f<c)={x in X | f(x)<c}, E(f>c)={x in X | f(x)>c}.<br><br> THE THEOREM. The function f:X->[0, dim H (X)] is a multifractal dimension function of A subset X set only if for any c in (0, dim H (X)) the following two conditions are fulfilled: 1)E(f>c) set is open in X; 2)For any x in E(f>c), dim H (E(f>c), x)>c.<br><br> An original method is suggested in proving the theorems, which allows a transfer of results from Rn space. The obtained results are central in this sphere. <br><br> 

Awards won at the 2003 ISEF
Third Place Awards of $250 - American Mathematical Society
________________________________________
2005 - MA302 
CLASSIFICATION OF RATIONAL ASSOCIATIVE OPERATIONS
Vladimir N. Trubnikov, Oleg V. Mikhaylovsky, Mikhail A. Ptichkin
Centre of mathematical education, Saint-Petersburg, Russia 

In our research project all formal rational associative operations above any field were found. We have made classification for all these operations above real number field and complex number field accurate within isomorphism. Then there were researched main properties of all such operations.<br><br>Our conclusions are of great significance for construction of new associative algebraic objects. Thus, the results of our investigations can be applied to creation of new associative systems’ series (for example rings) with operations which have some of main properties.<br><br>Our results essentially exceed the known results.<br><br> 

Awards won at the 2005 ISEF
Honorable Mention Award - American Mathematical Society
________________________________________
2003 - MA302 
MULTIFRACTAL DIMENSION FUNCTIONS OF RN AND QP SPACE SUBSETS
Evgeniy E. Loharu, Sergey O. Ivanov
Continous Math. Education Center of St. Petersburg, St. Peterburg, Russia

Set density functions known earlier, such as porosity functions, Lebesgue density function provide a poor account of the struture of fractal sets. Thus, porosity functions are identically equal to zero for dense sets, and Lebesgue density function equals zero on sets of zero Lebesgue measure. The paper introduces a new concept – a concept of multifractal dimension function. If X is a metric space and A subset X, then the multifractal dimension function of A set is a function f:X->[0, dim H (X)], which correlates to every point of X space a Hausdorff dimension of A set in this point.The main results of the paper are the theorems characterizing multifractal dimension functions on Rn and Qp. Let X is a closed subspace Rn or Qp, E(f<c)={x in X | f(x)<c}, E(f>c)={x in X | f(x)>c}.<br><br> THE THEOREM. The function f:X->[0, dim H (X)] is a multifractal dimension function of A subset X set only if for any c in (0, dim H (X)) the following two conditions are fulfilled: 1)E(f>c) set is open in X; 2)For any x in E(f>c), dim H (E(f>c), x)>c.<br><br> An original method is suggested in proving the theorems, which allows a transfer of results from Rn space. The obtained results are central in this sphere. <br><br> 

Awards won at the 2003 ISEF
Second Award of $1,500 - Team Projects - Presented by Science News
________________________________________
2009 - MA302 
COHOMOLOGIES OF THE DIHEDRAL GROUP OVER ARBITRARY RING
Andrey A. Gorshkov, Igor A. Pechko, 
Center of Mathematical Education, Saint-Petersburg, RUSSIA

In classical homology algebra the definitions of the homology and cohomology of the dihedral group first were given only over the integer ring. However, as it turned out, the same definition could be given over the arbitrary commutative ring. Nowadays the methods of homology algebra are so highly developed, so it is not embarrassing to use new definition for developing abstract homology algebra. But there are a few surprising effects from classical point of view. To understand this effects it may be very useful and important to have a larger amount of groups with calculated cohomologies.<br><br>In our work we built the Wall-Hamada resolution generalized for arbitrary ring. Then we presented it as a totalization of a bicomplex. After that we used the contravariance Hom-functor corresponding the trivial RD_n-module and finally calculate cohomologies of totalization of obtained bicomplex. They turned out to be the cohomology groups D_n (with coefficients in the trivial module).<br><br>Our results allow to make different hypotheses about behavior of a cohomologies of a certain definite group concerning of the examined ring. There is one very interesting hypothesis, that the fact about cohomologies`s dependence of a group only on integer`s arithmetic in the ring may be true for any finite group. 

Awards won at the 2009 ISEF
Fourth Award of $500 - Team Projects - Presented by Science News
________________________________________
2004 - MA303 
ON ONE AFFINE PROPERTY OF QUADRANGLES
Alexander S. Gurtyakov, Roman S. Sunteev, Sergey G. Sheshukov
Lyceum #2, Volgograd, Russia

Let ABCD be a convex quadrangle and E, F, G and H the midpoints of BC, CD, DA and AB respectively. Suppose that K, L, M and N are the intersection points of AE and BF, BF and CG, CG and DH, DH and AE respectively.<br><br>Denote by q the ratio of areas of ABCD and KLMN. This ratio is an affine invariant. The set of all quadrangles is partitioned into classes of quadrangles with equal values of q. Note that the value of q for a given quadrangle depends on an order in which its vertices are labeled.<br><br>We proved that all possible values of q form the half-segment [5; 6). <br><br>Assume that q = 5. We can omit the convexity of ABCD for this case. We proved that the corresponding class consists of such quadrangles ABCD that the quadrangle KLMN, obtained as above, is in fact a trapezoid or a parallelogram. Moreover, ABCD is a parallelogram if and only if KLMN is also a parallelogram (if it is not, then ABCD has no parallel sides). The opposite is also true, i.e. when KLMN has at least one pair of parallel sides then we get q = 5 for ABCD. <br><br>For a given triangle ABC we found the geometric locus of points D such that the quadrangle ABCD has q = 5.<br><br>Some results are generalized for self-crossing quadrangles.<br><br> 
________________________________________
2009 - MA303 
THE TWIN PRIME NUMBERS FROM THE ARBITRARY TOWARD INFINITY
Christopher J. Lebron-Vazquez, Juan C. Bianchi-Rivera, 
Ines Maria Mendoza De Munoz Marin, Cabo Rojo, PUERTO RICO

Twin Prime numbers have been studied since the sixteenth century until the present time. A prime number is a number that has as factors number one and itself. For example, number 3 has as factors the number 3 and one. Sometimes when you add 2 to some prime numbers, they generate a new prime number where P_1 and P_1 + 2 are prime numbers. These groups of prime numbers are called prime twins, such as numbers 11 and 13. In 1849 the French mathematician Polignac established the problem of proving if twin prime numbers reached to infinity. Many mathematicians have studied these numbers. Some of the academicians are Bouvelle, Fermat and Goldbach. Charles Bouvelle affirmed that: 6n-1,6n+1; produces consecutive prime numbers. Fermat’s equation was 2^(P-1) -1= PX which was used to verify if a number is prime. Goldbach, on the contrary, reached the conclusion that all even numbers higher than 4 can be represented by the sum of 2 prime numbers, 2x=P_1+P_2 ; 2x>4. <br><br>Our investigation is based on the study of the theories and equations of Fermat, Bouvelle and Goldbach, with the goal of finding patterns and relationships that can help us understand and show that twin pairs are infinitive. <br><br>We are studying numbers 2^x since we could observe that they can be represented by the sum of two prime numbers, where they each have twin pairs, 2^x=TP_1 + TP_2 where TP_2 > TP_1, and TP_2 is a new twin prime number.<br><br>P_1 means "P sub 1"<br><br>P_2 means "P sub 2"<br><br>TP_1 means "TP sub 1"<br><br>TP_2 means "TP sub 2" 
________________________________________
2006 - MA303 
RANDOMNESS, ARE WE CAPABLE?
Henoc Eduardo Rodriguez Rodriguez, Ivan Castro Nogueras
CROEM, Mayaguez, Puerto Rico

Abstract:<br><br> The research began with the following question: are human beings capable of generating random number sequences in a community and individually? If the answer was yes, their randomness had to be questioned as well as the possibility of a difference between the sequences generated by women and the ones generated by men.<br><br> Once those questions were made, the opinion was that humans were not capable of generating random number sequences because of a predisposition to generate some numbers more than others. The reason was quite simple. Every day life influences the individual with what he or she listens and observes. <br><br> By working on an established process, the investigators collected sequences and applied statistic methods such as the chi-square technique to evaluate them. Finally, the result of such an investigation is that the hypothesis was right. Even so, there is a chance of generating random numbers with some sort of restrictions in the process.<br><br><br> 
________________________________________
2010 - MA303 
BALLOT PROBLEM APPROACHED FROM N-DIMENSIONAL PATHS
Justin Tony Hou, Te-Wei Hsu, 
Kaohsiung Municipal Kaohsiung Senior High School, Kaohsiung City, CHINESE TAIPEI

Suppose n candidates A(1),...,A(n) are running for office and a fixed number of votes are casted. In how many ways that throughout the counting of votes, A(i) is never ahead of the candidate A(j) if i > j?<br><br> The closed formulae for this very hard and famous enumerative problem (called the Generalized Ballot Problem) are known only for n<6 and remain open for others. Recently a bijection between Motzkin paths and the ballot sequences with three candidates is established. Based on this bijection, we propose in this project the notion of "n-dimensional Motzkin paths". This notion will eventually establish the bijection between the higher-dimensional Motzkin paths and the ballot sequences with n candidates for any n. We are happy to announce that in this project the complete proof of the bijective relations has been given for n = 4, 5, 6. <br><br> At this stage of investigation, we have strong evidence that the method for attacking the cases n = 4 and 6 can be extended to solving all even cases. A slight modification of the above method will also lead to solving all odd cases. 

Awards won at the 2010 ISEF
Third Award of $1,000 - Team Projects - Presented by Intel
________________________________________
2009 - MA304 
CIRCUMSCRIBED AND INSCRIBED SPHERES IN THE FIVE PLATONIC SOLIDS
Luis Yediel Negron, Nydia Lopez De Leon, 
Francisco Morales High School, Naranjito, PUERTO RICO

This research is based on the study of the geometry of the regular convex polyhedrons. The purpose was to identify the relationship between the areas and the volumes of a circumscribed and inscribed sphere in each of these polyhedrons (Tetrahedron, Hexahedron, Octahedron, Dodecahedron and Icosahedron) also known as platonic solids. The research question was: what would occur with the ratios between the surface areas and between the volumes of the circumscribed and the inscribed sphere in a platonic solid when the number of the polyhedron faces increases? To find the radius of the spheres, the triangles’ properties and their theorems just as the golden proportion were applied. The areas and volumes were found using the respective formulas and the ratios between them were calculated.<br><br>After graphing and analyzing the data, the results revealed that when the polyhedron faces are of the same geometric form, the tendency of the ratio is to decrease. However, it was found that there is a direct relationship between the amount of the polyhedron edges and the areas and the volumes ratios of both spheres. These results could help in the spatial visualization as well as developing sufficient graph skills for the analysis of edification and architectonic constructions.<br><br>In a future research the compactness of the polyhedral structures will be studied through the cutting of its vertexes and the beveled of its edges. Based on these properties, it is proposed to design an algorithm to generate interspherical polyhedral structures. 
________________________________________
2008 - MA304 
THE ULAM'S SPIRAL
Keilyn M Vale-Lassalle, Yeilyn M. Vale-Lassalle, 
CROEM High School, Mayaguez, PUERTO RICO

The purpose of this investigation is to find evidence of any existing patterns in the Spiral of Ulam. The hypothesis states that a pattern was found in the spiral and a formula to explain it. It was discovered that by dividing the spiral horizontally, forming two quadrants A and B we found a pattern uniting the numbers diagonally. In the quadrant A the formula (2n-1)(4)= f being n the number of line and f gives the result of the differential that exist between one line and another uniting the numbers diagonally. The quantity of numbers that unites in this pattern found is given by the formula 2n-1= k being n the number of the file and k the quantity of numbers that are united. In the quadrant B the formula 2n(4)= f being n the number of the line and f the difference that exist between one line and another. The quantity of numbers that are united is given by a formula 2n =k being n the number of lines and k the quantity of numbers that are united. Both quadrants form pyramids that are constantly expanding. These patterns are related with the chart of 4 and 2. With these results the hypothesis is made true. For a future projection we will investigate if the spiral of Ulam has relationships with other charts. 
________________________________________
2006 - MA304 
SOME PROBLEMS IN STEINER MINIMAL TREES THEORY
Vitaly A. Agafonov, Lyubov V. Tupikina, Aleksandra S. Serkina
Lyceum #1511, Moscow, Russia

A shortest connected union of curves joining a given set M of terminal points in an ambient space is called a Steiner minimal tree (SMT) for M. <br><br>The aim of the present work is to investigate SMT in the plane for sets consisting of a small number of points. <br><br> Theorem 1: Assume that the vertex set of a 4-gon P can be spanned by two distinct non-degenerated locally minimal trees. Then P is convex. Moreover, the lengths of these trees are equal to each other if and only if the diagonals of P are perpendicular.<br><br>One of the main difficulties in SMT-theory is that there are very many possibilities for an SMT structure. If we change a terminal set M, then the structure of SMT for M also can change. Let we be given with (n-1) points m¬1… m¬n-1 in the plane, and let us consider all the terminal sets containing these points and one additional point A. This n-points set is denoted by M(A). We define a border as a locus of the point A such that M(A) can be spanned by SMTs of different structures. An area is a connected locus of the point such that only one SMT exists on M(A). A map for points m¬1… m¬n-1 is the system of the corresponding borders and areas. Two maps are said to be equivalent if there exists a one-to-one correspondence between the areas and the borders, which preserves the structures of SMT and the adjoining relation.<br><br> Theorem 2: For maps for 3 points, there exists exactly 3 classes of equivalence. These classes can be described in terms of the largest angle of the triangle constructed on these 3 points.<br><br>Thereby in the present work there was found the number of distinct SMTs on a given 4 terminals. The obtained results can be used for investigation of stability of networks’ types and for generalization of the algorithm of constructing of the shortest network for n points. 
________________________________________
2010 - MA304 
PRIME NUMBERS STUDY USING COMPUTATIONAL METHODS
Khaled Mahmoud Al-Hamad, Mahmoud Mansour, 
The Jubilee School, Amman, Amman, JORDAN

Prime numbers have intrigued curious thinkers for centuries, and this research project will focus on studying various properties of primes and continue some unfinished researches on this field. It’s really important to study the prime numbers because they have many practical applications in our life, like in cryptography, and computer security.<br><br>Computational methods were used to accomplish the study; three independent programs were made, the first two studied primes’ various aspects, like distribution, density, behavior, frequency, infiniteness along with many others, while the third program was focused on studying and proving the GoldBach conjecture. After that data was collected from each of the three programs, and an insight look at prime related books was taken. And finally the data was organized and an independent research on the result of each of the three programs was written.<br><br>In accord to the Prime Spiral software, the prime numbers tends to favor some diagonal lines than others, the research proved that each of these lines is infinite, and that these lines keep appearing even with a different midpoint. Also the Prime Spiral software can be used to draw any other types of numbers (not just primes), like perfect, triangular, square, abundant along with many others. The Prime Line software, showed that primes are infinite and that the increase in primes tends to become more linear as the drawn interval gets larger. The Goldbach Conjecture Simulator software managed to give a graphical proof that this conjecture is true(Results are more detailed in the research paper)<br><br>Finally, this research demonstrates what already existed, it adds to the previous, yet it finds out the new, and each of the three programs could be further investigated to gain more results. 
________________________________________
2005 - MA304 
SUB-SERIES SUMS OF ABSOLUTELY CONVERGENT SERIES
Stepan Ismakaev, Fedor Mikhaylovskiy
Center of Mathematical Education, Saint-Petersburg, Russia

Sub-series and series-permutations studying is a classic problem of mathematical analysis. Well-known Riemann's theorem affirms: for conditionally convergent series it is possible to succeed in getting convergation of serious-permutation to any earlier determined real number (even to infinity) by means of permutation of their elements. The problem of sub-series set structure of absolutely convergent series was investigated in our project completely. 
________________________________________
2004 - MA304 
TRIGONOMETRIC IDENTITIES AT THE INTERSECTION OF GEOMETRY, ALGEBRA, NUMBER THEORY, AND RECURSION
Tzu-Hao Kuo, Yu-Lin Li
Pingtung Seinor High School, Pingtung City, Taiwan, R.O.C.

In this project nine families and trigonometric identities involving the arctangent functions are discovered. This exciting event takes place at the intersection of geometry, algebra, number theory, and recursion. Initiated by the visual experiments with the grid paper under the dynamic geometry environment of the Geometer's Sketchpad, the special cases are further generalized with the method of mathematical induction aided by the spreadsheet computation. <br><br>The identities are expressed in terms of the Fibonacci numbers and the Lucas numbers. Their generalization involves quadratic recursion, while overlapping grids offer proofs without words. The raw power of the computer (programmed in C++ and Visual Basic) is used in the last part to generate a pattern of integral solutions of one crucial equation. The regularity in this pattern leads to further investigation of Diophantine equations.<br><br> 

Awards won at the 2004 ISEF
Second Award of $1,500 - Team Projects - Presented by Science News
________________________________________
2003 - MA304 
THE FLY PROBLEM IN METRIC SPACES.
Maksim M. Sinkevich, Yauhen Yeuseyenka
Gymnasia-School, Osypovichi, Mogilev reg., BELARUS

Let V be a metric space which is partitioned, E be its bounded subspace which consists of a finite number of parts. For each part Ei_ a nonempty set SE_i_ of neighboring parts is defined at that doesn’t contain Ei_. Suppose that some parts from the complement of E are also neighboring parts for some Ei_. We shall call them bounding. Let a arrow from a space A to a space B be the pair (A, B). About B we will say “direction”. For each Ei_ define an arrow ti_ with a direction from SE_i_. By a cycle of an arrow ti_ it is called such an ordered gathering of the arrows(ti_,...) of a part Ei_ that the set of their directions coincides with SE_i_ and non direction occurs twice. By movement of an arrow ti_ we shall call the cyclic sequence with the ti_’s cycle.<br><br>Starting with arbitrary part of the space E a FLY moves by arrows of the parts Ei_. Moreover, after any movement from any part Ei_ by any arrow ti_ the next arrow in the ti_’s movement associates with Ei_(i.e. the arrow ti_ rotates). <br><br>Denote by M(E) the maximum number of movements which the fly can do inside E (there are no arrows outside E). <br><br>The main results of the project are:<br><br>– M(E) is always finite;<br><br>– M(E)= 26 for a 3x3 square and M(E)=20 when E is a 3x3x3 triangle;<br><br>– M(E)≤110 for a 3x3x3 cube and M(E)≤42 for a tetrahedron;<br><br>– M(E) – upper bound for a cube surface;<br><br>– M(E) – upper bound for a nxn chessboard.<br><br>There were also considered some cases of co-rotations of the arrows.<br><br> 

Awards won at the 2003 ISEF
Fourth Award of $500 - Team Projects - Presented by Science News
________________________________________
2010 - MA305 
MATHEMATICAL STUDY OF THE OVERFLOWING OF THE RIVER PO
Andrea Labo, An-Phi Nguyen, Giovanni Battista Zanaboni
Liceo Scientifico Lorenzo Respighi, Piacenza, Piacenza, ITALY

Rivers are an important natural resource, but unluckily they can also be the cause of disasters.<br><br>Computer simulations ate becoming more and more important for predicting the effects of flood and to design suitable protection structures (such as dams, levees, etc...).<br><br>The aim of this project is to perform simulations concerning the river Po: in particular a portion of the river near Piacenza has been considered.<br><br>Different flood scenarios have been produced considering various incoming discharges corresponding to normal and extreme conditions.<br><br>In order to perform the above mentioned simulations an open-source Matlab© based code (Mod_FreeSurf2D) has been employed; the code has also been modified to obtain more realistic results.<br><br>The results of the simulations have been also compared to some historical data collected during the 2000 flood: the results are in good agreement. <br><br>The methodology adopted for the simulations of the river Po can be easily applied to other rivers: as an example a study concerning a portion of Mississippi river has also been carried out. <br><br>All the results show that these kinds of computer models are an essential and reliable tool for the forecasting and management of natural rivers. 
________________________________________
2004 - MA305 
SOCIAL NETWORKS: THE G^2 MODEL
Enrique Rene Colon -Baco, Yanick Moregola
Colegio San Ignacio, San Juan, Puerto Rico, United States

In this investigation, we studied three models for social networks. These models were the Random Graphs Model, the Fully Ordered Model, and the Small-World Model. After studying the previous social models we developed the G2 model. We then discussed the Six Degrees of Separation Problem in the light of our investigations and the development of our new model.<br><br> The Random Graphs Model consists in pure randomness. This model had some benefits, but had also big deficiencies. The Fully Ordered Model consists in connecting the points to its nearest neighbor. This model, like the Random Model, was not able to model the social world. Then we have the Small-World Model which combines the Random Graphs Model and the Fully Ordered Model. Of the previous models this is the one that best resembles our social world.<br><br> The G2 Model has characteristics of the Small-World Model that make it useful, and elements we added: embedding the graph in a plane, geometry and power laws. This are reasons why we consider our model very useful for social networks.<br><br> Later, the Six Degrees of Separation Problem was discussed. This problem states that any person around the globe can be connected to any other person within only six degrees of separation. This has been a phenomenon that many sociologists and mathematicians have studied. The models presented have been used to try to understand the Six Degree Problem. We consider that our model, the G2, is a new and efficient way to explain this phenomenon.<br><br> 
________________________________________
2005 - MA305 
ON STRUCTURES DETERMINED BY CONFIGURATION IN R^N
Manuel Rivera, Carlos Fonseca
Colegio San Ignacio de Loyola, San Juan, Puerto Rico

It is our contention that the topological properties of R^n, the n-dimensional Euclidian space, determine certain specific configurations upon random infinite subsets. That is to say, random infinite sets produce certain degree of order when they are “placed” in certain topological structures. This idea of order imposing itself upon chaos, so fundamental to Ramsey Theory, has proven to be quite helpful. However, few studies have dealt precisely with this theme. In this project we will pursue this difficult task with the desire to expand our comprehension of the topological invariants of random configurations. In general our general question might be written as following: Given a triple (R^n, S, A) ; where R^n is the n-dimensional Euclidian space; S a infinite set of elements of this space, and A a function or aspect; does there exists a S' determined by certain elements of S such that the value of the function A either tends to 0, to infinity, or it is bounded by a constant c? In doing so, we shall generalize and study some of the ideas of Erdos’s, Pach’s , and other researcher’s theorems on repeated structures in discrete configurations and on fundamental aspects of finite combinatorial geometry.<br><br> 

Awards won at the 2005 ISEF
Third Award of $250 - American Mathematical Society
________________________________________
2003 - MA305 
INVESTIGATING GEOMETRY OF GOTHIC WINDOW LOBES
Lucie Lammelova, Jana Svobodova
Grammer School, Ricany, The Czech Republic

There are many Gothic buildings around the world with some of their common elements such as fascinating Gothic window lobes. The geometry of window lobes became the subject of our research. No pictures or plans have persisted through centuries. That makes any renovation a really tough task. At the beginning we formulated the basic problem: does a common geometry of window lobes exist and if yes, which theorems or formulas does it use?As a first step of our research we had to collect data, plans and photographs of various window lobes. A detailed analysis of different elements of window lobes followed:1.Inscribe various elements into a circle k of radius r.<br><br>2.Circumscribe a circle around smaller circles of radius r.<br><br>3.Inscribe various elements into a Gothic arch.Having studied various documents we found that a network of circles may be used as a basis for Gothic traceries. Some arcs of the circles in the network then form the traceries. We were trying to keep a consistent strategy with methods known already to middle-age architects.We have designed a novel efficient approach. To build the geometry of Gothic window lobes on certain tasks about circles, known as Appolonius tasks, like identical transform, analogy, coplanar elements satisfying some conditions given in advance. With this method we have constructed some selected traceries of Gothic buildings including cathedrals of St. Barbora’s in Kutná Hora, St. Vitus in Prague and St. Patrick in New York.This research could be useful not only for us, but can be very interesting also for a science-oriented public namely when renovating some Gothic monuments. Our work can be used at high schools to motivate students in practical geometry. <br><br><br> 

Awards won at the 2003 ISEF
First Award of $1,000 - Eastman Kodak Company
________________________________________
2009 - MA305 
ON MACMAHON THEOREM ABOUT INDICES OF PERMUTATIONS
Nazerke Bakytzhan, Altynay Jumadildayeva, 
Zhautykov Republican Specialized Physics-Mathematical Secondary boarding school for gifted students, Almaty city, KAZAKHSTAN

MacMahon in 1913 has proved that major-indices and inversion-indices of permutations are equally distributed over a set of all permutations. We generalize this result. We prove that major codes and inversion codes are equally distributed also over a set of permutations with given right-maximal records. This result will not be true if one changes right-maximal records to other kinds of records. 

Awards won at the 2009 ISEF
Third Award of $1,000 - Team Projects - Presented by Science News
________________________________________
2005 - MA305 
ON STRUCTURES DETERMINED BY CONFIGURATION IN R^N
Manuel Rivera, Carlos Fonseca
Colegio San Ignacio de Loyola, San Juan, Puerto Rico

It is our contention that the topological properties of R^n, the n-dimensional Euclidian space, determine certain specific configurations upon random infinite subsets. That is to say, random infinite sets produce certain degree of order when they are “placed” in certain topological structures. This idea of order imposing itself upon chaos, so fundamental to Ramsey Theory, has proven to be quite helpful. However, few studies have dealt precisely with this theme. In this project we will pursue this difficult task with the desire to expand our comprehension of the topological invariants of random configurations. In general our general question might be written as following: Given a triple (R^n, S, A) ; where R^n is the n-dimensional Euclidian space; S a infinite set of elements of this space, and A a function or aspect; does there exists a S' determined by certain elements of S such that the value of the function A either tends to 0, to infinity, or it is bounded by a constant c? In doing so, we shall generalize and study some of the ideas of Erdos’s, Pach’s , and other researcher’s theorems on repeated structures in discrete configurations and on fundamental aspects of finite combinatorial geometry.<br><br> 

Awards won at the 2005 ISEF
Fourth Award of $500 - Team Projects - Presented by Ricoh
________________________________________
2008 - MA305 
RELATIONSHIP BETWEEN THE CONSTANTS A AND N IN SOME POLAR EQUATIONS
Liann Marie Marquez, Nydia Lopez, 
Francisco Morales High School, Naranjito, PUERTO RICO

This research analyzes the relationship of constants a and n in some polar equations. The problems were: How constants a and n affects the polar curves r=acos(n*THETA) and r=asen(n*THETA)? How is n related with the number of long and small petals and with their position in the graphs r=1+2cos(n*THETA) and r=1+2sen(n*THETA)? The hypotheses were: If a and n are integers, 1<n<11 and 0<a<6, the numbers of petals of polar curves r=asen(n*THETA) and r=acos(n*THETA) will be 2n and n the length of the petals. If n is an integer number, 0<n<21, in the polar curves r=1+2cos(n*THETA) and r=1+2sen(n*THETA), n will represent the number of all petals of the polar curves. To proof the hypotheses “The Geometer’s Sketchpad” graphic device was used. Results demonstrate that in the polar equations r=1+2cos(n*THETA) and r=1+2sen(n*THETA) the value of n is the number of petals. When n is an even number the smaller petals are between the long petals. If n is an odd number the smaller petals are inside the longer petals. In the polar curves r=asen(n*THETA) and r=acos(n*THETA), if n is an odd number there will be n petals; if n is an even number, the petals will be 2n. The constant a in polar equations; r=asen(n*THETA) and r=acos(n*THETA), will be the longitude of the petal. These results supported the hypotheses. Future research includes sailing boats competitions with different sails, where polar graphics measure the boat’s speed in different angles in respect to the changes in the wind’s speed. This will analyze their performance. 

Awards won at the 2008 ISEF
Fourth Award of $500 - Team Projects - Presented by Science News
________________________________________
2006 - MA306 
A NOVAL TECHNIQUE TO FIND CUBE ROOT
Amardeep , Vakeel Ahmad
Jawahar Navodaya Vidyalaya, Sardhana , Meerut , U.P. (INDIA)

To find out the cube root of integers and rational numbers the proper process is required to reach to the accuracy.<br><br>The new technique to find out the cube root has been discovered named as <br><br>“The classical division method”<br><br>This technique is very simple and accurate, enriches our understanding. This process gives us enjoyment and easily we can meet different challenges in real life. By this process we get an opportunity to use mathematics as a tool to tackle the different problems. In higher classes a number of process are there to find out the cube root like by using log tables, by differential calculus. Out of all these process this technique is very easy and easily understandable like to find square root by division method.<br><br>The impotent aspect of this technique is that, we can find out the cube root of integers, rational numbers of large number even in decimal form without using log tables. This process will help not only to students especially up to class x but also to the teachers.<br><br>This will be very effective and useful, as we can find cube root of a perfect cube within a few seconds.<br><br> 
________________________________________
2004 - MA306 
THE POSSIBLE REFORESTATION OF A LANDFILL
Luis Armando Otero-Figueroa, Pierre Alexander Crespo-llamas
Academia Ntra. Sra. de la Providencia, San Juan, Puerto Rico

The purpose of this project is to demonstrate that a forest can be built in an area that has been exposed to contamination for many years, a landfill, and even get positive results for the enviroment and the air.<br><br>Using a big plastic box, to be able to see through it, we put a substantial amount of waste material in it to simulate a landfill. Over this waste material, we placed four layers of cover enhancements. First, a plastic liner cover layer, then, a clay cover layer of approximated two (2) inches thick, over the clay, a layer of beach sand of approximated three (3) inches thick and then another layer of plastic liner cover over the beach sand layer. Now over the last layer we created a surface of fertile soil to be planted. Finally, we sowed fiber plants in the fertile soil.<br><br>After a month of observation, the fiber plants were growing normally. The roots were spreading in a horizontal pattern without reaching the other cover layers.<br><br>Thanks to the recollected data we concluded that a landfill can be reforested. With the different layers of cover enhancement: the plastic liner, the clay, and the beach sand, the waste materials contaminants were isolated. Also, with the procedure we proposed the work done by the landfill workers won't be damaged, and the air pollution problem will be improve. 
________________________________________
2007 - MA306 
PALINDROME RELATIONSHIPS: A NOVEL EQUATION
Jaime Antonio Morales, Lilibeth Cruz, Jonathan Charriez 
Francisco Morales High School, Naranjito, Puerto Rico

Palindromes are numbers that are read the same way from left to right and right to left. This investigation presents a study about the pattern showed by palindrome numbers which have a zero in the middle of them. When divided by other palindromes with two identical digits equivalent to the digits at the end of the original palindrome, the quotient shows a particular pattern. The pattern observed is a constant number equivalent to 91. A new equation to calculate palindrome numbers was established using this constant number. The equation is P = 91n, where P is a palindrome number, 91 is a constant number and n is equivalent to a palindrome number with two or more identical digits. When this equation is solved, the results are true if n is a palindrome with a minimum of two identical digits. In some cases, for example, with the numbers 242, 363, 464, the equation is also true. However, the equation does not work with other palindrome numbers that are not identical. The results of this investigation validate the hypothesis that the product of palindrome numbers with two or more identical digits multiplied by 91 is a new palindrome number. <br><br><br><br> 
________________________________________
2010 - MA306 
C-MEC PROGRAM
Angel X. Garcia-Colon, Omar L. Serrano, 
Ramon Power y Giralt, Las Piedras, PUERTO RICO

This research aims to create a program which can help consumers to save material and money spent in a concrete structure. After researching on building specifically raised the following question: Is there a program that helps the consumer to estimate materials and construction costs in particular? To answer this was made the following assumptions: Develop a program using a work sheet application to help compute the materials used in concrete construction and estimating the cost of materials. For the methodology of this research I find information about construction schedules, formulas and volume as the formulas of volume we reflect the approximate amount of materials needed for construction particularly simple. This program was designed in a platform that used a work sheet. It uses the formulas for volume to find the approximate amount of materials to use. The program is named C-MEC program which means: A program that finds and calculates the amount of materials and construction costs in projects of cement. This program is useful because helps people to save money. In conclusion by using C-MEC program consumers will benefit as they will have important information before requesting a quotation for your building in particular. 
________________________________________
2005 - MA306 
REFLEXIVE FOUR (4)
Daniel Diaz, Andres, Millan
C.R.O.E.M. High School, Mayaguez, Puerto Rico

In this investigation the reflexive four (4) numbers were studied. The reflexive four (4) numbers are natural numbers that when multiplied by four (4) the result has the same digits but inverted. Example: <br><br> X X X ...X<br><br> 1 2 3 n<br><br> <br><br> *4<br><br> ____________<br><br> X ... X X X<br><br> n 3 2 1<br><br> <br><br>The investigators proved that this characteristic exists in some natural numbers of four digits and more. They proved this by demonstrating that the first reflexive four (4) numbers that exists has 4 digits and that there are infinitely many numbers of this type obtained by cleverly manipulating and adding digits in the original reflexive four (4) numbers. The investigators studied many of the properties of the reflexive four (4) numbers and demonstrated that they were mathematically true or false. Many concrete examples of reflexive four (4) numbers are given.<br><br> 
________________________________________
2004 - MA306 
THE POSSIBLE REFORESTATION OF A LANDFILL
Luis Armando Otero-Figueroa, Pierre Alexander Crespo-llamas
Academia Ntra. Sra. de la Providencia, San Juan, Puerto Rico

The purpose of this project is to demonstrate that a forest can be built in an area that has been exposed to contamination for many years, a landfill, and even get positive results for the enviroment and the air.<br><br>Using a big plastic box, to be able to see through it, we put a substantial amount of waste material in it to simulate a landfill. Over this waste material, we placed four layers of cover enhancements. First, a plastic liner cover layer, then, a clay cover layer of approximated two (2) inches thick, over the clay, a layer of beach sand of approximated three (3) inches thick and then another layer of plastic liner cover over the beach sand layer. Now over the last layer we created a surface of fertile soil to be planted. Finally, we sowed fiber plants in the fertile soil.<br><br>After a month of observation, the fiber plants were growing normally. The roots were spreading in a horizontal pattern without reaching the other cover layers.<br><br>Thanks to the recollected data we concluded that a landfill can be reforested. With the different layers of cover enhancement: the plastic liner, the clay, and the beach sand, the waste materials contaminants were isolated. Also, with the procedure we proposed the work done by the landfill workers won't be damaged, and the air pollution problem will be improve. 
________________________________________
2004 - MA306 
PATTERNS IN GRAPH DECOMPOSITION FOR THE MN FAMILY
Fabiana Ortiz-Figueroa, Lia Marie Ortiz-Toro 
Carmen Belen Veiga, Juana Diaz, Puerto Rico

A graph is composed by vertices and edges. Vertices are mathematical objects that contain information. Edges are connections between these vertices. In this investigation, various objectives are considered: 1) to find patterns in graph decomposition, 2) to establish relations between prime numbers and pattern changes, 3) to establish the relationship between modules and routes that alter the original pattern. A family of graphs Mn was created and studied. The established hypothesis was that if a pattern in graph decomposition exists for graphs from the Mn family where n is a multiple of 4, then using prime numbers and modules, it will be determined which and how many routes will alter the original pattern. In the case of prime numbers, when these are relative primes, there will be no alteration to the original pattern. Using modules, a formula was created to obtain which routes will alter the original pattern. Various results concerning the Mn family were proved. In particular, if a graph has hamiltonian decomposition without alteration in the original pattern, then the number of vertices is not congruent to 0 (mod n + 1), jumping n. When the number of vertices is congruent to 0 (mod. n +1), jumping n and we jump (2m+1)(n+1)-1, for m¡Ý0, then its pattern will also be altered. Further investigations will be concentrated in the relationship between the prime factorization of the number of vertices and the quantity of routes that change. <br><br> 
________________________________________
2005 - MA307 
APPLICATION OF MODULAR CRYPTO-FUNCTION IN SECRET CODE THEORY.
Kevin Joel Rodriguez-Torres, Artemio Jose, Lopez- Gonzalez
Carmen Belen Veiga High School, Juana Diaz Puerto Rico

<br><br>Cryptography has a long and fascinating history. The most striking development in the history of cryptography came in 1976 when Diffie and Hellman published New Directions in Cryptography. This paper introduced the revolutionary concept of public-key -cryptography and also provided a new and ingenious method. Crypto- graphical methods provide securing electronic commerce for many financial institutions around the world. The search for new public-key schemes, improvements to existing cryptographic mechanisms, and proofs of security continues at a rapid pace. Security products are being developed to address the security needs of an information intensive society. To create a secret code based on prime numbers and basic functions can be useful in all systems and places that use codes. In consequence, creating a codification system based on a polynomial function and prime numbers will be useful in the elaboration of a secret code. We develop a basic code that is useful, advantageous and indecipherable, generated by the function f(x)= Rx(Px/p),where Rx is the remainder of the division of the polynomial Px by the number p, a prime or a product of two prime numbers. Furthermore, we produce a codification system by replacing Px for any expression of the form a^x where a is a big prime number and we show its utility, advantages and its indecipherability. If each prime number has a unique polynomial or exponential function, then it will have a unique inverse function.<br><br> 
________________________________________
2007 - MA307 
CONVEX POLYGONS FIT REGULAR PENTAGON FIT REGULAR HEXAGON
Edwin Xavier Torres, Kenneth J. Rodriguez
Carmen Belen Veiga High School, Juana Diaz, Puerto Rico

The main aim of this research is to find polygons inside of a regular pentagon and a regular hexagon with the half of the area. The hypotheses of the investigation are the following: if the area of a pentagon is found, then the construction of polygons inside of it with the half of its area is possible. If there are polygons inside the pentagon with the half of area, then can be determine designs based on graphs. If the area of a regular hexagon is found, then it is possible to construct polygons inside of it with the half of its area. If it is obtain polygons inside the hexagon with the half of area, then it will be able to construct mosaics.<br><br> Initially, a regular pentagon and a regular hexagon were sketched in a coordinated plane. Inside of them, the following polygons were constructed: square, triangle, rectangle, trapeze, rhombus and hexagon. The areas of these polygons were found using the Gauss formula: <br><br>S = 1 Ð D – I Ð u² , where D = x1y2 + x2y3 + … + XnY1<br><br> 2 <br><br> I = y1x2 + y2x3 + … + YnX1<br><br> The area of the pentagon was 108 u². The polygons with the half area inside the pentagon were: the rectangle, trapeze, rhombus and hexagon with 54 u². The hexagon area was 54 u². The polygons with the half the area inside of the hexagon were: the triangle, rectangle, rhombus, and trapeze with 24 u². After all this determination, figures were defined useful for the designs of mosaics and graphs.<br><br> 
________________________________________
2005 - MA307 
APPLICATION OF MODULAR CRYPTO-FUNCTION IN SECRET CODE THEORY.
Artemio Jose Lopez-Gonzalez, Kevin Joel Rodriguez-Torres
Carmen Belen Veiga High School, Juana Diaz Puerto Rico

<br><br>Cryptography has a long and fascinating history. The most striking development in the history of cryptography came in 1976 when Diffie and Hellman published New Directions in Cryptography. This paper introduced the revolutionary concept of public-key -cryptography and also provided a new and ingenious method. Crypto- graphical methods provide securing electronic commerce for many financial institutions around the world. The search for new public-key schemes, improvements to existing cryptographic mechanisms, and proofs of security continues at a rapid pace. Security products are being developed to address the security needs of an information intensive society. To create a secret code based on prime numbers and basic functions can be useful in all systems and places that use codes. In consequence, creating a codification system based on a polynomial function and prime numbers will be useful in the elaboration of a secret code. We develop a basic code that is useful, advantageous and indecipherable, generated by the function f(x)= Rx(Px/p),where Rx is the remainder of the division of the polynomial Px by the number p, a prime or a product of two prime numbers. Furthermore, we produce a codification system by replacing Px for any expression of the form a^x where a is a big prime number and we show its utility, advantages and its indecipherability. If each prime number has a unique polynomial or exponential function, then it will have a unique inverse function.<br><br> 
________________________________________
2010 - MA307 
STUDYING THE POLAR EQUATION OF THE ROSETTE CONCHOIDS FLOWER
Dielmarie Negron-Rivera, Angel J. Pacheco Santiago, 
Francisco Morales High School, Naranjito, PUERTO RICO

This research is based on the study of the polar equation of the Rosette Conchoids flower, r=a*cos(n*theta)+b with the aim of analyzing the different shapes of the graph when n is a non integer rational number. It was questioned what would occur to the graph under such parameters and how the constants a and b would be related to the length of the petals, the amount of those petals and the pattern of the graph when a=b, a>b and a<b. A graphing device was used to graph equations for different values of a, b and n. After analyzing the results, these revealed that the graphs always describe the shape of a limacon or a peanut curve and the amount of snails or petals are equal to the numerator of n. In addition, there was found a relationship between the values of a and b and the length of the petals in the graph that can be directly or inversely proportional considering a=b, a<b and a>b. The circumstances for which these particular relationships occurred were identified for three cases. These findings validate the hypothesis proposed and suggested new questions for future researches such as; what would happen in the equation if n is an irrational number? Also, how would the limit and integral processes suggest the finding of the area based on a region delimited by curve intervals in a graph? 
________________________________________
2003 - MA307 
9-FLIPS
Manuel Orlando Maldonado, Victor Jose Bonilla
CROEM High School, Mayaguez, Puerto Rico 

In this research the 9-flips numbers are studied. These are natural numbers that when multiplied by nine the result is the same number, but with its digits inverted; for example if abcd is a 9-flip of four digits, then (abcd) x 9 = dcba. Properties of 9-flips with n digits were studied. In particular, it is demonstrated that for n > 4 there exists at least a 9-flip of n digits. Also, given a 9-flip a1a2…an of n digits, the number of m x n digits a1a2…ana1a2…an……a1a2...an is also a 9-flip. Other general properties of 9-flips are proven and particular examples are presented. <br><br><br> <br><br> <br><br> <br><br><br><br><br> <br><br> <br><br> <br><br> 
________________________________________
2009 - MA307 
ISOAREAL AND ISOPERIMETRIC DEFORMATION OF CURVES
Chi Fai Ng, Li Kwok Chung, 
Shatin Tsung Tsin Secondary, Hong Kong, HONG KONG

In this project, we want to answer the following question: <br><br>“How to deform a curve such that the rate of change of perimeter is maximum while the area and the total kinetic energy are fixed?”<br><br>We worked on isosceles triangle as a trial. Then we studied the smooth simple closed curve and obtained the following results:<br><br>Firstly, we found a functiuon to represent the radial velocity of each point of the curve in polar coordinates;<br><br>Secondly, we found that the magnitude of the velocity at each point of the curve along the normal direction is equal to standard score of the curvature at that point;<br><br>Thirdly, we applied the results on Isoperimetric inequality;<br><br>Finally, we worked on the dual isoperimetric problem. 
________________________________________
2008 - MA307 
ABOUT SOME STRAIGHT LINES RELATED TO QUADRILATERIALS
Merey Zholdas, Sabit Shudanov, 
Kazakh-Turkish High School, Aktobe, KAZAKHSTAN

In geometry knowledge of features of collinear points can essentially make easier solutions of problems related to them. For example, straight lines of Euler and Nagel are used in solving some problems on triangles. But only few people know about existence of analogues of these lines for rectangles.<br><br>We were interested in Euler’s and Nagel’s lines of triangles, so we decided to find analogues of these lines for a rectangle, and now we want to share the results of our investigation. We not only descried analogues of these lines, but also we have generalized them.<br><br>In this scientific project we studied Nagel’s and Euler’s lines, found their features, considered all analogues of already known lines and proved their existence. 
________________________________________
2004 - MA307 
ADVANCES IN HADWIGER'S CONJECTURE
Manuel L Rivera, Carlos M. Fonseca
Colegio San Ignacio, Guaynabo, Puerto Rico

One of the most famous conjecture of the theory of graphs, Hadwiger's Conjecture, links colorings with the structure of graphs. This conjecture stated by Hadwiger in 1943 claims that every graph with chromatic number n, contains Kn as a minor. The conjecture has been proven for n=1, 2, 3, 4 and equivalent to the Four Color Theorem for n=5, 6. In this work we address the following question of Seymour related to the general conjecture: Does there exists a positive constant c such that G has a Ks-minor for s=n(1+c)/3 for G being a graph on n vertices with independence number at most 2? In doing so we introduce a new operation which we call PHI-contraction which allows us to proof a duality theorem that relates the graph minors in a graph and the PHI-minors in the complement of the graph. Such duality presents itself as an opportunity to reinterpret many known conjectures and in this way give a new approach toward finding a solution to the question. 

Awards won at the 2004 ISEF
Third Award of $1,000 - Team Projects - Presented by Science News
________________________________________
2003 - MA308 
POSITIVE INTEGERS K IN THE FIBONACCI AND LUCAS SPIRALS
Cynthia Caraballo
University Gardens High School, San Juan, Puerto Rico

Stanislav Ulam belonged to a group of mathematicians who came to the United States from Poland before and during the Second World War and played a very important role in the life of mathematics in this country. Ulam wrote all the positive integers ordered in a spiral, called Ulam¡¯s spiral. In this investigation, the Fibonacci and Lucas numbers were located in two different spirals and were compared. The sequence of Fibonacci is denoted by Fn=Fn-1+Fn-2, and the Lucas one by Ln=Ln-1+Ln-2.<br><br>Let Kn=(Cn+Dn)/(An+Bn), where An, Bn, Cn, and Dn are some particular numbers located in the Fibonacci (Fn) or Lucas (Ln) spirals. Is Kn always a positive integer?<br><br>For 3¡Ün¡Ü15 it was found that:<br><br>i. Kn is always a positive integer; <br><br>ii. The value of Kn in the Fibonacci spiral is always equal to the value of Kn in the Luca spiral;<br><br>iii. Kn is a Fibonacci number<br><br>It was demonstrated that Kn is always positive. Properties of these spirals were studied and some particular examples are presented.<br><br> 
________________________________________
2008 - MA308 
CHAOS: AN ANALYSIS OF MARKET NONLINEARITY
Dhruv Kedar, Shawn Mittal, 
Chantilly High School, Chantilly, VA

With recession and economic disaster approaching this nation, it would be an ideal situation for the Federal Reserve to be able to predict recessions beforehand, thus allowing them to cut interest rates earlier and avoid economic depression. The purpose of this project was to determine whether chaos is apparent in stock indices such as the DOW. To go about doing this, we first applied Hurt's equation (initially used to predict the flooding of the Nile) to ascertain whether fractal patterns existed in the rise and fall of daily stock prices. By using methods of power regressions (through log-log transformations) and rescaled range analysis, we concluded that the Hurst exponent we got, .87, indicated strong chaotic behavior in the market (any value between zero and .5 indicates random behavior). With this data, we used an online applet to construct a Duffing oscillator (with the frequency of .87 corresponding to the Hurst exponent). Our results surprisingly indicated that our oscillator most closely resembled that of the Lorenz attractor (one of the strange attractors that indicates the presence of chaos), and exhibited strong fractal patterns in its composition. Thus, unknowingly, market traders have been trading and investing with the real life application of fractal and chaotic dynamics. 

Awards won at the 2008 ISEF
Third Award of $1,000 - Team Projects - Presented by Science News
________________________________________
2010 - MA308 
AN IMPROVEMENT OF THE HARDY CONDITION
Konstantin A. Anisimov, Bogdan O. Neterebskiy, 
Center of Mathematical Education, Saint-Petersburg, Saint-Petersburg, RUSSIA

In the complex analysis there is a well known problem of finding the function of Hardy space by its points and values. It is known as the Nevanlinna-Pick problem. In the 20th century it was shown that in a particular case the solvability of the Nevanlinna-Pick problem comes to considering the properties of the power series coefficients: Suppose we are given a formal power series with positive coefficients. Let us consider the in-verse formal power series. The question arises as to under which condition on coefficients of the first series the coefficients of the second one will be non-positive except one. For the series with one variable the sim-plest sufficient condition was obtained by Hardy. Recently the improving of the Hardy condition for the one-variable series has been proved. This project continues the research into series with two or more va-riables using new methods we have obtained. The concept of collecting coefficients into special groups and considering these groups instead of separate coefficients was developed and applied to the NP-problem. This result allows us to closely approach the finding of the condition of solvability of the Nevanlinna-Pick prob-lem for the functions of the Hardy space. These result can be applied in such areas as Control theory, radio electronics, compression methods etc. 

Awards won at the 2010 ISEF
Third Award of $1,000 - Team Projects - Presented by Intel
________________________________________
2006 - MA308 
IMPOSSIBILITY OF CONSTRUCTION OF TRIANGLE BY USING THREE BISECTORS
Lado Meskhishvili, Irakli Baiadze
Georgian -American High School, Tbilisi, Georgia Republic

Geometric construction is a construction of a geometric figure using only straightedge (unmarked) and compass. Such constructions lay at the heart of the geometric problems of antiquity of circle squaring, cube duplication, angle trisection and billiard problems. The Greeks were unable to solve these problems, but was not until hundreds of years later that the problems were proved to be actually impossible under the limitation imposed. <br><br> In our project we investigate triangle’s construction problem if are given:medians,altitudes bisectors <br><br> Suppose construction of some figure is possible by using P1, P2,……, Pn data. For constructions are given - Q1, Q2,….,Qn data. If possible to express each Pß by using finite number of simple operations under Qß,then construction is possible. <br><br> Under simple operations are considered addition, difference, multiplication by rational number division,product and extraction of a square root.<br><br> We prove the following propositions: <br><br>Proposition1. Construction of triangle is possible by using three medians.<br><br>Proposition 2. Construction of triangle is possible by using three altitudes.<br><br> Constructions in both cases are done in case of bisectors we get opposite result:<br><br> THEOREM: Construction of triangle is impossible by using three bisectors.<br><br> Cases of medians and altitudes were known but our result about bisectors we have not seen before. <br><br> The problem is insoluble using compass and straightedge because the expression for square of triangle side contains extraction of a cube root like above-mentioned cube duplication angle trisection and billiard problems.<br><br> 

Awards won at the 2006 ISEF
Fourth Award of $500 - Team Projects - Presented by Science News
________________________________________
2009 - MA309 
PROBABILISTIC EVOLUTION IN DYNAMIC EQUILIBRIUM
Yong Zhi Zhou, Andrew (Juno) Jung, 
Pinetree Secondary School, Coquitlam, BC, CANADA

The purpose of Probabilistic Evolution in Dynamic Equilibrium is to mathematically determine trends of population growth in an idealized ecological setting. This project is primarily focused on Lowest Number Wins, a simple probability game modeled after population growth of a species. By analyzing the progressive outcomes of the game, we hoped to demonstrate that species’ growth rate is in an equilibrium state with various factors, a phenomenon we called "Probabilistic Evolution". We defined these factors as variables of population growth, and created a simulation of the game through some computational software, Mathematica. From the simulated data, we found the statistical effects of variables such as time, population, and probability of competition. We observed that a graph of population versus species varies logarithmically, and the population that produces maximum growth occurs reciprocally against the probability of participation. These trends, plus some empirical observations, verified our hypothesis that population growth evolves dynamically and in equilibrium with its variables. With Calculus, we were able to mathematically explain these observations and determine the growth rate as a recursive function of its population. This project details how the parameters of species, time, and competition affect the dynamic mechanics of population growth. This project also establishes with mathematical precision the properties, such as maximum growth rate and rate of evolution, of the population and species. 
________________________________________
2004 - MA309 
RARE NUMBERS
Soreilys Vazquez, Karla Martinez, Xiomara Bermudez
Francisco Morales, Naranjito Puerto Rico

This study was done to investigate if al numbers, including positive and negative ones are rare numbers. Positive numbers from 3,611 up to 4,000 were analyzed and negative numbers from –1 to –373 were also analyzed. The problem established was the following: 371 is a rare number because it is equal to the sum of the third power of its digits. Can you find other rare number using positive or negative numbers? The following hypothesis was established: There are other positive and negative numbers that are rare numbers because are equal to the sum of the third power of their digits. Information related to the topic was gathered and the methodology was established. After the selection of the amount of numbers analyzed, the digits of those numbers were raised to the third power and the figures were added. The results of the previous phase of the study demonstrated that there are some positive rare numbers from 1 to 3,610 and the new data analysis demonstrated that there are no rare numbers among the positive numbers between 3,611 and 4,000. It was also found that only –1 is rare number among the negative numbers from –1 to –373. 
________________________________________
2005 - MA309 
DYNAMIC GEOMETRY FOR THE NEW MILLENIUM
Viviana Padilla , Veronica Acosta
Aurea E. Quiles Claudio High School, Guanica, PR

The purpose of this work was the verification of postulates and theorems of the Euclidean Geometry through Dynamic Geometry. Dynamic Geometry lets user construct animated geometrical models through computer softwares that permit transformations of the original figures. The problem was to find out if postulates and theorems of the Euclidean Geometry can be verified using the Dynamic Geometry. It was inferred that this could be possible if the commands and options of Geometer’s Sketchpad and Cabri and the selected postulates and theorems of the Euclidian Geometry are studied. After the data collection about the theme, the selection of the postulates and theorems began. <br><br> Through the computer softwares, Geometer’s Sketchpad and Cabri the postulates and theorems about congruent triangles, parallelism, point of concurrency of the medians, altitudes, angle bisectors and perpendicular bisector of the sides of a triangle, circles, angles of a polygon and other conjectures were validated. The Dynamic Geometry demonstrated to be a useful tool in the study of the Euclidean Geometry and to prove geometrical conjectures. A projection for this investigation is to verify other mathematic axioms using other computer softwares like: Lugares, Cinderella and GEUP, among others. <br><br><br> 
________________________________________
2010 - MA309 
IMPROVINY OF INEQUALITIES BETWEEN MEANS
Abylay Galymzhanuly Kuanyshov, Assel Almazbekovna Aliyeva, 
High-School #134, Almaty, Almaty, KAZAKHSTAN

On math school competition we came across with the following inequality: . where x, y, z – positive real numbers”<br><br>Each of the project’s participants solved this problem in one’s own way. Then our supervisor noted that possibly this inequality had various generalizations. At that point we started our research.<br><br>Thus, in this work we proved the following inequality:<br><br>1. ,<br><br> where x, y, z – positive real numbers (statement 1).<br><br>2. ,<br><br> where (statement 2)<br><br>3. ,<br><br>where (statement 3)<br><br>4. Let . For all positive numbers , ,…, and <br><br>this inequality is right:<br><br> ( statement 4)<br><br>5. (statement 5)<br><br>In addition we found maximal none improving coefficients k, which makes this inequality right:<br><br>1. <br><br>It was “asymp” 4,4036<br><br>2. , KAPPA=1,<br><br>3. , KAPPA=1.5 

Awards won at the 2010 ISEF
Fourth Award of $500 - Team Projects - Presented by Intel
________________________________________
2005 - MA310 
PATTERNS WITH PRIME NUMBERS
Eileen J Colon, Yanira B. Colon
Lysander Borrero Terry High School, Villalba, Puerto Rico

The set of Prime Numbers is umpredictable, there are no formulas to find Prime Numbers and there isn't an order with them. With the purpose of finding some type of pattern in the set of Prime Numbers, the researchers started studying their Digital Roots. The Digital Root function (y=DR(x)), is a mapping from the positive integers onto the set of digits {1, 2,...9} and which assigns to each integer that digit obtained by continually adding the digits of the integer until a single digit remains. To find some type of pattern it was neccesary to get a considerable amount of Prime Numbers classified according to their digital root. Such classification was extremely tedious when done manually or by proving. The researchers studied options that would allow a more precise classification in the shortest time and with the necessary amount to establish reasonable conclusions. This analysis allowed to write a program in the computer programming language B.A.S.I.C. which permited to make a very exact classification. When observing the classification of prime numbers based in their digital roots it was noticed that they were distributed equally between the digital roots 1, 2, 4, 5, 7, and 8. Later on the researchers made a statistical analysis using the standard deviation of the data obtained.<br><br> After finishing the analysis with Prime Numbers, the researchers studied the digital roots of different powers of positive integers. It could be observed that there are series of recurrent patterns. 
________________________________________
2004 - MA310 
GENERALIZATION OF CARDANO'S FORMULA
Mikhail G. Blotski, Andrei I. Zhemaituk
BSU Liceum, Minsk, Belarus

It is well known that any cubic equation can be solved in radicals, that is roots of cubic polynomials can be expressed via their coefficients using four basic arithmetic operations along with operation of extracting roots of arbitrary powers. It is so called Cardano’s formula. On the other hand, the famous Abel’s theorem states that for any n>4 the equation of n-th degree, in general, can not be solved in radicals.<br><br>We consider a wide class of polynomials of arbitrary degree which can be solved in radicals and derive a formula for the roots of this polynomials. This formula is a generalization of Cardano’s one.<br><br>The obtained results can be used in Galois theory, general theory of polynomials, in particular, Chebyshev’s polynomials theory. Also using these results we can establish some interesting identities. 
________________________________________
2010 - MA310 
ADDITION CHAINS WITH VARIATIONS
Dmytro Bondarenko, Vladyslav Krasnolutskyy, 
Kyivo-Pecherskiy Lycee No 171 " Leader ", Kyiv, UKRAINE

Addition chains and addition-subtraction chains are widely used in public key cryptography to efficiently compute modulo exponentiation and group scalar multiplication. Previous researchers assumed in their analysis that the costs of two basic operations of addition chains, addition and doubling, are equal. However, analysis of cryptographic tasks shows that the cost of doubling may be between negligible (for squaring in a normal basis in Galois fields) and the cost of addition. <br><br> The project studies addition and addition-subtraction chains in the case when the cost of each addition (and subtraction) is 1, and the cost of each doubling is some a between 0 and 1. Our goal is to compare the chain with the lowest cost with classical addition chain.<br><br> We proved that there exists infinetely many integers for which classical addition chains are not optimal. We proposed new algorithm for finding the best possible chain (with the lowest weight) and compared its results to the classical chains. Moreover, up to date no polynomial algorithm known for finding shortest addition chain. And so efficient, but not the best chains are used for the tasks of cryptography. For the extreme case of “free” doublings we proposed new efficient algorithm that outperforms widely used ones (binary and window methods). 
________________________________________
2004 - MA310 
GENERALIZATION OF CARDANO'S FORMULA
Mikhail G. Blotski, Andrei I. Zhemaituk
BSU Liceum, Minsk, Belarus

It is well known that any cubic equation can be solved in radicals, that is roots of cubic polynomials can be expressed via their coefficients using four basic arithmetic operations along with operation of extracting roots of arbitrary powers. It is so called Cardano’s formula. On the other hand, the famous Abel’s theorem states that for any n>4 the equation of n-th degree, in general, can not be solved in radicals.<br><br>We consider a wide class of polynomials of arbitrary degree which can be solved in radicals and derive a formula for the roots of this polynomials. This formula is a generalization of Cardano’s one.<br><br>The obtained results can be used in Galois theory, general theory of polynomials, in particular, Chebyshev’s polynomials theory. Also using these results we can establish some interesting identities. 
________________________________________
2007 - MA310 
EDUCATION PROGRAM FOR DESCRIPTIVE GEOMETRY
Martin Hlavac, Michal Kren
Stredni Prumyslova Skola Uherske Hradiste, Uherske Hradiste, Czech republic

Descriptive geometry creates three-dimensional virtual space on a two-dimensional plane and develops technical thinking. We decided to do the education program for descriptive geometry, because there are no quality teaching tools at schools. We wanted to make descriptive geometry easier to understand and we created program for home learning and for school. <br><br>The program uses the most modern multimedia technology, but its system requirements are very low. First two chapters are about the theory of descriptive geometry and about principles of projection. Next chapters include about 40 pivotal exercises, which are solved during one year teaching descriptive geometry at high school. Each exercise is modeled by 3D graphics and you can take a view from different sides. Each exercise is also animated, to show how to draw every single example to an exercise book. Last chapter is a theoretical test.<br><br>We present our program on website www.deskriptiva.com, where you can find description, discussion and downloading. While working on this program we are in permanent touch with our users to improve it. The Czech version is used by more than 10 000 users. Of course this growing number encourages us to work on it constantly. Because of that we know that our work is not useless and we think it is the most important thing on every project.<br><br> 
________________________________________
2003 - MA310 
ABOUT THE ERDOS-WOODS` CONJECTURE
ZHIBEK KADYRSIZOVA, AKMARAL BEGZHIGITOVA
The republican school specialize in physics and math, Almaty, Kazakhstan

In the project the Erdos –Woods` conjecture on the existence of two different arbitrarily long uples of consecutive positive integers with corresponding members having the same prime factors. The description of the Erdos –Woods` quartets with the leader less than 100 is given. Two<br><br>Methods, namely, the methods of cascades and method of pseudo – classes are invented for this goal, they are demonstrated in solutions of concrete questions of the description. In the appendix the complete proof<br><br>of all questions of the description and applied MAPLE – programs are presented.<br><br> 

Awards won at the 2003 ISEF
Fourth Award of $500 - Team Projects - Presented by Science News
________________________________________
2007 - MA311 
SUFFICIENT CONDITIONS FOR SOLVABILITY OF NEVANLINNA-PICK PROBLEM IN THE CASE OF NON-NEGATIVE COEFFICIENTS OF REPRODUCING KERNEL.
Alexander V. Shchegolev, Konstantin S. Tyapochkin
Centre of Mathematical Education, Saint-Petersburg, Russian Federation

Our project deals with problem of characterization of functions, which are Nevanlinna-Pick kernels. There are some solutions in general form. This work is a research on particular Nevanlinna-Pick kernels.<br><br> G.Hardy suggested looking for NP-kernels among functions f(x) defined as a series a_n*x^n. He proved, that such a function is a NP-kernel, if all coefficients a_n are non-negative and all coefficients of a series, which is inverse to f(x) (let us denote them by b_n) are non-positive. So the problem is ti find conditions on coefficients a_n, sufficient for coefficients b_n be non-positive.<br><br> Hardy solved this problem for the case of all coefficients a_n are strictly positive, but his theorems and methods are inapplicable for the case of some coefficients a_n equals 0.<br><br> This project is a research for this case. First, we generalized Hardy's theorems for the case of only one coefficient a_n equals 0 and for the case of the first n coefficients equal 0. Then we proved, that structure of 0-coefficients is just the same for all series, which can be NP-kernels. Finally, we generalized Hardy's theorem for the case of arbitrary non-negative coefficients a_n.<br><br> <br><br> So, the results of our research are, on the one hand, generalization of Hardy's results, on the other hand, an analysis of the structure of 0-coefficients of series like NP-kernels.<br><br> 

Awards won at the 2007 ISEF
Fourth Award of $500 - Team Projects - Presented by Science News
________________________________________
2004 - MA311 
WALKING WITH A MILLIPEDE
Jarupon Sathirapongsasuti, Poomyos Wimonkittiwat, Natdanai Punnanithi
Triam Udom Suksa School, Bangkok, Thailand

With its excellent management, millipede’s overall legs surprisingly have a beautiful wavelike movement and consequently high stability occurs. The purpose of this project is to obtain a set of equations describing millipede’s leg movement, particularly the positions of the tip of the leg. Comparing acquired data, the wave velocity v_wave with the multiple of the frequency f and the wavelength LAMBDA, we attained the equation, v_wave = fLAMBDA. This corresponded to a continuous periodic wave.<br><br> From the sinusoidal regression analysis of ventral-view photographs, it was found that the ordinal number of a leg and the angle between the leg and the normal line, perpendicular to millipede’s body, was significantly related in sine function. The average coefficient of determination r^2 was 0.805. Having observed certain videos, we computed the (x, y) trajectory of the tip of the swinging leg through a cycloid function of time t and distance between the point of reference to each leg, d, given by: <br><br><br> x = v_millipede (t – d/v_wave) + r sin(kd – wt)<br><br> y = -r cos (kd – wt) , where y >= r cos (ZETA/2)<br><br><br>whereas v_millipede is the millipede’s velocity, r is the radius of a reference circle which a tip of a leg traces, k is the wave number, ZETA is the angle at the center of the circle path standing on the arc along which the tip traces, and w is the angular frequency.<br><br> From the regression analysis of the trajectory using time domain, the r^2 in x-t graph fitting has the average of 0.887, and that of y-t graph fitting is 0.879. The computed trajectory is affirmed in this experimental session. Moreover, the acquired equation set is applicable to turning. Through the observation, a simulating program confirmed the result. This project contributes to the progress in multi-legged robot. <br><br><br> 

Awards won at the 2004 ISEF
First Award of $1,000 - Sigma Xi, The Scientific Research Society
________________________________________
2004 - MA312 
CONSTRUCTING BOXES WITH N-TETRACUBES
Tair Assangali, Nurlan Bakitzhanov
Kazakh-Turkish Lycee, Aktobe, Kazakhstan

The idea of our project is taken from popular game – “Tetris”. The main problem is to pack parallelepipeds and cubes with N-tetracubes, which consist from four single cubes. From this we know that, if cube or parallelepiped can be packed from N-tetracubes, then we call this cube or parallelepiped, N-cube or N-box.<br><br>We found all conditions which are need to pack N-cube or N-box. If our box doesn’t correspond to one or more conditions, it can’t be packed from N-tetracubes.<br><br>During the research of our project, we met a few problems of science research. We solve them by using our methods and theorems. <br><br>In conclusion we collected all conditions which needed to pack N-box, and by using these conditions we created the main theorem. With our common theorem we can know, will box or cube be N-box or not. 

Awards won at the 2004 ISEF
Honorable Mention Award - American Mathematical Society
________________________________________
2004 - MA313 
ADDITIONAL STAINER`S PROBLEM
Zhanna Bekzhankyzy, Yelena Pak
Technical Lyceum #20, Taldykorgan, Kazakhstan

There is the next theorem in the Richard Courant’s and Herbert Robbins’s book “What is Mathematics?”: “Let a, b, c be the distances from the some point to vertexes A, B and C of triangle ÀBC. If each of angles of ABC is less, than 120 degrees, then the value a+b-c reaches its minimum at the point C; if one of them (for example, C) is greater than 120 degrees, then a+b-c gets its minimum at the point P, from which two of the sides of ABC are seen in angle 60 degrees, and the last one is seen in angle 120 degrees”. We had tried to prove this theorem ourselves, but unexpectedly we found that it is not right.<br><br>In our project we have given the full solution of this problem about minimum of a+b-c. Our proof is based on the next lemmas.<br><br> Lemma 1. If at least one of two points F1 and F2 is placed out of ring w (with O-center of w), X=M is the point of minimum of difference F2X-F1X on the circle (X is point of w), then lines F1M and F2M make with OM equal angles.<br><br> Lemma 2. If point P (which we are looking for) is not A, B or C, then B and C cannot be both in the ring wA with center A and radius PA=a; the same is right for points A and C and ring wB.<br><br> Lemma 3. If X=P is the point of minimum for value of AX+BX-CX and P is not a vertex of ABC, then two of the segments AB=x, AC=y, BC=z are seen from point P in angle 60 degrees, and the third one in angle 120 degrees.<br><br> The main result:"Let ABC be some triangle of a plane. If angle A>=60 degrees, angle B>=60 degrees, angle C<60 degrees,then there is the point P, from which AC and BC are seen in angle 60 degrees and AB is seen in angle 120 degrees; and the S(X)=AX+BX-CX gets its minimum at point X=P. In all other cases S(X) gets its minimum at the vertex of minimal of two angles A or B.<br><br><br><br> 
________________________________________
2005 - MA313 
THE MALTESE CROSS AND PROBLEMS ON COMPARISON OF AREAS
Aslan Kalabayev, Ulan Kozybakov
Aktobe Kazakh-Turkish Lyceum, Aktobe, KAZAKHSTAN

In this work have been investigated the following questions related to the Maltese Cross problem:<br><br>1) The problem of finding ratio between areas of a square and a figure formed by lines connecting vertices with the midpoints of opposite side.<br><br>2) The same problem with a figure formed by lines connecting vertices <br><br>with points dividing opposite sides in a fixed ratio. Finally, in the present work is considered the generalized problem for any quadrilateral and ratio. Particularly for a ratio 1/2, i.e. for side midpoints, the value of the ratio is between 1/5 and 1/6.<br><br> <br><br>Goal<br><br>The famous Maltese Cross problem says that in a square the ratio <br><br>between the area of a square formed by lines connecting a vertex with the midpoint of one of the opposite sides, and the area of main square is 1/5. The main goal of this paper is to solve Maltese Cross problem for an arbitrary quadrilateral.<br><br>Summary <br><br>The mentioned problem was solved using the methods of analytic geometry: affine and Cartesian coordinate system. During the investigation were found new and useful inequalities. Particularly, were found upper and lower bounds for the obtained ratio. Also we have found the complete class of quadrilaterals, for which the lower bound of this ratio is achieved.<br><br> 
________________________________________
2004 - MA313 
ADDITIONAL STAINER`S PROBLEM
Zhanna Bekzhankyzy, Yelena Pak
Technical Lyceum #20, Taldykorgan, Kazakhstan

There is the next theorem in the Richard Courant’s and Herbert Robbins’s book “What is Mathematics?”: “Let a, b, c be the distances from the some point to vertexes A, B and C of triangle ÀBC. If each of angles of ABC is less, than 120 degrees, then the value a+b-c reaches its minimum at the point C; if one of them (for example, C) is greater than 120 degrees, then a+b-c gets its minimum at the point P, from which two of the sides of ABC are seen in angle 60 degrees, and the last one is seen in angle 120 degrees”. We had tried to prove this theorem ourselves, but unexpectedly we found that it is not right.<br><br>In our project we have given the full solution of this problem about minimum of a+b-c. Our proof is based on the next lemmas.<br><br> Lemma 1. If at least one of two points F1 and F2 is placed out of ring w (with O-center of w), X=M is the point of minimum of difference F2X-F1X on the circle (X is point of w), then lines F1M and F2M make with OM equal angles.<br><br> Lemma 2. If point P (which we are looking for) is not A, B or C, then B and C cannot be both in the ring wA with center A and radius PA=a; the same is right for points A and C and ring wB.<br><br> Lemma 3. If X=P is the point of minimum for value of AX+BX-CX and P is not a vertex of ABC, then two of the segments AB=x, AC=y, BC=z are seen from point P in angle 60 degrees, and the third one in angle 120 degrees.<br><br> The main result:"Let ABC be some triangle of a plane. If angle A>=60 degrees, angle B>=60 degrees, angle C<60 degrees,then there is the point P, from which AC and BC are seen in angle 60 degrees and AB is seen in angle 120 degrees; and the S(X)=AX+BX-CX gets its minimum at point X=P. In all other cases S(X) gets its minimum at the vertex of minimal of two angles A or B.<br><br><br><br> 
________________________________________
2005 - MA313 
THE MALTESE CROSS AND PROBLEMS ON COMPARISON OF AREAS
Aslan Kalabayev, Ulan Kozybakov
Aktobe Kazakh-Turkish Lyceum, Aktobe, KAZAKHSTAN

In this work have been investigated the following questions related to the Maltese Cross problem:<br><br>1) The problem of finding ratio between areas of a square and a figure formed by lines connecting vertices with the midpoints of opposite side.<br><br>2) The same problem with a figure formed by lines connecting vertices <br><br>with points dividing opposite sides in a fixed ratio. Finally, in the present work is considered the generalized problem for any quadrilateral and ratio. Particularly for a ratio 1/2, i.e. for side midpoints, the value of the ratio is between 1/5 and 1/6.<br><br> <br><br>Goal<br><br>The famous Maltese Cross problem says that in a square the ratio <br><br>between the area of a square formed by lines connecting a vertex with the midpoint of one of the opposite sides, and the area of main square is 1/5. The main goal of this paper is to solve Maltese Cross problem for an arbitrary quadrilateral.<br><br>Summary <br><br>The mentioned problem was solved using the methods of analytic geometry: affine and Cartesian coordinate system. During the investigation were found new and useful inequalities. Particularly, were found upper and lower bounds for the obtained ratio. Also we have found the complete class of quadrilaterals, for which the lower bound of this ratio is achieved.<br><br> 
________________________________________
2013 - MA001 
RATIONAL FIXED POINTS OF POLYNOMIAL INVOLUTIONS
Stanislav Ivanov Atanasov
Model High School of Mathematics "Academician Kiril Popov", Plovdiv, BULGARIA

This project analyzes a problem posed by Jean-Pierre Serre. Consider a polynomial map F:(x_1,..., x_n)->(f_1(x_1,..., x_n),...,f_n(x_1,..., x_n)), where f_i, 1 <= i <= n are polynomials in K[x_1,..., x_n] for a field K. If F(F(x))=x for every vector x in K^n, we say that F is a polynomial involution. It is known that if K is algebraically closed, then every polynomial involution F admits a fixed point, i.e., a point x in K^n such that F(x)=x. Serre's question deals with the case when K=Q - the set of rational numbers and asks: if the polynomial involution F has rational coefficients,i.e., all polynomial components have rational coefficients, does it necessarily possess a rational fixed point?<br><br>We answer Serre's question affirmatively in the case that deg f_i = 1 for 1 <= i < n, as well as for n = 2. We pose the question of classification of polynomial involutions of C^n according to the degrees of their polynomial components and solve this problem almost completely. Furthermore, we show that Serre's question is closely related to the elusive Cancellation Problem in the field of algebraic geometry. We also derive that for a polynomial involution to give a negative answer to Serre's question, we need to find wild and nonlinearizable polynomial automorphism; such an automorphisms is not known so far. 

Awards won at the 2013 ISEF
Second Award of $500 - American Mathematical Society
________________________________________
2011 - MA001 
DEVELOPING ANALYTICAL APPROACHES TO FORECAST WIND FARM PRODUCTION, PHASE II
Kate Alexandra Geschwind
Mayo High School, Rochester, MN

In this project, data-driven methodologies for the development of mathematical models of three wind farms are presented. Hourly observed weather data was taken from three national weather stations nearest to the locations of the wind farms, Rochester, MN; Tulsa, Oklahoma; and Burlington, VT, for the months of December 2009 through May 2010. Each wind farm was of a different size to demonstrate how size affects the ability to accurately forecast the wind farm output, as well as each wind farm was of a different location, to demonstrate how geographical location affects the ability to accurately forecast wind farm output. Multiple explanatory variables were then derived to be used in each data set: wind speed squared, wind speed cubed, a one hour lag of the actual wind farm production, and the change in the one hour lag. These variables, along with wind speed, were combined in various ways to form different mathematical models using regression analysis and artificial intelligence. Each model was then used to calculate the RMSE for the testing data from the wind farm that it had been derived from. The RMSE values were calculated over four different forecast time periods: a one hour forecast, a six hour forecast, a twelve hour forecast, and a twenty-four hour forecast. The models with the lowest RMSE for each wind farm were then deemed the most accurate in forecasting the wind farm output of that particular wind farm. 

Awards won at the 2011 ISEF
Certificate of Honorable Mention - American Mathematical Society
Certificate of Honorable Mention - American Statistical Association
________________________________________
2012 - MA001 
MATHEMATICAL TIME-MODELS OF NETWORKS IN THE WORLD
Heeyoon Kim
Rockdale Magnet School for Science and Technology, Conyers, GA

Mathematical models have opened up exciting avenues of research, including social interactions among people. This project developed a model that could simulate behavior amongst a representative sample population of 300 people. The model was based on a differential equation (Marvel et al, 2011) and was used to project the relative lengths of times needed for various countries to stabilize: Afghanistan, Egypt, Ghana, Iran, Japan, and the United States. The effect of different cultural factors on stabilization was also observed using the model. By isolating the effects of each, it was possible to find the source of potential instability and division for each country. The model relied on matrices to organize relationship values between each pair of people, and the differential equation, dX/dt = X^2 to model changes of those relationships, where X is the matrix and dX/dt represents change. A computer program was written that could generate the iterations of adding X^2 to X. Square heat diagrams were drawn to better observe the changes in the different countries. After generating and analyzing for variance, it was found that many of the hypotheses (based on current events) were supported, which suggests that the model was accurate. For example, Afghanistan was found to be relatively unstable, whereas Japan and the United States stabilized quickly. The overall differences in the times for stability were found to be significant by ANOVA (F=3.89; df=5; P<0.05). Also, it was found that ethnicity was the greatest source of division in Afghanistan. These findings contribute vital information to promote unity and stability of world networks. 

Awards won at the 2012 ISEF
Second Award of $1,000 - Franklin & Marshall College
________________________________________
2011 - MA001 
DEVELOPING ANALYTICAL APPROACHES TO FORECAST WIND FARM PRODUCTION, PHASE II
Kate Alexandra Geschwind
Mayo High School, Rochester, MN

In this project, data-driven methodologies for the development of mathematical models of three wind farms are presented. Hourly observed weather data was taken from three national weather stations nearest to the locations of the wind farms, Rochester, MN; Tulsa, Oklahoma; and Burlington, VT, for the months of December 2009 through May 2010. Each wind farm was of a different size to demonstrate how size affects the ability to accurately forecast the wind farm output, as well as each wind farm was of a different location, to demonstrate how geographical location affects the ability to accurately forecast wind farm output. Multiple explanatory variables were then derived to be used in each data set: wind speed squared, wind speed cubed, a one hour lag of the actual wind farm production, and the change in the one hour lag. These variables, along with wind speed, were combined in various ways to form different mathematical models using regression analysis and artificial intelligence. Each model was then used to calculate the RMSE for the testing data from the wind farm that it had been derived from. The RMSE values were calculated over four different forecast time periods: a one hour forecast, a six hour forecast, a twelve hour forecast, and a twenty-four hour forecast. The models with the lowest RMSE for each wind farm were then deemed the most accurate in forecasting the wind farm output of that particular wind farm. 

Awards won at the 2011 ISEF
Third Award of $1,000 - Mathematical Sciences - Presented by Intel
________________________________________
2012 - MA002 
MATHEMATICAL MUSIC, METHODICAL MASTERPIECES II: A CONTINUING STUDY ON THE CREATION OF MUSIC USING IRRATIONAL NUMBERS
Danae Jean Pick
Olivet Private School, Port Saint Lucie, FL

The objective of this project was twofold. Part 1: To create an original musical piece based on the twelfth root of two (1.059…, frequency distance between half tones), using a hand created deterministic translational algorithm which assigned numbers to musical notes and rhythms. <br><br>Algorithm used:<br><br>Bass clef: A (low) =1, B (low) =2, C (low) =3, D (low) =4, E=5, F=6, G=7, A=8, B=9, Quarter rest=0<br><br>Treble clef: C=1, D=2, E=3, F=4, G=5, A=6, B=7, C (8va) =8, D (8va) =9, Quarter rest=0<br><br>Rhythm assignments (treble clef only): 1=Quarter, 2=Eighth, 3=Double Eighth, 4=Four 16th notes, 5=Dotted Quarter, 6=Trill, 7=Dotted Quarter, 8=Half, 9=Triplet, 0=Quarter rest.<br><br>Procedure:<br><br>1. Took every number in the sequence and matched it to the assigned note and duration value.<br><br>2. Plotted each note manually into computerized music notation software, composing a musical piece based on the 12th root of 2.<br><br>Part 2: The researcher created a consonance vs. dissonance survey. The survey was conducted to determine whether the American public prefers consonance or dissonance in music. Four compositions were divided into sets (Set 1 ALG1-Dissonance: Music of Pi, Music of the Golden Number Phi. Set 2, ALG2-Consonance: Music of Pi, Music of the Golden Number Phi). <br><br>The researcher compared each score for consonance and dissonance, repetition, sequence and fragmentation. A few conclusions are that the 12th root of 2 contained the most fragmented repetition, and that the American public prefers consonance over dissonance when presented with music formed using an even-tempered tuning system. 
________________________________________
2013 - MA002 
ON WEITZENBOECK DERIVATIONS OF FREE METABELIAN ASSOCIATIVE AND LIE ALGEBRAS
Rumen Rumenov Dangovski
Sofia High School of Mathematics, Sofia, BULGARIA

For a fixed field K of characteristic 0 we consider a nonzero locally nilpotent linear derivation D of the polynomial algebra K[X_d] in d variables, which we shall call a Weitzenboeck derivation. Weitzenboeck proved that the algebra of constants K[X_d]^D of the polynomial algebra in d variables is finitely generated. The same question about the generation of the algebra of constants may be posed for finitely generated algebras, which are relatively free in a given variety of algebras over the field K. In this case, the algebra of constants is usually not finitely generated. Despite the fact that this proposition holds for the algebra of constants (L_d/L_d'')^D of the free metabelian Lie algebra L_d/L_d'', in this research we study this algebra and show that the vector space of the constants (L_d'/L_d'')^D in the commutator ideal L_d'/L_d'' of the free metabelian Lie algebra is a finitely generated K[X_d]^D-module. Moreover, we use specific combinatorially motivated apparatus (Elliott's method, MacMahon calculus and Schur functions) to establish an algorithm which calculates the Hilbert series of these algebras. The results are of algebraic importance because they help us find the generators of the algebra of constants for special values for d. Similarly, we pose the problem for free metabelian associative unitary algebras. As a whole, we manage to show that these vital non-finitely generated algebras of constants possess some features typical for the finitely generated ones. 

Awards won at the 2013 ISEF
Certificate of Honorable Mention - American Mathematical Society
________________________________________
2011 - MA002 
REFORMULATING THE NEWTON DIRECTION COMPUTATION AS A LINEAR LEAST SQUARES PROBLEM FOR SMOOTHED OVERDETERMINED L1 FUNCTIONALS
Matthew Russel Bauerle
Bauerle Homeschool, Fenton, MI

From robust data fitting in statistics to L1 finite element methods<br><br>for fluid dynamics, the overdetermined L1 minimization problem arises<br><br>in many practical applications. First, this paper presents a proof of the convergence of Newton's method for the smoothed overdetermined L1<br><br>minimization problem. Next, new formulas are derived for computing<br><br>the Newton search direction by solving a linear least squares problem.<br><br>Numerical examples show that the new least squares reformulation<br><br>stably computes L1 minimizers for coefficient matrices with condition<br><br>numbers from 10^3 to 10^6. In contrast, the existing normal equations formulation completely failed to provide an accurate solution for<br><br>the ill-conditioned coefficient matrices with condition numbers of 10^5<br><br>and 10^6. The reformulation currently holds great potential for any<br><br>overdetermined ill-conditioned L1 minimization problem and can be<br><br>easily extended to nonlinear or underdetermined problems. This paper presents many practical applications and the circumstances where<br><br>ill-conditioned matrices appear. 

Awards won at the 2011 ISEF
Intel ISEF Best of Category Award of $5,000 for Top First Place Winner - Mathematical Sciences - Presented by Intel
$ 3,000.00 will be the total of all awards. First award minimum of $1500.00 - Mu Alpha Theta, National High School and Two-Year College Mathematics Honor Society
________________________________________
2011 - MA003 
THE ORDERING OF THE DIVISORS OF AN INTEGER AND RELATED NUMBER THEORETIC FUNCTIONS
Myles Reid Scolnick
Cherry Creek High School, Greenwood Village, CO

The purpose of this research was to explore the set ordering of the divisors of an integer n as well as the two closest divisors. This was done by creating number theoretic functions, such as Euler’s Tau and Sigma functions, in order to study properties of integers. These number theoretic functions were defined as:<br><br>F(n)=(x,y) an ordered pair of the two closest divisors of n<br><br>Delta(n)=x-y <br><br>The problem that this research seeks to resolve is a way to order all the divisors of an integer n. A special case that was heavily look upon is when n = p^lq^m, thus F(p^lq^m). These functions were studied form different points of view to further the understanding of this research. Firstly, these functions were looked at as a directed graph, for example a tree graph. Also these were looked at as a semi-group of ordered comparisons, and lastly, these were studied using rational approximations to represent irrational numbers with the use of continued fractions and their convergent. Using inductive and deductive reasoning, many connections have been found. First, it can be concluded that only two precise relationships between both primes of n are needed to fully order the set of divisors of n. Secondly, different formulas have been developed for all integers n where n is divisible by one prime or two distinct primes. Lastly, the conclusion has led to an astounding understanding of the creation of a fully ordered set of the divisors of n through the methods mentioned. <br><br> 
________________________________________
2013 - MA003 
ANGLES AND BRIDGES: THE EFFECTS OF VARIOUS ARCS ON THE STRENGTH OF BRIDGES
Reece Matthew Leleux
Ovey Comeaux High School, Lafayette, LA

A bridge is important for travel. When I pass over a bridge, I wonder what makes it unique mathematically. For test #1 my project looks at the classic arch abutment bridge and how a change in the arch affects its efficiency (strength and price). I derived a bridge equation from the vertex form of a parabola.<br><br>I expected that if the variable “p” in the bridge equation increased, the structural strength and the price would decrease. <br><br> Three sizes of bridges were selected: 40 m by 4 m, 36 m by 8 m and 32 m by 12 m, with 3 different vertexes and 3 different values of “p”. The West Point Bridge Builder program analyzed minimal thickness of each member of the bridge (strength) and the cost of the entire project. The ratio of percent compression of each member was recorded for each bridge using a thickness of 150 mm in order to measure where stress occurs in each bridge. Data supports my hypothesis; in addition, the height of the arch of the bridge can increase as the size decreases without losing structural integrity. For test #2, I tested for the same factors for an arch truss bridge with 3 vertex and 3 values of “p” for three different sizes: 40 m, 36 m and 32 m long bridges. After the data was analyzed, the hypothesis was accepted that the higher values of “p” would increase the price and also the strength. 
________________________________________
2013 - MA004 
CONTROLLING A FLYING CAM: THEORY AND APPLICATION
Laura Maehler
Aldegrevergymnasium Soest, Soest, GERMANY

People all over the world enjoy watching sports on TV. But to transfer the excitement and tension of the match, a subtle camera work is required, the impression is indeed regulated by the presentation. For this it is important to shoot from different directions. But how is this enabled? <br><br>The so-called flying cams provide such great pictures and insure an exciting sports-experience even at home.<br><br>In my research I described the functioning of some of those cameras, the spidercam and the cablecam, mathematical. Furthermore I examined how to improve the cameras controlling. <br><br>I found out, that the cablecam moves along the cross sections of quadrics, which are created by threat constructions. These threat constructions are engendered by the ropes, which are contrived in a very special fair lead. <br><br>To describe the cameras movement along the x- and y-axis I calculated its path from the cross section of two ellipsoids.<br><br>To prove the theory and to comprehend its working, I built a model of the cablecam. <br><br>Seeing all this technology and options encourage to think of other possible areas of application. For example flying cameras could also be used as crime prevention. Seeing crowds from above and being able to see everything from every angle could really help security guards to control and to supervise events. 
________________________________________
2012 - MA004 
ON FINDING ROOTS RELATIVE TO SEMIPRIME MODULI
Zen Tang
Little Rock Central High School, Little Rock, AR

This project attempted to seek a method for finding roots of an arbitrary number relative to a semiprime modulus that would be computationally faster than the fastest method currently available, which involves factoring the modulus. To do this, an experimental program was run on a computer to compare the running times of using a quadratic sieve factoring algorithm and a trial-and-error approach to take roots relative to a modulus, with the aim of discovering a pattern among the roots of various numbers. Random numbers and moduli were chosen, and a total of 125 different trials were run. The run times of the two methods were recorded and compared. The data show the quadratic sieve algorithm performing much better than the trial-and-error approach on almost all cases. Also, the run times of the trial-and-error method were seen to follow a roughly exponential increase as the independent variable increased, similar to the fact that current factoring algorithms also have exponentially increasing run times. In 80% of the data sets, critical values from t-tests that signified at least 95% certainty of a significant difference were found, while in the rest of the data, the t-tests indicated that the differences weren’t statistically significant. This finding reinforces the current fact that the most efficient way to find roots relative to nonprime moduli is to first factor the modulus. 
________________________________________
2011 - MA004 
CONTINUED FRACTIONS AND MATHEMATICAL CONSTANTS
Gabriel Wolfgang Shih
Laramie Senior High School, Laramie, WY

The purpose of my project was to know the connection between continued fractions and mathematical constants. My engineering goal was to study the continued fractions and investigate its applications to mathematical constants. <br><br>The mathematical constants I examined were the Pythagoras’ constant, Fibonacci numbers and the golden ratio, Archimedes’ constant, Euler’s number, and Euler–Mascheroni constant. These famous numbers are used in fields of mathematics and non-mathematics. In addition, I observed rational and irrational numbers as continued fractions.<br><br>My project is important because one can be more knowledgeable from learning many new mathematical concepts. A significant application of continued fractions is deciphering efficiently in cryptography. Also individuals can appreciate the beauty of continued fractions and mathematical constants.<br><br>In the future project I would extend the theory of continued fractions to more mathematical constants as well as functions, including the exponential function, the logarithmic function, and the trigonometric functions. 
________________________________________
2011 - MA005 
AN INTRODUCTION TO RSA CRYPTOGRAPHY AND MODERN INTEGER FACTORISATION METHODS
Jason Mark Benedict Long
The Glasgow Academy, Glasgow, UNITED KINGDOM

This project explores RSA cryptography and the problem of integer factorisation from a mathematical and computational perspective, examining some of the recent breakthroughs in these fields. A specific objective of the project was to consider the problem of parameter optimisation in the quadratic sieve algorithm.<br><br>The work began with a study of the mechanics of RSA cryptography and the mathematical foundations that underpin its security and widespread use. A detailed empirical and theoretical analysis of the quadratic sieve (QS) and the elliptic curve method (ECM) was undertaken, and these algorithms were implemented using Haskell as the primary programming language. This allowed the two methods to be compared, both mathematically and observationally. Finally, the implementation of the QS was used to study the problem of parameter optimisation and relevant empirical results were gathered and analysed.<br><br>Due to the computation required to estimate optimal parameters using a brute force approach, a cluster of 10 computers was used to collect data. Graphs were constructed to show the frontier at which factorisation becomes feasible on the 2-dimensional plane of possible bound choices. These graphs were then compared to the theoretical asymptotically optimal bound choice.<br><br>The project also compared alternative factorisation methods, both exponential and sub-exponential, for a range of instances, and examined the complementary problem of primality testing and primality proving; both probabilistic and deterministic methods were considered.<br><br>This field is central to secure communications in the digital age and this project tackles some of the most dynamic and influential questions of modern mathematics. 
________________________________________
2012 - MA005 
THE PROBABILITY OF GENERATING THE SYMMETRIC GROUP WITH A COMMUTATOR CONDITION
Raman A. Birulia
School No. 41, Minsk, Minsk, BELARUS

In 1892, Netto conjectured that almost all pairs of permutations will generate the symmetric or the alternating group. 76 years passed before the theorem was finally proved by Dixon. This paper investigates a refinement of Netto's conjecture. <br><br> Let B(n) be the set of pairs of permutations from the symmetric group of degree n with a 3-cycle commutator, and let A(n) be the set of those pairs which generate the symmetric or the alternating group of degree n. I find effective formulas for calculating the cardinals of both sets. More precisely, I show that #B(n)/n! is a discrete convolution of the partition function and a linear combination of divisor functions, while #A(n)/n! is the product of a polynomial and Jordan's totient function. In particular, it follows that the probability that a pair of random permutations with a 3-cycle commutator generates the symmetric or the alternating group of degree n tends to zero as n tends to infinity, which makes a surprising contrast with Dixon's classical result. In addition to solving the original problem, I obtained several applications connected with Diophantine equations and lattices in RxR.<br><br> Key elements of my proofs are Jordan's theorem from the 19th century, a formula by Ramanujan from the 20th century and a technique of square-tiled surfaces developed by French mathematicians Lelièvre and Royer in the beginning of the 21st century. This paper uses and highlights elegant connections between algebra, geometry, and number theory. 

Awards won at the 2012 ISEF
Second Award of $500 - American Mathematical Society
________________________________________
2013 - MA005 
APPLICATIONS OF DIRICHLET SERIES
Asbjorn Christian Nordentoft
Aurehoj Gymnasium, Gentofte, DENMARK

The aim of the research project is to investigate the connection between mathematical analysis and elementary number theory. It is fascinating that the tools of analysis can be used to prove number theoretical theorem, because the two branches of mathematics appear very different. The focus of the research is the analytic concept of Dirichlet series, their properties and applications in number theory. The main contribution of the project is the proof of a theorem, which links the analytic behavior of some infinite series to the infinitude of certain sets of primes. The theorem will show to have many interesting number theoretical applications, which are also presented.<br><br>The link between the fundamentally different perspectives of mathematics, analysis and number theory, is recognized as profound and reflects the beauty within mathematics. 

Awards won at the 2013 ISEF
Third Award of $250 - American Mathematical Society
Alternate for CERN trip - European Organization for Nuclear Research-CERN
Third Award of $1,000 - Mathematical Sciences
________________________________________
2011 - MA006 
THE MONTY HALL PROBLEM PROBABILITIES - THE LAW OF LARGE NUMBERS
Laura Marie Moore
Gallup Catholic High School, Gallup, NM

This project is being done to test the theory of probabilities by applying the mathematics of probabilities (the law of large numbers) to a popular game show LET’S MAKE A DEAL with the popular game show host Monty Hall. Thus, the name of the science project: The Monty Hall Problem. The purpose of the Monty Hall problem is to determine the probability when given the choice of a prize behind one of three doors. The Monty Hall Problem is, when the contestant has a choice between two doors after the third was eliminated, is the probability of choosing the correct door still 33.3% (1/3) or is it 50% (1/2) if you keep the original choice or door. <br><br>The hypothesis is, since the original project involved three doors, it should not matter if one were eliminated. Utilizing the law of large numbers, the probability of choosing the correct door would still be 33.3% (1/3) if you choose to keep your original choice or door. <br><br>The experiment was tested with a bowl of 300 marbles: 100 red for door one, 100 white for door two, and 100blue for door three. The marbles were pulled randomly to determine which door contained the prize and which door the contestant picked. There were 2000 trials. The experiment was done again with 3 marbles. The experiment was done on the computer using Microsoft Excel and the random number generator with one million trials. There were so many samples so as to prove the Law of Large Numbers.The results came out to approximately 33.3% correct if the contestant kept his original choice. 
________________________________________
2012 - MA006 
PARTIALLY CONJUGATE-PERMUTABLE SUBGROUPS AND THEIR APPLICATIONS
Viachaslau I. Murashka
Gymnasium No. 71, Gomel, BELARUS

Our project describes finite groups with conjugate-permutable conditions on their subgroups. In 1997, T. Foguel introduced the following concept: A subgroup H of a group G is called conjugate-permutable if H H^g= H^g H for all g in G. He showed that, for a finite group, any conjugate-permutable subgroup is subnormal. Some interesting results in further study of conjugate-permutable subgroups and its applications are obtained by Mingyao Xu and Qinhai Zhang (2005), Shirong Li and Zhongchuan Meng (2007), and others. <br><br> We introduce a novel concept of R-conjugate-permutability. Let R be a subset of the group G. A subgroup H of G is called R-conjugate-permutable if H H^x= H^x H for all x in R. We study properties of a finite group G with R-conjugate-permutable subgroups, when R is the Fitting subgroup F(G) or its generalizations F*(G) introduced by H. Bender in 1970 and F'(G) introduced by P. Shmid in 1972. We construct examples of groups with R-conjugate-permutable subgroups which are neither conjugate-permutable nor subnormal. Some theorems for describing finite nilpotent or supersolvable groups are obtained by using this concept. This project develops powerful methods based on particular properties of the groups F(G), F*(G) and F'(G). As a result, we give new strong criteria for a finite group to be nilpotent or supersolvable in terms of R-conjugate-permutable subgroups. 

Awards won at the 2012 ISEF
Third Award of $250 - American Mathematical Society
Fourth Award of $500 - Mathematical Sciences - Presented by Intel
________________________________________
2013 - MA007 
MATHEMATICAL EXPRESSION OF CPGS IN LEECH HEART INTERNEURONS, YEAR II
Siddharth Yarlagadda
Episcopal High School of Jacksonville, Jacksonville, FL

The purpose of this project is to develop an ordinary differential equation file of the nine-dimensional model of anti-phase bursting in the central pattern generator of a leech heart interneuron that will run in XPP-Auto, a software that solves differential equations. These equations have been previously developed and illustrate the bursting pattern of three coupled neurons. First, a file of the three-dimensional reduced model was written and tested for bursting activity in XPP. Once the file ran, three data points were obtained and used as one set of initial conditions for the 9D model. Then, the 9D model file was written. While writing the 9D file, several factors had to be accounted for: file structure, numeric method, time of integration, and time step (dt). Once the file ran in XPP, a visual test was administered first and then an analytical test in which three parameters, Ek, El, and VK2_shift, were altered while changes in activity were analyzed. Once the file showed normal behavioral patterns in response to these alterations, only then could the file be deemed an accurate representation of the 9D CPG model. The analyzed data eventually validated the .ode file; however, not to its full extent as the bifurcation parameter had to be set to zero for the file to function. This result is important because the model is a key to understanding the CPG, which is responsible for motor behaviors such as heartbeat, respiration, and locomotion. Having an XPP representation of the model could open many gateways in future experimentation and in understanding these behaviors. 
________________________________________
2011 - MA007 
HOW TO DISCOVER INFINITELY MANY THEOREMS RELATED TO THE PEDAL CIRCLE
Wayne Liao
National Taichung First Senior High School, Taichung City, CHINESE TAIPEI

Given a point and a triangle, the pedal circle is the unique circle determined by the perpendicular feet from the point to three sides.<br><br> By using the dynamic geometry software GSP, we have discovered chains of infinitely many theorems of the pedal circle:<br><br>Chain 1: For a point and a quadrilateral, the four pedal circles, formed by omitting each vertex in turn, are concurrent in a point, named the pedal point. For a point and a pentagon, the five pedal points similarly formed, are concyclic, named the pedal circle. This process can be extended repeatedly.<br><br>Chain 2: Given a point and a cyclic quadrilateral, the centers of the four pedal circles so formed, lie on a circle NC (3; 4). Given a point and a cyclic pentagon, the five centers of the corresponding circles NC (3; 4), formed by omitting each vertex in turn, lie on a circle NC (3; 5). This process can be extended repeatedly.<br><br>Chain 3: Given a point and a cyclic hexagon, the centers of the six pedal circles (with respect to pentagon) so formed, lie on a circle NC (5; 6). Given a point and a cyclic heptagon, the seven centers of the corresponding circles NC (5; 6), lie on a circle NC (5; 7). This process can be extended repeatedly.<br><br>Chain n: Established by repeating the process above by starting with cyclic (2n)-polygon. 
________________________________________
2012 - MA007 
CONTINUED FRACTIONS AND E
Frederik Benzing
Landesgymnasium fur Hochbegabte, Schwaebisch Gmuend, Baden-Wuerttemberg, GERMANY

123In mathematics, the field of continued fractions seems to be depleted. However, some proofs in this field can still be simplified. This work first generalizes a proof of the simple continued fraction expansion of e to an expansion of exp(x) where x is variable, then studies further expansions of exp(x) and investigates the relation between certain continued fractions and the eigenvalues of certain matrices. <br><br>The first gernilazation is based on basic knowledge about continued fractions and elementary calculus. By the means of the latter recurrence equations for a series of integrals are established. These equations can be related easily to a certain continued fraction and by determining the boundary value of the integral-series, one obtains the value of the continued fraction. The first conclusion of this work is that for 0<x<1 this method can be modified to get a simple continued fraction representation of exp(x). Furthermore, conditions to generalize the method to any real x are discussed. <br><br>The work also elaborates on additional integral-series and proves recurrence equations, from which new proofs for different continued fraction expansions of exp(x) are derived. Some of these expansions seem to be new.<br><br>These results can be used to approximate exp(x) but they are rather piece of pure mathematics.<br><br>Moreover, eigenvalues of certain matrices are expressed as continued fractions. For this step the link of both matrices and continued fractions to recursive sequences and elementary operations of Linear Algebra are used. Again, the result forms part of theoretical mathematics. 

Awards won at the 2012 ISEF
Certificate of Honorable Mention - American Mathematical Society
________________________________________
2012 - MA008 
1,400 YEARS OF CHANGE: LETTER FREQUENCIES IN THE IRISH LANGUAGE
Aoife Dorathea Gregg
Loreto College, Dublin 2, IRELAND

This project resulted from an investigation of issues in Irish language cryptography relating to letter frequency. An analysis of historical Irish language texts, across a range of fourteen centuries, showed distinct changes in frequencies of letters and short sequences of letters. This led to the hypothesis that letter frequency analysis could be used to date Irish language texts.<br><br>The project found strong patterns of change in Irish language letter frequencies. The most notable were a roughly linear increase over time in the frequency of the letter A, with a corresponding decrease in the frequency of I, and a similar increase and decrease in the frequencies of G and C respectively. The frequency of the séimhiú, a dot or H added to soften consonant pronunciation, changed in a more complex way.<br><br>Statistical techniques, including single variable and multivariate regression, were used to characterise the variations in letter frequencies. This formed the basis for a tool to estimate dates of documents based on letter frequency patterns. When tested, the dating tool developed was found to be very effective for dating Irish language documents.<br><br>Supplementary work with historical English language texts was also carried out, which showed that there have been significant changes in English letter frequency, and that these changes have the potential to be used to date English texts.<br><br>This project demonstrated that letter frequency analysis can make a useful contribution to analysis of historical texts. The dating tool developed complements already established linguistic dating techniques based on language usage. 
________________________________________
2013 - MA008 
OLYMPIC HOST-COUNTRY ADVANTAGE
Brandon Michael Williams
Belleview High School, Belleview, FL

The purpose of the project is to see if there is a Host Country advantage in the Olympic Games and if it is measurable. The importance of this discovery effects the sports world, the citizens of the host country and the world's view of the country. Go to www.olympic.org to acquire the medal data for every host country for the Summer and Winter Olympics. Boycott years are excluded so not to skew the data. In Excel create a worksheet for each host country and tabulate medals by Olympiad. Create a workbook for both the Summer and Winter Olympics. Create a summary sheet showing All Modern Olympic games on the rows including the status of games that are not included in the calculations. Key calculations for each Olympic host are X=Four years Before(% total medals) Independent variable, Y=Host Country Performance (% total medals) Dependent Variable, Ratio Home/Away, Prediction of Host Nation % of Total Medals, Host Nation Medals Expected, Standard Error From-To, Projected.<br><br>The first analysis is to review the Ratio Home/Away. The results mostly show values of greater than one meaning a Host Country Advantage.<br><br>The second analysis is to perform Linear Regression Analysis by using the independent variable X=4 years before and Y=Host Country Performance in host year. Time periods of data were tested to determine the Linear Regression Model with the highest R-squared value for best fit.<br><br>The line of best fit for the Summer Olympics was y=.827x+.0227 plus or minus .015 = percentage of total medals.<br><br>The line of best fit for the Winter Olympics was y=1.2584x+.0101 plus or minus .026 = percentage of total medals.<br><br>There is a slightly larger host country advantage in the Summer than the Winter Olympics.Out of all the Olympics 43 of the 46 gained in medal count from the prior games. 
________________________________________
2011 - MA008 
PERFECT TILING OF A RECTANGLE INTO RECTANGLES
Tzu-Hsuan Su
Taipei Municipal Jianguo High School, Taipei City, CHINESE TAIPEI

Throughout the project, all rectangles are assumed to have sides with integral lengths and all lie in horizontal/vertical directions. Here we study the combinatorial properties of dissections of a rectangle into smaller ones. Following Duijvestijn and Leeuw, a dissection is said to be perfect if all rectangles have different areas. A dissection is called even if the area of each rectangle is in even number.<br><br> Given an m by n rectangle, we proceed with three following problems: What is the maximum number f(m, n) of component rectangles in a perfect even dissection? What is the number N(m, n) of perfect even dissections? What is the maximum number g(m, n) of component rectangles in a perfect dissection?<br><br> Our main results are: The function f(m, n) is equal to the unique integer k such that the product mn lies in the close-open interval [k(k+1), (k+1)(k+2)). The function N(m, n) is less than or equal to the number of all possible ways to express the number [mn-f(m, n)(f(m, n)+1)]/2 as a sum of positive integers minus a small term. The small term is the number of ways to express mn as a sum of distinct even positive integers, and one of which contains at least one prime factor greater than m and n. The function g(m, n) is less than or equal to the unique integer k such that the product mn lies in the close-open interval [k(k+1)/2, (k+1)(k+2)/2). 

Awards won at the 2011 ISEF
Second Award of $500 - American Mathematical Society
________________________________________
2013 - MA009 
HOW DOES CHANGING THE REAL AND IMAGINARY PERTURBATION AFFECT THE MANDELBROT FRACTAL?
Pranav Ishaan Warman
Academy at the Lakes, Land O' Lakes, FL

The question that my project addressed was how changing the real and imaginary perturbation affected the Mandelbrot fractal. This experiment is of interest because fractals are the images of chaos; these images of chaos are used for predicting weather, creating fictional movie backgrounds, and forming artwork. If by changing the perturbation, which in turn changes the starting iteration, the fractal changes then there are several new fractals, which can be used, and tested against the graph of weather. Chaotic fractals are graphed by using an iterative method. It uses the equation z= z + c, when we iterate it the previous value of z that was found is used to create a new value of z. This iteration process is continued until the value either converges to a complex value or diverges. In my experiment I changed the starting iteration value from z=0 to z=1, z=2, until the integer set [-5,5]. I did this by using a common Fractal Generator known as Fractint, which is a common fractal graphing software. I then viewed a pictorial view of the fractals, observed, and recorded the changes. My data supported my hypothesis and the Mandelbrot fractal changed greatly by having an increased amount of divergent points and fewer and fewer convergent values. 
________________________________________
2011 - MA009 
MAGA METHOD TO FIND DETERMINANTS OF MATRICES
Michael G. Carmona-Soto
Petra Mercado Bougart, Humacao, PUERTO RICO

The determinant of a matrix is obtained from its elements. These determinants are useful in the solution lineal systems and the calculation of the inverse matrix and it has other multiple applications. A magic square is a square/grid which spaces of grids, whole numbers have been placed so that the addition of each row, column and diagonal is the same number, called magic constant. One of the most common methods to fill the magic square is the Loubére method, used only in odd order. The purpose of this investigation is to find a new method to find determinants of matrices. After evaluating the existing methods the following problem was proposed: Is there any method to find determinants of matrix using the magic square? The hypothesis was: Using the Loubére algorithms method to fill the magic square can be the development of a new method to find determinants of matrices 3x3. After observing algorithms the MaGa method is created, by which determinants of matrices 3x3 are found. After concluding this investigative assignment, it was observed that when the algorithms of the Loubére method to fill magic square, a new, easier, more organized, and systematic method can be developed. This investigation recommends the use of the MaGa method to find the determinants of matrices 3x3. It should also be used for the solution of equations with three variables. It also facilitates the solution of equations for students, making mathematics interesting and fun. 
________________________________________
2012 - MA009 
ON THE FINE CLASSIFICATION OF PERIODIC ORBITS OF CONTINUOUS ENDOMORPHISMS ON THE REAL LINE WITH APPLICATION IN CHAOS THEORY
Rashad Abdulla
West Shore Junior/Senior High School, Melbourne, FL

This project reveals a fine classification of periodic orbits of continuous endomorphisms on the real line based on their cyclic permutation and directed transition graphs. The new notion of minimal 2(2k+1)-orbits is introduced, meaning that the map has a 2(2k+1)-orbit, but no smaller odd periodic orbits and no orbits of period 2(2l+1), with l<k, where l and k are positive integers. By employing methods of symbolic dynamics and graph theory, it is proved that there are four types of minimal 2(2k+1)-orbits for continuous endomorphisms on the real line. Each type is fully characterized by the structure of the directed graph of transitions and cyclic permutations with accuracy up to inverse graphs. Application of the fine classification of 2(2k+1)-orbits in Chaos Theory reveals a new universal law of non-monotonic distribution of 2(2k+1) cycles within chaos. It is revealed that for all single parameter continuous unimodal endomorphisms, superstable 2(2k+1)-orbits are distributed according to the following diagram, <br><br>Chaos <= ... <= 30 <= 22 <= 26 <= 18 <= 22 <= 14 <= 18 <= 10 <= 14 <= 6<br><br>where the parameter decreases in the direction of the arrow. It is demonstrated that all the minimal 2(2k+1)-cycles arise as periodic windows within chaos following universal law. In fact, among two superstable cycles of the same period, the one on the left is Type I minimal according to introduced fine classification. Hence, theoretical results of the project on the fine classification of periodic orbits are relevant in chaos theory and open a way for numerous applications towards prediction of chaotic systems arising in many branches of science and engineering. 

Awards won at the 2012 ISEF
Certificate of Honorable Mention - American Mathematical Society
________________________________________
2011 - MA010 
DEVELOPING INTUITIONS FOR ESTABLISHING CONJECTURES CONCERNING ARBELOS'S AREA
Axel Manuel Pagan-Nieves
Francisco Morales High School, Naranjito, PUERTO RICO

The purpose of this research was to show that arbelos's area, limited by three semicircles, is equal to the area of the circle with diameter DC. Throughout this study, we examined the relationship between the lengths of the diameters of the semicircles and the diameter of the circle.<br><br>Our question was: is arbelos's area (the region delimited by three tangential semicircles with diameters AB, AD, and DB, where D belongs to the segment AB, and in the same plane) equal to the area of the circle with diameter CD? To answer our question, we formulated the following hypothesis: If the arbelos is the region delimited by three tangential semicircles with diameters AB, AD, and DB, where D belongs to the segment AB, and laying in the same plane then, the area of the arbelos is equal to the area of the circle with perpendicular diameter to AB in D.<br><br>To confirm our hypothesis, we used Pythagoras' theorem, the formula for the area of a circle, and algebraic algorithms to identify the area of the arbelos and the area of the circle.<br><br>Finally, we were able to conclude that, independently of the value (less than one) that represents the diameter to the left of the center of AB, the area of the arbelos is equal to the area of the circle. Our results, confirmed our initial hypothesis. 
________________________________________
2013 - MA010 
TRANSCRIPTION OF CETACEAN SONG USING A FREQUENCY AND TIME-DOMAIN ANALYSIS
Alice Ning-Xue Yan
Liberty High School, Hillsboro, OR

Out of 77 species of cetaceans, Megaptera novaeangliae (commonly known as the humpback whale) produces the most complex vocalizations, which are referred to as “songs”. The individual songs are within the frequency range of 8 hertz to 4 kilohertz, typically last anywhere from 5 to 30 minutes, and may be repeated continuously over the course of several hours. The songs consist of varied pitches—squeaks, moans, grunts, and roars.<br><br>While it is not possible to evaluate the meanings of the vocalizations, it is possible to transcribe the pitches of these songs to sheet music. Hence, the goal of this experiment was to transcribe samples of whale song to sheet music using the tunings of the equal tempered scale as a guide, as well as to determine the accuracy of the sheet music as compared to the original whale call. Samples of humpback whale song were obtained, and then software was used to track the fundemental frequencies of the whale songs, and pitches were assigned to each of the frequencies based off of the pre-defined tunings of the equal tempered scale. Sheet music was then created based off of the data collected from the pitch assignments, and the instrumental version of the whale call was compared to the original whale call.<br><br>While the project seems frivolous in nature, it may lead to a better understanding to the communications of the humpback whale. While we cannot communicate with whales, we can analyze their calls in given situations and perhaps decipher patterns in their calls. 
________________________________________
2012 - MA010 
INTEGRALS OF RATIONAL FUNCTIONS
Magda Lee Hlavacek
Saginaw Arts and Sciences Academy, Saginaw, MI

Some collections of functions are closed under differentiation and integration, and others are not. The ring of polynomials, for example, is closed under differentiation and integration. <br><br>Rational functions are functions that can be expressed as a ratio of polynomials. The derivative of a rational function is always a rational function. In some cases, the integral of a rational function is also a rational function. However, in other cases, the integral of an rational function is not a rational function. The goal is to find a decision procedure which can be used to tell whether a rational function integrates to a rational function.<br><br>Casework was used to break the problem into smaller parts. For each case, methods such as finding the square-free part of polynomials was used to determine whether the case applied to a given polynomial. Then, methods such as partial fraction decomposition were used to determine whether or not certain collections of rational functions integrate to rational functions. <br><br>It was found that one can determine whether rational functions in which the denominator only has one distinct linear factor integrate to rational functions. In the general case, no such decision procedure was found. However, necessary requirements were found that a rational function must satisfy in order integrate to a rational function. In order to integrate to a rational function, the degree of the numerator must be less than the degree of the denominator minus one, and the denominator cannot contain an unrepeated linear factor. 

Awards won at the 2012 ISEF
Award of three $1,000 U.S. Savings Bonds, a certificate of achievement and a gold medallion. - United States Army
________________________________________
2011 - MA011 
DISCRETE COMPUTATION OF CONTINOUS AREAS
Yamil Rosario- Vazquez
University Gardens High School, San Juan, PUERTO RICO

This research deals with the study of the discrete volume of polytopes. A polytope is the generalization of the bi-dimensional polygon concept, or polyhedron (tri-dimensional), to any other dimension. Geometrically, a polytope is a finite region of space enclosed by a finite number of hyper planes. Examples of tridimensional polytopes include crystals, boxes, tetrahedrals and convex objects. It is interesting to see how many problems in combinatorics, number theory, and many other areas of mathematics can be expressed by means of polytopes that exist in a Euclidian space. Computation of areas and volumes, which involve the application of integrals, is a topic that in theory has been highly studied. Moreover, the approximations of surface areas of common objects is complex. This is due to the fact that surfaces are irregular. Therefore to find the area of these through integration is not feasible. It is necessary to identify an alternate method to the use of integrals. This project focuses on the approximations of areas of linear functions in the interval [0,1]. A formula was developed using inductive reasoning by counting points in grids. This formula provides an approximation of the area below the curve of linear functions. It was verified that the expression: <br><br>[M(n/2)+1](n+1)/(n+1)^2 converges to the integral[0,1] of GsubM(x)dx is correct since the limit of n when it tends to the infinite of <br><br>[M(n/2)+1](n+1)/(n+1)^2=M/2. This formula can be used in Civil Engineering, Architecture, Cartography, Land Survey, and for general Mathematical knowledge. 
________________________________________
2013 - MA011 
EFFICIENT CHARACTERISTIC 3 GALOIS FIELD OPERATIONS FOR ELLIPTIC CURVE CRYPTOGRAPHIC APPLICATIONS
Vinay Sridhar Iyengar
Oregon Episcopal School, Portland, OR

Galois fields are a fundamental concept in abstract algebra and have a wide variety of applications in applied mathematics and computer science. Galois fields of characteristic 3, where the number of field elements is a power of 3, have a distinctive application in building high-security elliptic curve cryptosystems. However, they are not typically used because of their relative inefficiency in computing polynomial operations when compared to conventional prime or binary Galois fields. The purpose of this research was to design and implement characteristic 3 Galois field algorithms with greater overall efficiency than those presented in current literature, and to evaluate their applicability to elliptic curve cryptography. The algorithms designed were tested in a C++ program and using a mapping of field element logarithms, were able to simplify the operations of polynomial multiplication, division, cubing, and modular reduction to that of basic integer operations. In exchange for the initial precomputation and storage costs, the algorithms designed significantly outperformed the best characteristic 3 algorithms presented in literature. Furthermore, they performed elliptic curve scalar multiplication with an efficiency comparable to prime Galois fields and showed a distinct applicability to elliptic curve cryptosystems with constrained environments or large-scale storage potential. In conclusion, this research presents a novel method of optimizing the performance of characteristic 3 Galois fields and has major implications for the field of elliptic curve cryptography. 

Awards won at the 2013 ISEF
Full Tuition Presidential Scholarship - Florida Institute of Technology
Trip to the EU Contest. - European Union Contest for Young Scientists
Award to Travel to Trento, Italy to participate in summer school "Web Valley" - Fondazione Bruno Kessler
________________________________________
2012 - MA011 
SMALL GEOMETRIC PROGRESSIONS MODULO N FOR DETERMINISTIC POLYNOMIAL SELECTION
Aishwarya A. Vardhana
Jesuit High School, Portland, OR

In this project we present a solution to an unsolved problem known as ”Small Geometric Progressions Modulo N” proposed by Dr. Peter L. Montgomery in 1993. Montgomery’s problem postulates the creation of two irreducible cubic polynomials with properties ideal for the success (i.e. factorization)<br><br>of the General Number Field Sieve (GNFS); asymptotically the fastest algorithm for factoring a large integer N with no small prime factors, such as an RSA modulus. The first step of the GNFS, known polynomial selection, sieves through<br><br>random polynomials to find one or two polynomials with decent yield. We propose a deterministic method for creating two polynomials with ideal properties such as small rational coefficients, a common root m, and no common factor. These polys will produce the most smooth numbers (i.e. numbers with small prime factors) in the shortest amount of time thereby reducing the overall computation time of the GNFS. Using higher level modular arithmetic, Linear Algebra, and Number theory we were successful in solving the Montgomery's problem. The solution is a deterministic algorithm in polynomial time, much more efficient and flexible in comparison to the traditionally used Kleinjung's method. 

Awards won at the 2012 ISEF
Intel ISEF Best of Category Award of $5,000 for Top First Place Winner - Mathematical Sciences - Presented by Intel
________________________________________
2013 - MA011 
EFFICIENT CHARACTERISTIC 3 GALOIS FIELD OPERATIONS FOR ELLIPTIC CURVE CRYPTOGRAPHIC APPLICATIONS
Vinay Sridhar Iyengar
Oregon Episcopal School, Portland, OR

Galois fields are a fundamental concept in abstract algebra and have a wide variety of applications in applied mathematics and computer science. Galois fields of characteristic 3, where the number of field elements is a power of 3, have a distinctive application in building high-security elliptic curve cryptosystems. However, they are not typically used because of their relative inefficiency in computing polynomial operations when compared to conventional prime or binary Galois fields. The purpose of this research was to design and implement characteristic 3 Galois field algorithms with greater overall efficiency than those presented in current literature, and to evaluate their applicability to elliptic curve cryptography. The algorithms designed were tested in a C++ program and using a mapping of field element logarithms, were able to simplify the operations of polynomial multiplication, division, cubing, and modular reduction to that of basic integer operations. In exchange for the initial precomputation and storage costs, the algorithms designed significantly outperformed the best characteristic 3 algorithms presented in literature. Furthermore, they performed elliptic curve scalar multiplication with an efficiency comparable to prime Galois fields and showed a distinct applicability to elliptic curve cryptosystems with constrained environments or large-scale storage potential. In conclusion, this research presents a novel method of optimizing the performance of characteristic 3 Galois fields and has major implications for the field of elliptic curve cryptography. 

Awards won at the 2013 ISEF
Intel ISEF Best of Category Award of $5,000 - Mathematical Sciences
________________________________________
2012 - MA011 
SMALL GEOMETRIC PROGRESSIONS MODULO N FOR DETERMINISTIC POLYNOMIAL SELECTION
Aishwarya A. Vardhana
Jesuit High School, Portland, OR

In this project we present a solution to an unsolved problem known as ”Small Geometric Progressions Modulo N” proposed by Dr. Peter L. Montgomery in 1993. Montgomery’s problem postulates the creation of two irreducible cubic polynomials with properties ideal for the success (i.e. factorization)<br><br>of the General Number Field Sieve (GNFS); asymptotically the fastest algorithm for factoring a large integer N with no small prime factors, such as an RSA modulus. The first step of the GNFS, known polynomial selection, sieves through<br><br>random polynomials to find one or two polynomials with decent yield. We propose a deterministic method for creating two polynomials with ideal properties such as small rational coefficients, a common root m, and no common factor. These polys will produce the most smooth numbers (i.e. numbers with small prime factors) in the shortest amount of time thereby reducing the overall computation time of the GNFS. Using higher level modular arithmetic, Linear Algebra, and Number theory we were successful in solving the Montgomery's problem. The solution is a deterministic algorithm in polynomial time, much more efficient and flexible in comparison to the traditionally used Kleinjung's method. 

Awards won at the 2012 ISEF
First Award of $3,000 - Mathematical Sciences - Presented by Intel
________________________________________
2013 - MA011 
EFFICIENT CHARACTERISTIC 3 GALOIS FIELD OPERATIONS FOR ELLIPTIC CURVE CRYPTOGRAPHIC APPLICATIONS
Vinay Sridhar Iyengar
Oregon Episcopal School, Portland, OR

Galois fields are a fundamental concept in abstract algebra and have a wide variety of applications in applied mathematics and computer science. Galois fields of characteristic 3, where the number of field elements is a power of 3, have a distinctive application in building high-security elliptic curve cryptosystems. However, they are not typically used because of their relative inefficiency in computing polynomial operations when compared to conventional prime or binary Galois fields. The purpose of this research was to design and implement characteristic 3 Galois field algorithms with greater overall efficiency than those presented in current literature, and to evaluate their applicability to elliptic curve cryptography. The algorithms designed were tested in a C++ program and using a mapping of field element logarithms, were able to simplify the operations of polynomial multiplication, division, cubing, and modular reduction to that of basic integer operations. In exchange for the initial precomputation and storage costs, the algorithms designed significantly outperformed the best characteristic 3 algorithms presented in literature. Furthermore, they performed elliptic curve scalar multiplication with an efficiency comparable to prime Galois fields and showed a distinct applicability to elliptic curve cryptosystems with constrained environments or large-scale storage potential. In conclusion, this research presents a novel method of optimizing the performance of characteristic 3 Galois fields and has major implications for the field of elliptic curve cryptography. 

Awards won at the 2013 ISEF
First Award of $3,000 - Mathematical Sciences
________________________________________
2012 - MA011 
SMALL GEOMETRIC PROGRESSIONS MODULO N FOR DETERMINISTIC POLYNOMIAL SELECTION
Aishwarya A. Vardhana
Jesuit High School, Portland, OR

In this project we present a solution to an unsolved problem known as ”Small Geometric Progressions Modulo N” proposed by Dr. Peter L. Montgomery in 1993. Montgomery’s problem postulates the creation of two irreducible cubic polynomials with properties ideal for the success (i.e. factorization)<br><br>of the General Number Field Sieve (GNFS); asymptotically the fastest algorithm for factoring a large integer N with no small prime factors, such as an RSA modulus. The first step of the GNFS, known polynomial selection, sieves through<br><br>random polynomials to find one or two polynomials with decent yield. We propose a deterministic method for creating two polynomials with ideal properties such as small rational coefficients, a common root m, and no common factor. These polys will produce the most smooth numbers (i.e. numbers with small prime factors) in the shortest amount of time thereby reducing the overall computation time of the GNFS. Using higher level modular arithmetic, Linear Algebra, and Number theory we were successful in solving the Montgomery's problem. The solution is a deterministic algorithm in polynomial time, much more efficient and flexible in comparison to the traditionally used Kleinjung's method. 

Awards won at the 2012 ISEF
Mu Alpha Theta will award up to three awards for a total of up to $4500. First award minimum of $1500.00 - Mu Alpha Theta, National High School and Two-Year College Mathematics Honor Society
________________________________________
2013 - MA011 
EFFICIENT CHARACTERISTIC 3 GALOIS FIELD OPERATIONS FOR ELLIPTIC CURVE CRYPTOGRAPHIC APPLICATIONS
Vinay Sridhar Iyengar
Oregon Episcopal School, Portland, OR

Galois fields are a fundamental concept in abstract algebra and have a wide variety of applications in applied mathematics and computer science. Galois fields of characteristic 3, where the number of field elements is a power of 3, have a distinctive application in building high-security elliptic curve cryptosystems. However, they are not typically used because of their relative inefficiency in computing polynomial operations when compared to conventional prime or binary Galois fields. The purpose of this research was to design and implement characteristic 3 Galois field algorithms with greater overall efficiency than those presented in current literature, and to evaluate their applicability to elliptic curve cryptography. The algorithms designed were tested in a C++ program and using a mapping of field element logarithms, were able to simplify the operations of polynomial multiplication, division, cubing, and modular reduction to that of basic integer operations. In exchange for the initial precomputation and storage costs, the algorithms designed significantly outperformed the best characteristic 3 algorithms presented in literature. Furthermore, they performed elliptic curve scalar multiplication with an efficiency comparable to prime Galois fields and showed a distinct applicability to elliptic curve cryptosystems with constrained environments or large-scale storage potential. In conclusion, this research presents a novel method of optimizing the performance of characteristic 3 Galois fields and has major implications for the field of elliptic curve cryptography. 

Awards won at the 2013 ISEF
First Award of $3,000 - Mu Alpha Theta, National High School and Two-Year College Mathematics Honor Society
________________________________________
2012 - MA011 
SMALL GEOMETRIC PROGRESSIONS MODULO N FOR DETERMINISTIC POLYNOMIAL SELECTION
Aishwarya A. Vardhana
Jesuit High School, Portland, OR

In this project we present a solution to an unsolved problem known as ”Small Geometric Progressions Modulo N” proposed by Dr. Peter L. Montgomery in 1993. Montgomery’s problem postulates the creation of two irreducible cubic polynomials with properties ideal for the success (i.e. factorization)<br><br>of the General Number Field Sieve (GNFS); asymptotically the fastest algorithm for factoring a large integer N with no small prime factors, such as an RSA modulus. The first step of the GNFS, known polynomial selection, sieves through<br><br>random polynomials to find one or two polynomials with decent yield. We propose a deterministic method for creating two polynomials with ideal properties such as small rational coefficients, a common root m, and no common factor. These polys will produce the most smooth numbers (i.e. numbers with small prime factors) in the shortest amount of time thereby reducing the overall computation time of the GNFS. Using higher level modular arithmetic, Linear Algebra, and Number theory we were successful in solving the Montgomery's problem. The solution is a deterministic algorithm in polynomial time, much more efficient and flexible in comparison to the traditionally used Kleinjung's method. 

Awards won at the 2012 ISEF
Alternate for trip - American Committee for the Weizmann Institute of Science
________________________________________
2013 - MA012 
CLASSIFICATION OF SOME FUSION CATEGORIES OF RANK 4
Hannah Kerner Larson
South Eugene High School, Eugene, OR

Fusion categories are fundamental mathematical objects that appear in many areas of math, physics, and computer science. The classification of fusion categories is a difficult, open problem of current interest in mathematics. This project makes progress towards solving this problem by studying the next unsolved case of smallest rank.<br><br> One way to approach the classification of fusion categories is to study related objects called fusion rings and determine which fusion rings can be associated with fusion categories. A fusion ring is a ring with a fixed basis such that the structure constants are in Z+ and some duality constraints are satisfied. An isomorphism of a fusion ring with the Grothendiek ring of a fusion category is called a categorification.<br><br> A fusion ring of rank four must have either 2 or 4 self-dual basis elements. I prove that out of infinitely many fusion rings of rank 4 with 2 self-dual elements, at most 7 admit categorification. 

Awards won at the 2013 ISEF
Second Award of $500 - American Mathematical Society
________________________________________
2012 - MA012 
GRAPH THEORY AND LOCALITY SENSITIVE HASHING FOR DICOM IMAGE ANALYSIS
Markus Robert Woltjer
Wilsonville High School, Wilsonville, OR

Medical imaging and data-basing is a very common process, and requires many software resources on the part of medical corporations. Databases are not particularly organized, and this can give cause problems when searching for and retrieving specific images. The goal of this project is to find a mathematical scheme based on a hash function that is sensitive to similarity of images. While the problem sounds simple, computers need complex processes for tasks that humans find intuitive and easy. Above that, medical images in general have characteristics that make a hash function much more complicated. The three most prominent are invariance to translation, magnification, and rotation. To overcome these, the function is topological in nature—ignorant to the basic properties of an image. The simple properties that computers use to encode images with pixels are operated so that the output reflects the topological nature of an image specifically. Near-identical organs or the ones with the same irregularities are grouped (having a similar hash output) regardless of the way they were photographed. To compare graphs (of vertices and edges) topologically, DICOM images are converted using edge detection and other image simplification techniques. A query image is then used to retrieve similar images from the database by finding the images with the nearest hash values. The hash function used in this project is a combination of length ratios and angles on the centroid. Graph theory and topology are ideal for the task as well as being easy on computer storage and run time. 

Awards won at the 2012 ISEF
Third Award of $1,000 - Mathematical Sciences - Presented by Intel
________________________________________
2013 - MA012 
CLASSIFICATION OF SOME FUSION CATEGORIES OF RANK 4
Hannah Kerner Larson
South Eugene High School, Eugene, OR

Fusion categories are fundamental mathematical objects that appear in many areas of math, physics, and computer science. The classification of fusion categories is a difficult, open problem of current interest in mathematics. This project makes progress towards solving this problem by studying the next unsolved case of smallest rank.<br><br> One way to approach the classification of fusion categories is to study related objects called fusion rings and determine which fusion rings can be associated with fusion categories. A fusion ring is a ring with a fixed basis such that the structure constants are in Z+ and some duality constraints are satisfied. An isomorphism of a fusion ring with the Grothendiek ring of a fusion category is called a categorification.<br><br> A fusion ring of rank four must have either 2 or 4 self-dual basis elements. I prove that out of infinitely many fusion rings of rank 4 with 2 self-dual elements, at most 7 admit categorification. 

Awards won at the 2013 ISEF
Fourth Award of $500 - Mathematical Sciences
________________________________________
2012 - MA012 
GRAPH THEORY AND LOCALITY SENSITIVE HASHING FOR DICOM IMAGE ANALYSIS
Markus Robert Woltjer
Wilsonville High School, Wilsonville, OR

Medical imaging and data-basing is a very common process, and requires many software resources on the part of medical corporations. Databases are not particularly organized, and this can give cause problems when searching for and retrieving specific images. The goal of this project is to find a mathematical scheme based on a hash function that is sensitive to similarity of images. While the problem sounds simple, computers need complex processes for tasks that humans find intuitive and easy. Above that, medical images in general have characteristics that make a hash function much more complicated. The three most prominent are invariance to translation, magnification, and rotation. To overcome these, the function is topological in nature—ignorant to the basic properties of an image. The simple properties that computers use to encode images with pixels are operated so that the output reflects the topological nature of an image specifically. Near-identical organs or the ones with the same irregularities are grouped (having a similar hash output) regardless of the way they were photographed. To compare graphs (of vertices and edges) topologically, DICOM images are converted using edge detection and other image simplification techniques. A query image is then used to retrieve similar images from the database by finding the images with the nearest hash values. The hash function used in this project is a combination of length ratios and angles on the centroid. Graph theory and topology are ideal for the task as well as being easy on computer storage and run time. 

Awards won at the 2012 ISEF
Mu Alpha Theta will award up to three awards for a total of up to $4500. First award minimum of $1500.00 - Mu Alpha Theta, National High School and Two-Year College Mathematics Honor Society
________________________________________
2013 - MA012 
CLASSIFICATION OF SOME FUSION CATEGORIES OF RANK 4
Hannah Kerner Larson
South Eugene High School, Eugene, OR

Fusion categories are fundamental mathematical objects that appear in many areas of math, physics, and computer science. The classification of fusion categories is a difficult, open problem of current interest in mathematics. This project makes progress towards solving this problem by studying the next unsolved case of smallest rank.<br><br> One way to approach the classification of fusion categories is to study related objects called fusion rings and determine which fusion rings can be associated with fusion categories. A fusion ring is a ring with a fixed basis such that the structure constants are in Z+ and some duality constraints are satisfied. An isomorphism of a fusion ring with the Grothendiek ring of a fusion category is called a categorification.<br><br> A fusion ring of rank four must have either 2 or 4 self-dual basis elements. I prove that out of infinitely many fusion rings of rank 4 with 2 self-dual elements, at most 7 admit categorification. 

Awards won at the 2013 ISEF
Second Award of $2,000 - Mu Alpha Theta, National High School and Two-Year College Mathematics Honor Society
________________________________________
2012 - MA012 
GRAPH THEORY AND LOCALITY SENSITIVE HASHING FOR DICOM IMAGE ANALYSIS
Markus Robert Woltjer
Wilsonville High School, Wilsonville, OR

Medical imaging and data-basing is a very common process, and requires many software resources on the part of medical corporations. Databases are not particularly organized, and this can give cause problems when searching for and retrieving specific images. The goal of this project is to find a mathematical scheme based on a hash function that is sensitive to similarity of images. While the problem sounds simple, computers need complex processes for tasks that humans find intuitive and easy. Above that, medical images in general have characteristics that make a hash function much more complicated. The three most prominent are invariance to translation, magnification, and rotation. To overcome these, the function is topological in nature—ignorant to the basic properties of an image. The simple properties that computers use to encode images with pixels are operated so that the output reflects the topological nature of an image specifically. Near-identical organs or the ones with the same irregularities are grouped (having a similar hash output) regardless of the way they were photographed. To compare graphs (of vertices and edges) topologically, DICOM images are converted using edge detection and other image simplification techniques. A query image is then used to retrieve similar images from the database by finding the images with the nearest hash values. The hash function used in this project is a combination of length ratios and angles on the centroid. Graph theory and topology are ideal for the task as well as being easy on computer storage and run time. 

Awards won at the 2012 ISEF
Award scholarship of $5,000 - Oregon Institute of Technology
________________________________________
2013 - MA012 
CLASSIFICATION OF SOME FUSION CATEGORIES OF RANK 4
Hannah Kerner Larson
South Eugene High School, Eugene, OR

Fusion categories are fundamental mathematical objects that appear in many areas of math, physics, and computer science. The classification of fusion categories is a difficult, open problem of current interest in mathematics. This project makes progress towards solving this problem by studying the next unsolved case of smallest rank.<br><br> One way to approach the classification of fusion categories is to study related objects called fusion rings and determine which fusion rings can be associated with fusion categories. A fusion ring is a ring with a fixed basis such that the structure constants are in Z+ and some duality constraints are satisfied. An isomorphism of a fusion ring with the Grothendiek ring of a fusion category is called a categorification.<br><br> A fusion ring of rank four must have either 2 or 4 self-dual basis elements. I prove that out of infinitely many fusion rings of rank 4 with 2 self-dual elements, at most 7 admit categorification. 

Awards won at the 2013 ISEF
Each winning project will receive $3,000 in shares of UTC common stock. - United Technologies Corporation
Alternate for trip - American Committee for the Weizmann Institute of Science
________________________________________
2011 - MA013 
AN EXPANSION ON ANALYSIS OF KASNER POLYGONS GENERATED BY ITERATIVE PROCESSES
Minsuk Kim
Paul VI Catholic High School, Fairfax, VA

Sequences of polygons generated by performing iterative processes on an initial polygon have been studied extensively. One of the most popular sequences is the one sometimes referred to as Kasner polygons. Given a polygon K, the first Kasner descendant K' of K is obtained by placing the vertices of K’ at the midpoints of the edges of K. <br><br>More generally, for any fixed m in (0, 1) one may define a sequence of polygons {K_t} such that t is greater thab 0¸ where each polygon K_t is obtained by dividing every edge of K_t-1 into the ratio m: (1 - m) in the counterclockwise (or clockwise) direction and taking these division points to be the vertices of K_t.<br><br>We study the following problem<br><br>Let m be a fixed number in (0, 1) and let n be a fixed integer greater than 3. Further, let K be a convex n-gon and denote by K', the first m-Kasner descendant of K. What can be said about the ratio between the area of K' and the area of K, when K varies in the class of convex n-gons?<br><br>We provide a partial answer to this question.<br><br>Theorem.<br><br>If K is a triangle <br><br> then K'/K = 1 - 3m(1 - m).<br><br>If K is a quadrilateral <br><br> then K'/K = 1 - 2m(1 - m).<br><br>If K is a pentagon <br><br> then 1 - 2m(1 - m) < K'/K < 1 - m(1 - m).<br><br>If K is a n-gon has more than six sides <br><br> then 1 - 2m(1 - m) < K'/K < 1. 
________________________________________
2012 - MA013 
FINDING A WINNING STRATEGY IN THE GAME "CHOMP"
Nurbek Manbetuly Kungozhin
Republican Specialized School in Physics and Math, Almaty, KAZAKHSTAN

Experiment technigue: Analayzing results of the programmwich writes out all advantageous positions for the size of a broad 3*120, the following Hypotesis is put forward. Hypotesis: Advantageous positions have certain law. <br><br>Novelty of research: 1.the big class of advantageous positions for the size of broad 3*n, including with the unlimited sizes of lines is presented.2. The program of the control of game and the program-robot of search of advantageous positions are written 3. A winning strategy for the parallelepiped 2*2*n is found.<br><br>Independence degree: programs are independently written, all advantegeous positions are found and proved almost.<br><br>Results:<br><br>1.The graphic program of the control of observance of game rules is written.<br><br>2. For the limited size of a parallelepiped the program-robot of search of advantageous positions is written<br><br>3/ For the finding of the general advantageous strategy not enough usual the analysis from the end search of advantageous positions as steady law even is not traced at the big review of advantageous positions. 
________________________________________
2013 - MA013 
ON THE FINE CLASSIFICATION OF PERIODIC ORBITS OF CONTINUOUS ENDOMORPHISMS ON THE REAL LINE WITH APPLICATION IN CHAOS THEORY, YEAR II
Rashad Abdulla
West Shore Junior/Senior High School, Melbourne, FL

The project solves the conjecture on the fine classification of minimal orbits of continuous endomorphisms on the real line based on their cyclic permutations and digraphs. It is proved that for minimal 4(2k+1) orbits, there are 64 possible types of digraphs with accuracy up to inverse graphs. The method of proof introduced in this project allows the generalization of the result to the case of the minimal 2^n*(2k+1) orbits. in this case, there are 2^(2^(n+1)-2) types of digraphs with accuracy up to inverse graphs. It is demonstrated that the first appearance of the 2^n*(2k+1) periodic windows within the chaotic regime in the bifurcation diagram of the one parameter family of logistic type unimodal continuous endomorphisms is always a minimal orbit with Type I digraph. The reason is hidden in the fact that the topological structure of a unimodal map with single maximum is equivalent to the structure of the Type I piecewise linear endomorphism produced by the minimal orbit. The project reveals that the first two appearances of all the 2^n*(2k+1) periodic windows with k greater than or equal to 3, as well as the first appearances of the 5(2^n) and 3(2^n) orbits while increasing in parameter, are distributed according to the following law:<br><br>...<=28<=36<=20<=28<=12<=...<=30<=22<=26<=18<=22<=14<=18<=10<=14<=6<=... <=11<=7<=9<=5<=7<=3<=...<br><br>where the parameter decreases from right to left. These results have a tremendous impact towards the prediction of chaotic dynamical systems, relevant in the analysis of weather prediction, population dynamics, and fluid mechanics. 
________________________________________
2013 - MA014 
RETROSPECTIVE STUDY TO ANALYZE PATIENTS’ RISK FACTORS TO PREDICT THE LIKELIHOOD OF DEVELOPING MULTIPLE SCLEROSIS
Madison Nicole Greco
Oviedo High School, Oviedo, FL

Multiple Sclerosis (MS) is a chronic, often disabling disease of the central nervous system (CNS), in which cells of the immune system attack the myelin sheath surrounding nerve fibers in the CNS.<br><br> The primary objective of this investigation was to use patient risk factors collected from patient information (including demographic information, patient history, family history, physical examination), and laboratory test results to predict their likelihood of developing MS. The secondary objective was to determine if computer models would be able to predict a patient’s likelihood of developing MS with at least 75% accuracy and which model is the most accurate.<br><br> After de-identified patient information was collected, the data was compared to national information for MS patients to create a patient profile, which did not differ significantly from the population. The data was split into a Training Set (60%) and an Evaluation Set (40%) which was tested using the Random Forest, Logistic, Neural Network (Multilayer Perceptron), and J48 algorithms as classifiers. Their correctly classified instances were 87.5%, 77.5%, 90%, and 92.5%, respectively. At 92.5%, the J48 algorithm had the highest percentage of correctly classified instances, in addition to the greatest kappa statistic (0.7541) and lowest root relative squared error (67.5364); indicating it was the most accurate algorithm.<br><br> Future applications could include early detection or diagnosis of MS. Through use of computer models to generate a probability for a patient developing the disease, one can definitively classify “low risk” or “high risk” patients; which could be used to aid existing medical opinions. 
________________________________________
2012 - MA014 
ON THE D-MODULAR RHO-LABELINGS OF THE UNION OF TWO CYCLES
Alexander Charles Su
University High School, Normal, IL

Purpose: To determine whether d-modular rho-labelings are possible for unions of two cycles, and show the labeling scheme if possible, where this labeling generates a starter for an infinite set of graphs.<br><br>Procedure: <br><br>1.Split the set of all cycles into divisions mod four, or group them according to their remainder when divided by four. All possible combinations result in 10 groups.<br><br>2.Classify by divisor d that determines what larger graph is decomposed by the union, which must divide twice the total number of edges in the union.<br><br>3.Finally, these divisors were divided into groups classified by how large the cycles were, or whether they were odd or even. <br><br>4.The cases were calculated individually, with many examples and then each group was generalized. This provided the desired labeling scheme for each required labeling.<br><br>5.By compiling all the possible cases in a generalized form, all possible d-modular rho-labelings have been shown.<br><br>Conclusion:<br><br>The d-modular rho-labeling of a union of two cycles is definitely possible, and all possible labelings were shown with a generalized form. The other cases that were not represented were either proven to be completely impossible due to restrictions on the host graph or were trivial because they are equivalent to previously solved labelings. These unions of cycles can be labeled to be set up as starters for a family with an infinite number of graphs. 
________________________________________
2011 - MA014 
EIGENVALUES OF THE ZAKHAROV-SHABAT SYSTEM
Will Gooding
Roanoke Valley Governor's School for Science and Technology, Roanoke, VA

The purpose of this experiment was to observe the existence and location of eigenvalues of the Zakharov-Shabat system when the chirp of the Gaussian type pulse was manipulated. The hypothesis was that if the chirp factor was greater than fifteen, then no eigenvalues would be supported by the system. <br><br> This objective was accomplished primarily using Mathematica software. The differential equations for the Zakharov-Shabat system were solved using the chirped Gaussian pulse, allowing the chirp factor to vary. The initial eigenvalues, present with no chirp, were then tracked until “knocked off” the imaginary axis, at which point they were tracked through the complex plane to a chirp of 50.<br><br> Initially, three eigenvalues were present with a chirp of zero, occurring at xi=3.75i (#1), 2.89i (#2) and 0.41i (#3). From there, eigenvalues #1 and #2 moved towards each other along the imaginary axis until they collided at (c,xi)=(1.186,3.25i). These eigenvalues would produce a soliton solution in the system up until c=18.50, at which point the eigenvalue hit the real axis. Eigenvalue #3 moved up the imaginary axis, until, at (c,xi)=(4.285,1.84i), the eigenvalue “turned around” and moved down the imaginary axis and was tracked up to a chirp of 50. This eigenvalue would produce a soliton solution in the system up until c=10.762.<br><br> Based on this, the most ideal case in a real world system would be the single eigenvalue case from a chirp of 10.762 to 18.50. 
________________________________________
2013 - MA015 
STRUCTURE OF THE SET OF PRIMITIVE PYTHAGOREAN TRIPLES OVER GAUSSIAN INTEGERS
Tomoko Tachibana
Tsuyama National College of Technology, Tsuyama, JAPAN

The purpose of this study is to show the structure of the set of all primitive Pythagorean triples over Gaussian integers (G-PPTs). In 1963, Barning showed that any primitive Pythagorean triple (PPT) is transformed into (3,4,5) after a finite number of transformations by a matrix M. I call (3,4,5) the core PPT and M a generator matrix.<br><br> <br><br>In this study of G-PPTs, I started with the following hypotheses: (1) for the set of all G-PPTs, there exists a generator matrix G, and (2) there exists only a single core C, in the set {G-PPTs}. First, I constructed G by using the parameter expression of G-PPTs which was proposed by Cross in 1986. Next, I checked the existence of the cores in the set {G-PPTs}. This numerical computation showed that there exist many cores in the set {G-PPTs}, contrary to hypothesis (2). In fact, for any G-PPT P, there are pairs (C,C') such that (1) P is first transformed into a G-PPT C, then (2) C is transformed into C', and (3) C' is transformed into C, for the case that P is not transformed into either (3,4,5) or (4i,3i,5i). I call (C,C') a relative C-pair. <br><br> <br><br>This result means that the set {G-PPTs} can be partitioned into subsets having relative C-pairs (C,C') and cores (3,4,5) and (4i,3i,5i) by using G. Moreover, I defined the potential U(P) for each G-PPT P and proved that if a pair (C,C') is a relative C-pair then U(C)=U(C'). 
________________________________________
2011 - MA015 
CUTTING AND COMBINING POLYHEDRONS AND FILLING SPACE USING ORIGAMI
Mari Kimura
Ritsumeikan Senior High School, Kyoto, JAPAN

Some studies, I have discovered that cubes and rhombic dodecahedrons are one of the void-filled solid. However, the definition of what void-filled polyhedrons hasn’t been clear. So I tried build new void-filled polyhedrons using origami and to analyze it for understanding the unsolved void-filled polyhedrons.<br><br>I considered the formation of a polyhedron from a sheet of origami paper and examined whether space can be filled in a new polyhedron created by combining origami-formed polyhedron. In this research, I decided to create polyhedrons which occupy space using three methods: (1) division, (2) cutting, and (3) combination. <br><br>(1)Here, division refers to dividing a polyhedron into a new solid so that the vertex, sides, or diagonal lines of the polyhedron before the division are shared. (2) Cutting refers to dividing a polyhedron at a desired point without passing through the vertex, etc. of the source polyhedron. (3) Combination refers to the formation of a new space-filling polyhedron by combining polyhedron formed by division or cutting. <br><br>To begin with, I made a pyramid with the sheet and used this element as a basic part. By using the element and trapezoid pyramids, I found out that it’s possible to make rhombic and trapezoidal dodecahedron.<br><br>Based on my findings, I concluded that we can make solid which can take up space using origami. In addition, some applications and extensions of the investigation seem to include the following: we may be able to make models for molecular biology as well as cardiovascular models using origami. 
________________________________________
2013 - MA015 
STRUCTURE OF THE SET OF PRIMITIVE PYTHAGOREAN TRIPLES OVER GAUSSIAN INTEGERS
Tomoko Tachibana
Tsuyama National College of Technology, Tsuyama, JAPAN

The purpose of this study is to show the structure of the set of all primitive Pythagorean triples over Gaussian integers (G-PPTs). In 1963, Barning showed that any primitive Pythagorean triple (PPT) is transformed into (3,4,5) after a finite number of transformations by a matrix M. I call (3,4,5) the core PPT and M a generator matrix.<br><br> <br><br>In this study of G-PPTs, I started with the following hypotheses: (1) for the set of all G-PPTs, there exists a generator matrix G, and (2) there exists only a single core C, in the set {G-PPTs}. First, I constructed G by using the parameter expression of G-PPTs which was proposed by Cross in 1986. Next, I checked the existence of the cores in the set {G-PPTs}. This numerical computation showed that there exist many cores in the set {G-PPTs}, contrary to hypothesis (2). In fact, for any G-PPT P, there are pairs (C,C') such that (1) P is first transformed into a G-PPT C, then (2) C is transformed into C', and (3) C' is transformed into C, for the case that P is not transformed into either (3,4,5) or (4i,3i,5i). I call (C,C') a relative C-pair. <br><br> <br><br>This result means that the set {G-PPTs} can be partitioned into subsets having relative C-pairs (C,C') and cores (3,4,5) and (4i,3i,5i) by using G. Moreover, I defined the potential U(P) for each G-PPT P and proved that if a pair (C,C') is a relative C-pair then U(C)=U(C'). 
________________________________________
2011 - MA015 
CUTTING AND COMBINING POLYHEDRONS AND FILLING SPACE USING ORIGAMI
Mari Kimura
Ritsumeikan Senior High School, Kyoto, JAPAN

Some studies, I have discovered that cubes and rhombic dodecahedrons are one of the void-filled solid. However, the definition of what void-filled polyhedrons hasn’t been clear. So I tried build new void-filled polyhedrons using origami and to analyze it for understanding the unsolved void-filled polyhedrons.<br><br>I considered the formation of a polyhedron from a sheet of origami paper and examined whether space can be filled in a new polyhedron created by combining origami-formed polyhedron. In this research, I decided to create polyhedrons which occupy space using three methods: (1) division, (2) cutting, and (3) combination. <br><br>(1)Here, division refers to dividing a polyhedron into a new solid so that the vertex, sides, or diagonal lines of the polyhedron before the division are shared. (2) Cutting refers to dividing a polyhedron at a desired point without passing through the vertex, etc. of the source polyhedron. (3) Combination refers to the formation of a new space-filling polyhedron by combining polyhedron formed by division or cutting. <br><br>To begin with, I made a pyramid with the sheet and used this element as a basic part. By using the element and trapezoid pyramids, I found out that it’s possible to make rhombic and trapezoidal dodecahedron.<br><br>Based on my findings, I concluded that we can make solid which can take up space using origami. In addition, some applications and extensions of the investigation seem to include the following: we may be able to make models for molecular biology as well as cardiovascular models using origami. 
________________________________________
2013 - MA015 
STRUCTURE OF THE SET OF PRIMITIVE PYTHAGOREAN TRIPLES OVER GAUSSIAN INTEGERS
Tomoko Tachibana
Tsuyama National College of Technology, Tsuyama, JAPAN

The purpose of this study is to show the structure of the set of all primitive Pythagorean triples over Gaussian integers (G-PPTs). In 1963, Barning showed that any primitive Pythagorean triple (PPT) is transformed into (3,4,5) after a finite number of transformations by a matrix M. I call (3,4,5) the core PPT and M a generator matrix.<br><br> <br><br>In this study of G-PPTs, I started with the following hypotheses: (1) for the set of all G-PPTs, there exists a generator matrix G, and (2) there exists only a single core C, in the set {G-PPTs}. First, I constructed G by using the parameter expression of G-PPTs which was proposed by Cross in 1986. Next, I checked the existence of the cores in the set {G-PPTs}. This numerical computation showed that there exist many cores in the set {G-PPTs}, contrary to hypothesis (2). In fact, for any G-PPT P, there are pairs (C,C') such that (1) P is first transformed into a G-PPT C, then (2) C is transformed into C', and (3) C' is transformed into C, for the case that P is not transformed into either (3,4,5) or (4i,3i,5i). I call (C,C') a relative C-pair. <br><br> <br><br>This result means that the set {G-PPTs} can be partitioned into subsets having relative C-pairs (C,C') and cores (3,4,5) and (4i,3i,5i) by using G. Moreover, I defined the potential U(P) for each G-PPT P and proved that if a pair (C,C') is a relative C-pair then U(C)=U(C'). 
________________________________________
2011 - MA015 
CUTTING AND COMBINING POLYHEDRONS AND FILLING SPACE USING ORIGAMI
Mari Kimura
Ritsumeikan Senior High School, Kyoto, JAPAN

Some studies, I have discovered that cubes and rhombic dodecahedrons are one of the void-filled solid. However, the definition of what void-filled polyhedrons hasn’t been clear. So I tried build new void-filled polyhedrons using origami and to analyze it for understanding the unsolved void-filled polyhedrons.<br><br>I considered the formation of a polyhedron from a sheet of origami paper and examined whether space can be filled in a new polyhedron created by combining origami-formed polyhedron. In this research, I decided to create polyhedrons which occupy space using three methods: (1) division, (2) cutting, and (3) combination. <br><br>(1)Here, division refers to dividing a polyhedron into a new solid so that the vertex, sides, or diagonal lines of the polyhedron before the division are shared. (2) Cutting refers to dividing a polyhedron at a desired point without passing through the vertex, etc. of the source polyhedron. (3) Combination refers to the formation of a new space-filling polyhedron by combining polyhedron formed by division or cutting. <br><br>To begin with, I made a pyramid with the sheet and used this element as a basic part. By using the element and trapezoid pyramids, I found out that it’s possible to make rhombic and trapezoidal dodecahedron.<br><br>Based on my findings, I concluded that we can make solid which can take up space using origami. In addition, some applications and extensions of the investigation seem to include the following: we may be able to make models for molecular biology as well as cardiovascular models using origami. 
________________________________________
2013 - MA015 
STRUCTURE OF THE SET OF PRIMITIVE PYTHAGOREAN TRIPLES OVER GAUSSIAN INTEGERS
Tomoko Tachibana
Tsuyama National College of Technology, Tsuyama, JAPAN

The purpose of this study is to show the structure of the set of all primitive Pythagorean triples over Gaussian integers (G-PPTs). In 1963, Barning showed that any primitive Pythagorean triple (PPT) is transformed into (3,4,5) after a finite number of transformations by a matrix M. I call (3,4,5) the core PPT and M a generator matrix.<br><br> <br><br>In this study of G-PPTs, I started with the following hypotheses: (1) for the set of all G-PPTs, there exists a generator matrix G, and (2) there exists only a single core C, in the set {G-PPTs}. First, I constructed G by using the parameter expression of G-PPTs which was proposed by Cross in 1986. Next, I checked the existence of the cores in the set {G-PPTs}. This numerical computation showed that there exist many cores in the set {G-PPTs}, contrary to hypothesis (2). In fact, for any G-PPT P, there are pairs (C,C') such that (1) P is first transformed into a G-PPT C, then (2) C is transformed into C', and (3) C' is transformed into C, for the case that P is not transformed into either (3,4,5) or (4i,3i,5i). I call (C,C') a relative C-pair. <br><br> <br><br>This result means that the set {G-PPTs} can be partitioned into subsets having relative C-pairs (C,C') and cores (3,4,5) and (4i,3i,5i) by using G. Moreover, I defined the potential U(P) for each G-PPT P and proved that if a pair (C,C') is a relative C-pair then U(C)=U(C'). 
________________________________________
2012 - MA015 
COMPLETING GRAPHS
Katherine Leigh Cordwell
Manzano High School, Albuquerque, NM

We develop a new method that applies abstract algebra to find the missing edges of an incomplete simple k-partite graph. Each graph may be thought of as a quotient ring, which is generated by taking the polynomial ring over the vertices of the graph and dividing out by the edge ideal, the ideal generated by the edges of the graph. The missing edges of the graph may then appear in the test ideal: if there are n minimal prime ideals associated to the edge ideal, the test ideal is found by taking the sum of all intersections of n-1 associated primes.<br><br> We explore the successes and limitations of this method, investigating various cases in which it does and does not work. We largely focus on bipartite graphs, with some generalizations to multipartite graphs. In particular, we prove that the method works when applied to any multipartite graph missing one edge between level X and level Y, as long as there are at least two vertices at each of levels X and Y. Additionally, we show that the method applies to any bipartite graph missing an arbitrary number of edges from one vertex. We also prove that, given a bipartite graph missing two distinct edges, the method adds in both missing edges as well as two extra edges. To better understand this result, we consider what happens when the method is applied to unions of complete graphs. 

Awards won at the 2012 ISEF
Second Award of $500 - American Mathematical Society
Second Award of $1,500 - Mathematical Sciences - Presented by Intel
________________________________________
2011 - MA016 
A FRACTAL ANALYSIS OF MUSICAL COMPLEXITY THROUGH TIME
Jeffery Brunson Holste
Mills E. Godwin High School, Henrico, VA

The relationship between the era of music and the fractal dimension of melody lines from the music of each era was analyzed. Twelve pieces - four selected from each of the three time periods Baroque, Classical, and Romantic - were studied at the eight measure level. After converting each melody line’s notes to frequencies (Hz), each piece was analyzed for its respective fractal dimension using the Grassberger-Procaccia algorithm. Using Microsoft Excel, the fractal dimension was calculated with music from the Baroque era having a mean dimension of 1.2372, music from the Classical era having a mean dimension of 1.0081, and music from the Romantic era having a mean dimension of .775. A t-test showed that the Baroque music’s mean fractal dimension was both significantly different from that of the Classical and the Romantic eras. However, the t-test did not indicate a statistically significant difference between music of the Classical era and that of the Romantic era. A linear regression of the year of publication or composition of each piece versus each respective piece's fractal dimension revealed a negative correlation with a slope significantly less than zero. This suggests a correlation between the era of music and its fractal dimension. Further studies should analyze additional pieces within each of these three musical eras as well as consider other eras than those examined here. 
________________________________________
2012 - MA016 
CHROMATIC NUMBERS OF FUNCTIGRAPHS AND THEIR EXTENSIONS - PHASE 2
George Qi
Texas Academy of Mathematics and Science, Denton, TX

My research examines the chromatic numbers of functigraphs and extensions of functigraphs. Functigraphs are generalizations of permutation graphs, which were introduced in 1967 by Chartrand and Harary. A functigraph, C(G,f), is obtained from a graph G and a function f: V(G) -> V(G') where G' is a disjoint copy of G. The chromatic number of a graph is the least number of colors needed to assign a color to each vertex of G such that no two adjacent vertices are given the same color. Recently, Chen et al. determined the bounds on the chromatic number.<br><br>I extend upon the results of Chen et al. by extending functigraphs to even more general, novel graphs that I have invented, thus making these graphs more applicable because they not only encompass the original graphs but include many more. I created three extensions of functigraphs, and in doing so, determined the bounds for each and proved all of them to be sharp.<br><br>First, I define F(G1,G2,f) to be obtained from a graph G1 and a function f: V(G1) -> V(G2), where G2 is an arbitrary graph. <br><br>Second, I define C(G,f,n) to be the graph in which each vertex of a graph G maps n edges to the vertices of its disjoint copy, G'. Therefore, this mapping is not a function. <br><br>Third, I combined the two above to create an even more generalized graph, which may be preferred in some cases over the others. I define F(G1,G2,f,n) to be the graph in which each vertex of a graph G1 maps n edges to the vertices of an arbitrary graph G2. <br><br>Additionally, I determined a greater number of functions than had previously been determined such that the chromatic number of the functigraph meets the lower bound. I also provided a more elegant proof for the chromatic number of C(Cn,f) and its bounds using the Greedy Algorithm. 
________________________________________
2013 - MA016 
RESOLVING AN OPEN PROBLEM RELATED TO FIGURATE NUMBERS BY PELL EQUATIONS
Yu-Fang Hsu
National Nanke International Experimental High School, Tainan, CHINESE TAIPEI

In this research we focus on the polygonal numbers and the centered polygonal numbers, the two most important classes of the figurate numbers. When the polygons are triangles, the corresponding sequence of triangular numbers (3-polygonal numbers) is {1, 3, 6, 10, 15,…}. When the polygons are squares, the corresponding sequence of square numbers (4-polygonal numbers) is {1, 4, 9, 16, 25, …}, and so forth. By solving a bivariate quadratic equation of integral coefficients either by the method of completing the square or with Pell equations, we are able to find the numbers that are simultaneously m-polygonal numbers and n-polygonal numbers. On the other hand, when the intersection of m-polygonal numbers and n-polygonal numbers forms a finite set, we are able to determine its cardinality. We also settle an open problem that the intersection of 3-polygonal numbers, 4-polygonal numbers, and 5-polygonal numbers is {1}. Moreover, when the center of the regular polygons are placed at a common dot, the sequence of the figurate numbers are called r-centered polygonal numbers. By analyzing the recursive formula of related Pell equations, we are able to characterize the intersection of r-centered polygonal numbers and s-centered polygonal numbers for arbitrary r and s. Finally, a geometric illustration for all properties is demonstrated. 

Awards won at the 2013 ISEF
Certificate of Honorable Mention - American Mathematical Society
Third Award of $1,000 - Mathematical Sciences
________________________________________
2013 - MA017 
SERIES INVOLVING THE RECIPROCAL OF POCHHAMMER'S SYMBOL
Bertrand Andrew Stone
Nicolet High School, Glendale, WI

In this project, the characteristics and applications of series involving an exponential term and the reciprocal of the shifted factorial are investigated. These series are characterized by behavior significantly different from that of series conventionally researched. The study of such series is motivated by the relatively rapid convergence of these series as an algorithmic consideration and by their relation to integrals with infinitely differentiable real-variable functions or analytic complex-variable functions and polynomial factors in the integrand. Several levels of generality are explored. First, a theorem is proven regarding some characteristics of abstract series of the form in question. Second, an abstract integral formula is presented, along with a criterion for the convergence of a remainder arising in partial sums of the integral formula. Third, a generalization of the relation between the integral and series involving the reciprocal of the shifted factorial to a complex integral over a piecewise differentiable curve is presented. Fourth, a criterion for the convergence of the remainder to zero is presented, depending upon Cauchy's estimate for analytic functions bounded on circles. Fifth, a formula bounding the remainder and an approximate for the tendency of the remainder are presented. Finally, an application to an integral of the form above is investigated. In summary, this project consists of a detailed investigation of series of the form described above including a consideration of the existence and rapidity of convergence of series representations for integrals, as well as applications. 
________________________________________
2012 - MA017 
MMSE VS. MCDR: TOOLS IN DETECTING ALZHEIMER'S DISEASE
Mackinzie J McDaniel
The Villages Charter High School, The Villages, FL

The purpose of this science fair experiment is to determine if the Mini Mental State Examination (MMSE) is more sensitive than the Modified Clinical Dementia Rating (mCDR) in detecting Alzheimer’s disease in patients of 65 years or older. In determining this, this researcher hopes to provide physicians and testing sites with up to date statistical proof as to which of these test are the best and most efficient to use. This researcher predicts that the MMSE will be more sensitive to Alzheimer’s. <br><br>To conduct this experiment, first data was collected. After receiving data, scores on each test were graphed against age, diagnosis, and gender. Analysis was made through t tests and correlations for each graph to determine significance. Record data and draw conclusions.<br><br>The results of this experiment showed that the graphs comparing the scores for both tests vs. age had slopes of zero. The results of all the graphs showed that neither tests were biased by age or gender. The t tests for Score vs. Diagnosis graphs for both tests showed that the test scores could predict with 95% confidence the diagnosis for MCI and AD within two standard deviations of the mean. In conclusion, this project showed that both the MMSE and mCDR are significant in detecting Alzheimer’s disease in patients of 65 years or older. 
________________________________________
2011 - MA017 
LINEARLY MANY FAULTS IN (N,K)-STAR GRAPHS
Allen Yuan
Detroit Country Day School, Beverly Hills, MI

Parallel computing network architectures based on star graphs were shown to have desirable properties with respect to fault tolerance – in particular, it was shown that when relatively large numbers of vertices are deleted, the network will stay mostly connected. However, star graphs suffer the problem of having a factorial number of vertices, resulting in large gaps in the possible vertex count of star graphs. Thus, it is important to find an alternative to the star graph for well-structured interconnection networks. In this paper, we explore the connectivity of one such alternative, the (n,k)-star graph, a generalization of the star graph which does not run into the same problem of vertex count. We show that the (n,k)-star has properties similar to that of the star graph by giving both asymptotic and precise results relating vertex deletion in (n,k)-star graphs to the size of the disconnected components of the graph. In addition to solving the original problem, the results in the paper are shown to immediately resolve several other issues of connectivity for (n,k)-star grap hs, establishing them as a viable structure for future parallel computing networks. 

Awards won at the 2011 ISEF
First Award of $3,000 - Mathematical Sciences - Presented by Intel
Award of three $1,000 U.S. Savings Bonds, a certificate of achievement and a gold medallion. - United States Army
First Award of $3,000 - Air Force Research Laboratory on behalf of the United States Air Force
Each winning project will receive $2,000 in shares of UTC common stock. - United Technologies Corporation
________________________________________
2013 - MA018 
ON THE SPLITTING OF MO(2) OVER THE STEENROD ALGEBRA
Maurice Olympic Shih
Laramie Senior High School, Laramie, WY

Proposed in 1987 by Franklin P. Peterson, the hit problem in algebraic topology aims to decompose elements of a graded module into linear combinations of lower degree elements acted on by operators called Steenrod squares. From this, the set of non-decomposable elements can be used to construct a minimal generating set. A related problem is to determine whether the cohomology of the Thom space MO(2) splits over the Steenrod algebra, which has a cohomology comprised of two variable symmetric polynomials. We must determine whether the cohomology can be split into direct summands based on the minimal generating set by studying cases with finite-dimensional subalgebras. A splitting is first constructed for each case based on one and two basis operations. For the cases of three and four, we conjecture that this is not possible based on analysis. From this result we conclude that MO(2) is not split over the Steenrod algebra. The motivation for this problem is to better understand the real unoriented cobordism spectrum MO. 
________________________________________
2011 - MA018 
CONVERGING LOGARITHMIC SPIRALS ON THE COMPLEX PLANE
Goutham Kapa
Troy High School, Troy, MI

The iteration of the logarithmic function on a complex number is rather complicated to calculate by hand; however, using computer programs to mathematically simplify the logarithmic operations gives us data that suggests that iterating the logarithmic function on any complex number will eventually reach a unique limit no matter what the initial inputted value is. My project revolves around giving a rigorous mathematical proof to explain why this convergence occurs. Manipulating common equations in Complex Analysis and applying the Mean Value Theorem on the complex plane shows not only that the limit does exist but also shows that continuously taking the logarithm of a number, real or complex, will showcase a function that converges to the limit at a constant rate. Furthermore, collecting data using mathematical software supports the proof with hard data. Graphing the data points on the complex plane, we see that the function creates a spiral that has interesting properties on its own. With these new constants we can find new equilibriums in areas applying complex logarithms such as quantum mechanics, computer science, and electrical engineering. Thus, this research gives an interesting proof for a unique mathematical phenomenon in which potential real-world application may lie. 
________________________________________
2012 - MA019 
DESIGNING LINKAGE WITH DECOMPOSITION
Hung-Chi Sun
National Hsinchu Commercial Vocational High School, Hsinchu City, CHINESE TAIPEI

Invented in 1864, the Peaucellier Inversor was the first planar linkage capable of transforming rotary motion into perfect straight line motion, and vice versa. If a camera moves along the “rotor” then the output of the inversor is observed to trace out an interesting curve satisfying r = cos(t) + a sec(t) in polar coordinates, the equation of Conchoids of de Sluze. Wishing to construct the curve-tracing animation of the linkage, two methods of vector decomposition are developed. Method 1: Decomposition through Rhombus. Method 2: Decomposition through Parallel Projections. The decompositions build the frame upon which the linkage is designed.<br><br> In each method the position vector is expressed as a sum of vectors of constant length. Method 1 is nonlinear and treats all Conchoids of de Sluze, including the Cissoid of Diocles, the Right Strophoid and the Trisectrix of Maclaurin as special cases. Method 2 is linear and treats all trochoids, including the Cardioid, the Nephroid, the Deltoid, the Astroid and the Roses as special cases.<br><br> Demonstration 1: Double generation property of trochoids. Two pairs of circles are shown to trace out the same curve, with each consisting of a fixed circle and a rolling circle with different ratios in radius. Demonstration 2: The formation, with the accompanying linkages and animations, of trochoids, Conchoids of de Sluze and lemniscates are shown.<br><br> All animations make use of Cabri II Plus for animation, ruler and compass constructions and the Geometer’s Sketchpad for black-box curve constructions. 
________________________________________
2013 - MA019 
INTERCALATES GALORE
Sarah Lee Shader
Laramie High School, Laramie, WY

This project investigates the existence of ubiquitous m by n Latin rectangles, which are Latin rectangles with the property that each cell is in at least one 2 by 2 Latin subsquare. The results of this fundamental research may aid in the understanding of intercalates in Latin squares, the use of intercalates to catalog various types of Latin squares, and in the study of critical sets (that is, specified cells that can be uniquely completed to a Latin square). A variety of techniques are developed to prove the existence of m by n ubiquitous Latin rectangles whenever m or n is even. In particular, Cayley tables of the dihedral group are shown to have many intercalates, and then modified to construct ubiquitous Latin rectangles of various dimensions. Quilting, or the repetition of a pattern of cells, is also used to construct families of ubiquitous Latin rectangles from smaller ubiquitous Latin rectangles. In addition, by modifying the Cayley tables of the dihedral groups, 2n by (2n+1) ubiquitous Latin rectangles are constructed for all n greater than 9. Using directed graphs and counting arguments, it is proven that a ubiquitous 3 by n Latinrectangle with n odd does not exist. 

Awards won at the 2013 ISEF
Certificate of Honorable Mention - American Mathematical Society
________________________________________
2011 - MA019 
ON THE SECOND EIGENVALUE AND EXPANSION OF BIPARTITE REGULAR GRAPHS
Wenyu Cao
Phillips Academy, Andover, MA

Expander graphs are graphs that exhibit the seemingly contradictory features of high connectivity and few edges. The construction of expander graphs is an area of research with several applications in error-correcting codes, derandomization, networking, complexity theory, and cryptography. I studied bipartite regular, or biregular, graphs. Biregular graphs are bipartite graphs with an additional symmetric property: any two vertices in the same vertex set have the same number of emanating edges. Specifically, I researched how biregular graphs behave as expander graphs and how biregular graphs with high expansion, or connectivity, can be constructed. My analysis relied on combinatorial graph theory to study walks taken on a biregular graph and algebraic graph theory to investigate the eigenvalues of biregular graphs. I also introduced probability theory to handle random biregular graphs. First, I proved an upper bound on the second eigenvalue of a random biregular graph. Since a known result states that a smaller second eigenvalue necessarily implies higher expansion, I was able to guarantee expansion for random biregular graphs with high probability through this bound. Additionally, I created a new model for random biregular graphs. This new model allows random biregular graphs to be generated computationally fast and provides mathematical structure for these random objects. With this model, I proved a better, and near optimal, bound on the second eigenvalue of a randomly generated biregular graph. Finally, I explicitly constructed a class of asymptotically optimal graphs. My results show that both random generation and my construction produce expander graphs with high expansion. 

Awards won at the 2011 ISEF
Third Award of $1,000 - Mathematical Sciences - Presented by Intel
Tuition Scholarship Award in the amount of $8,000 - Office of Naval Research on behalf of the United States Navy and Marine Corps
________________________________________
2012 - MA020 
DERIVING RAPIDLY-CONVERGING FORMULAS FOR THE GAMMA FUNCTION
Bertrand Andrew Stone
Nicolet High School, Glendale, WI

The gamma function, the real or complex generalization of the factorial, has been studied for centuries by many of the greatest mathematicians – among them Euler, Gauss, Weierstrass, Legendre, and Ramanujan. In this paper, derivations of rapidly-converging limit-sum and double- and triple- sum formulas for the gamma function are presented. These formulas are based upon Euler’s gamma integral. The formulas are consequences of applying Maclaurin series to the integral or using integration by parts backwards, deliberately creating an infinite recursion. In some results, limits are taken, and in others, the interval of integration is partitioned. Convergence of the formulas is depicted graphically in multiple ways. As an additional result, algorithms based upon some of these formulas are given. 
________________________________________
2013 - MA020 
A MARGINAL UTILITY-BASED ITERATIVE ALGORITHM FOR ESTABLISHING CARDINAL PREFERENCES
Patrick Hirsch Liscio
Niles Township West High School, Skokie, IL

Effective ranking systems are desirable in a wide variety of decision-making processes. Especially in sports, teams’ seasons are frequently decided, and millions of dollars change hands, based on ranking methods that are either arbitrary or ineffective. This algorithm seeks to create a more accurate ranking system capable of predicting future results, with an initial application to college football and possible future applications to other types of decisions. In order to accomplish this, a computer algorithm was programmed in Java code to rank college football teams over the course of a season. The model uses a regression analysis of the relationship between points and winning percentage in order to convert game scores into a meaningful ranking of teams. The algorithm was tested against Las Vegas point spreads in order to determine its effectiveness. This ultimately proved effective in predicting the scores of college bowl games. It was able to beat the spread in 56.6% of the games tested, which was well above the 52.4% needed to make a profit betting on games. However, more trials would be needed in order to determine whether these results are statistically significant. 
________________________________________
2011 - MA020 
PROPERTIES OF HAWKINS PRIMES
Aaron Lawrence Zweig
Randolph High School, Randolph, NJ

Prime numbers are the subject of many conjectures, but due to their irregular distribution it is very difficult to prove theorems regarding primes. The Sieve of Eratosthenes isolates prime numbers by removing multiples of surviving elements. This process may be randomized, known as the Hawkins random sieve. Beginning with the set of natural numbers excluding one, the sieve progresses by taking smallest value A, which for the first cycle is always 2, and using its reciprocal 1/A as the probability for which subsequent values are removed. Then, the smallest remaining value greater than A is assigned to B, whose reciprocal 1/B is used as the new probability with which to remove subsequent values. This process is repeated infinitely, and the non-deterministic set of remaining values is the set of Hawkins primes. Because Hawkins primes are generated in a similar manner to the way primes are isolated, they share many properties regarding distribution.<br><br>This project proves a probabilistic analogue of the Bunyakovsky conjecture. Using the set of all possible sets of Hawkins primes, it is shown that the set of all values produced by a non-constant polynomial will contain infinitely many Hawkins primes almost surely. 

Awards won at the 2011 ISEF
Certificate of Honorable Mention - American Mathematical Society
Third Award of $1,000 - Mathematical Sciences - Presented by Intel
________________________________________
2011 - MA021 
THE MATHEMATICS OF BREAST CANCER: A COMPARISON OF THE GOMPERTZ AND LOGISTIC EQUATIONS IN TERMS OF TUMOR CARRYING CAPACITY
Elise Victoria Meade
Sanford H. Calhoun High School, Merrick, NY

The Gompertz and logistic equations are exponential functions used to track the growth of breast cancer tumors. Each equation is set in terms of the maximum tumor volume or tumor carrying capacity. In this study two new equations derived from the aforementioned functions were created to solve for two variables within the original equations. These functions were also put in terms of the tumor carrying capacity. The extracted data measuring the initial and final volumes of 23 tumors were plugged into the newly derived equations. The new equations were expressed in terms of the tumor carrying capacity and were altered until the lowest variance for both beta (the relative growth rate) and alpha (the rate of decay of beta) variables was discovered. The same procedure was done for the variables in the logistic equation. The results were compiled graphically and differences between the two curves were determined. The points of inflection were found for both functions illustrating a symmetrical difference between them. The logistic equation more appropriately represented tumor growth rates. These derived equations may be able to predict future tumor growth rates. The results indicate that an intermediate volume reading would facilitate future studies. 
________________________________________
2012 - MA021 
AERIAL NAVIGATION: A MATHEMATICAL SYSTEM OF EQUATIONS CAPABLE OF NAVIGATING AN AERIAL DEVICE WITHOUT THE USE OF SATELLITES
Gerald Paul Lawlor
Notre Dame High School, Chattanooga, TN

What would happen if satellites were occluded by violent solar flares, shut-down due to cyber attacks, or physically demolished by intercontinental ballistic missiles? Without satellite communications, planes and other aerial devices would be incapable of flying because their only accurate navigation system is solely dependent on satellite communications. The focal point of this project was to successfully create a mathematical system of equations that has the capacity of accurately navigating an aerial device between two global points without the aid of satellite communications.<br><br>Using different types of mathematics (e.g. advanced algebra, plane and analytical geometry, trigonometry, discrete mathematics, and calculus), multiple formulas were developed that have the ability to calculate the variables, angles, and distances that are absolutely necessary for the operation of an independent aerial navigation system. Various specific formulas based on topological and geographical factors were then created by the substitution and representation of variables in the central angle formula. Every formula was arranged in a specific order to compose an overall system of equations that can successfully and accurately navigate aerial devices between two geographical points without the use of satellite communications.<br><br>In the future, this project could be used to program an independent aerial navigation system that does not rely on satellite communications. This would allow planes, drones, and missiles to successfully fly between two global points utilizing advanced geographical mappings from the National Geospatial Association. 

Awards won at the 2012 ISEF
Scholarship Award of $15,000 per year, renewable annually - Florida Institute of Technology
________________________________________
2013 - MA021 
KAPREKAR’S CONSTANT: A JOURNEY TO NEW BASES
Daniel Matan Hanover
John L. Miller Great Neck North High School, Great Neck, NY

The number 6174 has arisen as a famous solution to a problem of multiple steps. First, take any four-digit number which has at least two distinct digits. Then, rearrange the digits of the original number in ascending and descending order, take these two numbers, and find the difference between the two. Finally, repeat this routine using the difference as the new four-digit number. D. R. Kaprekar was the first to discover in 1949 that this process, sometimes called Kaprekar Routine, would always yield 6174 within 7 iterations. Since, this number remains unchanged after an application of Kaprekar Routine, it became known as Kaprekar’s Constant. Previous works have shown that the only base 10 Kaprekar’s Constants are 495 and 6174. However, little attention has been given to other bases or figuring out which digits and which bases actually have a Kaprekar’s Constant. This paper extends the scope though which Kaprekar’s Constant is viewed by establishing laws for other bases and digits regarding the properties of their respective Kaprekar’s Constant. In addition, it explains why these laws occur and what they could possibly indicate. 

Awards won at the 2013 ISEF
Third Award of $1,000 - Mathematical Sciences
________________________________________
2012 - MA021 
AERIAL NAVIGATION: A MATHEMATICAL SYSTEM OF EQUATIONS CAPABLE OF NAVIGATING AN AERIAL DEVICE WITHOUT THE USE OF SATELLITES
Gerald Paul Lawlor
Notre Dame High School, Chattanooga, TN

What would happen if satellites were occluded by violent solar flares, shut-down due to cyber attacks, or physically demolished by intercontinental ballistic missiles? Without satellite communications, planes and other aerial devices would be incapable of flying because their only accurate navigation system is solely dependent on satellite communications. The focal point of this project was to successfully create a mathematical system of equations that has the capacity of accurately navigating an aerial device between two global points without the aid of satellite communications.<br><br>Using different types of mathematics (e.g. advanced algebra, plane and analytical geometry, trigonometry, discrete mathematics, and calculus), multiple formulas were developed that have the ability to calculate the variables, angles, and distances that are absolutely necessary for the operation of an independent aerial navigation system. Various specific formulas based on topological and geographical factors were then created by the substitution and representation of variables in the central angle formula. Every formula was arranged in a specific order to compose an overall system of equations that can successfully and accurately navigate aerial devices between two geographical points without the use of satellite communications.<br><br>In the future, this project could be used to program an independent aerial navigation system that does not rely on satellite communications. This would allow planes, drones, and missiles to successfully fly between two global points utilizing advanced geographical mappings from the National Geospatial Association. 

Awards won at the 2012 ISEF
First Award of $3,000 - Air Force Research Laboratory on behalf of the United States Air Force
________________________________________
2013 - MA022 
15 PENNIES
Mikaela Ann Fischer
Waverly/South Shore High School, Waverly, SD

In my project “15 Pennies” I wanted to see if using the formula (n+1) would allow me to win the game every time if I am able to participate first. My hypothesis stated that I would be able to win the game every time if I went first even with variations. In the original game, two players can choose one, two, or three pennies and the player that has to take the last penny loses. During my experiment I changed the number of choices one could select (n), the number of pennies in the game, and the numbers of players. After my experiment I found that having the first turn allows me to win the game every time except when my A1 value is zero. The A1 value is the number of pennies that needs to be taken to get the other numbers within reach. If there are more than two players the object of the game becomes more not to lose rather than trying to win. With the additional player there are more variables making the (n+1) formula less reliable. Graphing my results (used sequence and series gaming principle) of p(n)=A1+(n-1)d. This game allows people to analyze more in-depth the series and gaming principle with the formula needed to win the game “15 Pennies.” 
________________________________________
2012 - MA022 
FRACTAL DIMENSION ANALYSIS TO PREDICT CANCER
Madhurima Das
Plymouth High School, Canton, MI

Irregular shapes in nature are hard to describe using regular Euclidean geometrical forms, and as shown by Benoit Mandelbrot, are better represented by their fractal dimension. Recent research results show that cancer cells have a higher fractal dimension, or irregularity, than benign cells. One technique of determining fractal dimension is the box-count method, in which boxes of a specific size are overlaid on an image and then the number of boxes with pieces of the image in them are counted. The process is repeated with boxes of smaller sizes, and the ratio of boxes used to boxes counted is then graphed to determine the fractal dimension. In this project, the box-count method of fractal dimension analysis is used on images of specific cells/cell clusters (histological data) in order to establish the difference in fractal dimensions between malignant and benign tissues. The goal is to demonstrate (with a large data set) that there is a clear and quantifiable difference between the fractal dimensions of the two types of tissues; this method can then be used as a potential mathematical method to detect and diagnose cancer at an early stage. The images used in this project are actual pictures of cancerous and noncancerous cells that were obtained from open source databases managed by research institutions. These databases have been made available to the public to be used in research.<br><br> 

Awards won at the 2012 ISEF
Second Award of $1,500 - Mathematical Sciences - Presented by Intel
________________________________________
2013 - MA023 
THE STUDY AND EXPERIMENTATION OF CRYPTANALYSIS, CRYPTOGRAPHY IN FINDING THE MOST EFFECTIVE MEANS OF TRANSMITTING ENCRYPTED MESSAGES
Joseph Daniel Wells
Weber High School, Pleasant View, UT

Cryptology has been used for generations as a form of secret communication for encrypting messages. This project studied the effectiveness of different form of encryption to determine the most efficient method. The message “Codes are mostly made for wars” was encrypted in seven different ciphers and or encrypting methods: Greek Code, Mathematical Code, Robot Picture Code, Graphic Design Picture Code, Wells Code, Wells Program Code, and Wells Engine Picture Code. Wells Program Code was written in C# to encrypt these three dimensional images. The hypothesis was that the most efficient method of sending encrypted message would be the Wells Engine Picture Code. It is believed this method contains unique three dimensional symbols that the test subjects have never seen. Test subjects were given fifteen minutes to decipher the encrypted message. During the first five minutes, the test subjects were not allowed any hints. After five minutes, one hint was given to the test subjects. At ten minutes a second hint was given. Subjects were given five additional minutes to determine if they were able to find the hidden message. The average percent of letters/message that were deciphered and the percent of hints required were recorded. In methods other then the Wells Engine Picture Code, either the encrypted message was found or more than 6% of the message was deciphered. This data shows that Wells Engine Picture Code was the most efficient at hiding the message, with only 1% of the message deciphered. 
________________________________________
2011 - MA023 
PRIME NUMBERS AND THE CYCLICITY OF GROUPS OF UNITS
Cissy Chen
Fairview High School, Boulder, CO

Cyclic groups, an integral part of group theory, can be generated by some element a (called a generator) by raising a to different powers. The group of units of Z(n) (the set of congruence classes modulo n) contains all positive integers that are relatively prime to n modulo n. This project determines which n produce cyclic groups of units of Z(n). First, I generated the first 100 groups of units using Mathematica. Then, I tested for cyclicity by raising each of the elements to different powers and comparing them to the original group of units. The results of these tests led to several conjectures characterizing the n that produce cyclic groups of units. In sum, the group of units is cyclic if and only if n is 2, 4, a power of an odd prime (pox), or twice the power of an odd prime (2pox). This theorem was proved in two parts: one, to show the groups of units under the given criteria are cyclic, and two, to show the groups of units of n not under those criteria (in which case n must be a multiple of 8 or have at least two distinct odd prime factors) are not cyclic. These results have applications in cryptography, primality testing, and molecular symmetry. Specifically, the characterization of cyclic groups is especially significant in cryptography. Many encryption systems, such as the Diffie-Hellman Key protocol, are based on the properties of cyclic groups. 
________________________________________
2012 - MA023 
CLASSIFYING GENERIC SMOOTH CURVES IN THE PROJECTIVE PLANE RELATED TO ALGEBRAIC CURVES OF DEGREE 5
Kevin Sherwin
Roslyn High School, Roslyn Heights, NY

I studied the space of generic immersions of the circle to the projective plane, RP2, such that any small perturbation removing all double points was isotopic to those of real algebraic curves of degree 5. I looked to construct and classify all distinct curves belonging to this space. Constructions were based on the three perestroikas that were developed by Vladimir Arnold. The perestroikas include the direct self-tangency, the inverse self-tangency, and the triple point perestroika. For each of these three types of perestroikas, Arnold introduced a numerical invariant that changes under perestroikas of this kind and does not change under other perestroikas and can be utilized to distinguish between different curves quantitatively. I calculated, for each curve of degree 5, the values of invariants generalized to the projective plane. I unexpectedly found that the majority of the curves under consideration shared the same Arnold invariant values. Consequently, the Arnold invariants were inadequate in distinguishing these curves from one another. This finding suggests directions for further research to develop new invariants that can distinguish all of these generic smooth curves from one another. 

Awards won at the 2012 ISEF
Fourth Award of $500 - Mathematical Sciences - Presented by Intel
________________________________________
2012 - MA024 
ON MATHEMATICAL GROWTH MODELS: REPRESENTING ACCURATE, MEANINGFUL, AND CONTINUOUS CHANGE WITH DYNAMIC LIMITING FACTORS
Jeffrey Neal Rosenberg
Calabasas High School, Calabasas, CA

Objective<br><br>Growth models have been quintessential in economic, biological, medical, chemical, industrial, and sociological application. Those models however, are only applicable to constant systems. The objective is to derive more accurate growth models that account for changes in limiting factors.<br><br>Methods<br><br>The basic form of the logistic model was used as the basis of the growth models, and was then manipulated with other basic functions; i.e. trigonometric, sigmoidal, step, etc. Arithmetic and calculus methods were also utilized to further manipulate data; i.e. integration, differentiation, etc. Data was collected from published information to ensure accuracy of the models produced. The data was compiled using a TI-84 calculator and excel model.<br><br>Results<br><br>The three major applications provided with data points were a model of Japanese population growth (n=140, sigmoidal), cumulative oil future sales (n=56, phase-wise), and Lynx-Hare (predator-prey) population relationship (n=60, sinusoidal). All models were projected against the corresponding set of data points, and the error never exceeded 5%. Other theoretical models were derived to match estimated growth in sales with seasonality, diffusion of innovation, and more.<br><br>Conclusion<br><br>These new models with dynamic limiting factors can measure and predict growth accurately, long-term. Human population continues to expand the resources available and decreases the effects of limiting factors, which require dynamic models. The projected sales of a product based on certain economic patterns make these models practical. Applications branch as far as medicine, chemistry, and most likely, everything that changes. To understand the complex and dynamic world, dynamic and hopefully simple models must come to match it. 
________________________________________
2013 - MA024 
MOVING AT THE SPEED OF LIGHT, PART 3: PARTIAL MOLAR VOLUMES
Patrick Pachiano
Christ The King Cathedral School, Lubbock, TX

This project applies a mathematical treatment to determine partial molar volumes from refractive index measurements. The hypothesis is that if the Lorentz/Lorenz equation describing the relationship between refractive index and density is accurate, then it can be used to calculate partial molar volumes from refractive index measurements. In a previous year’s project, both a homemade laser refractometer and a research-grade Abbe refractometer were used to measure the refractive index of aqueous solutions of ethylene glycol. The data from that project was used in this year’s project. The first step was to calculate the molar volume of each mole fraction solution using the Lorentz/Lorenz equation, the molar refractivity of water and ethylene glycol, and mole fraction of ethylene glycol. The second step was to use a quadratic regression on the molar volume versus mole fraction data to obtain the quadratic coefficients. Finally, the quadratic coefficients were used to calculate the partial molar volume of water and ethylene glycol as a function of mole fraction on ethylene glycol. As a reference, this technique was applied to literature values of refractive index versus mole fraction ethylene glycol. The results of this project show that application of the Lorentz/Lorenz can be used to calculate partial molar volumes. 
________________________________________
2011 - MA024 
PROPERTIES OF MAXIMAL FUNCTIONS MEASURING SMOOTHNESS
Evgeny Vyacheslavovich Varganov
Lyceum #572, Center of Mathematical Education, Saint-Petersburg, RUSSIA

Maximal functions play an important role in understanding the differentiability properties of functions, singular integrals theory and partial differential equations. The most well-known function was defined by Hardy and Littlewood. This work deals with maximal functions M#f and Mbf defined by Calderon. These functions allow to measure higher derivatives in case of maximal functions. For example, if M^bf is locally summable then f is a Sobolev function. However, nothing is known about the relation between M^#f and M^bf. Particularly, there is no answer to the question whether M^bf is summable in case M^#f is summable. The first task was to answer this question. The answer is negative, but for a wide class of functions this work describes it is positive. The second task was to study general properties of these operators. A set of non-trivial properties was proved in this part, including the estimates of their local behavior. This work has a certain theoretical interest and provides an opportunity to better understand a nature of these maximal functions. 
________________________________________
2011 - MA025 
FRACTALS, HAUSDORFF DIMENSION, AND DIAGNOSIS OF CANCER
Daniel David White
Somerset High School, Somerset, MA

The objective of this experiment was to establish a mathematical means of diagnosing premature cancer using fractals and Hausdorff dimension. Fractals are objects that possess recursive patterns. The Hausdorff dimension is used to describe these repetitive patterns. Cauliflower is one example of a naturally occurring fractal and is the foundation of this experiment, serving as a cell substitute for practicality.<br><br> The diameters of each branch and sub-branch of a cauliflower form a predictable ratio, which is proportional to the Hausdorff dimension. Therefore, if the ratio changes then this dimension will also change. This is analogous to cells: if a burned cauliflower has a different dimension or structure than a normal cauliflower, then an atypical cell will have a different dimension than a typical cell.<br><br> A cauliflower was dissected into its branches and sub-branches. The diameters were measured and ratios derived. To contrast, a cauliflower was burned and the ratio between its diameters was compared to that of a typical cauliflower. Finally, an original computer program was created to compare the ratios of images of a typical cell and an atypical cell.<br><br> It was found that a cauliflower had a specific ratio between its branches and sub-branches. This ratio averaged 3, and with Hausdorff dimension computed, produced 2.3. When this process was repeated with burned cauliflower, the ratio averaged 4 and the dimension was approximately 1.8. As expected, the computer program output different dimensions for the typical and atypical cells. Therefore, mathematical diagnosis of premature cancer is possible. 
________________________________________
2012 - MA025 
(C, B, A)-PERMUTATIONS, THEIR YOUNG DIAGRAMS AND ARNOLD DISCRETE DYNAMIC SYSTEMS
Danila Alexandrovich Baygushev
Lyceum "Vtoraia shkola", Moscow

The aim of my work is to consider the discrete analog of a well-known problem (the so-called “rearrangements of segments” problem) set by V.I. Arnold in 1958. Namely, I consider set {1,2,...,n} and divide it into three non-empty blocks A, B, C (which consist of sequent numbers). Then I move them in the following way: C, B, A. The resulting permutation of set {1,2,...,n} will be called (C,B,A)-permutation. <br><br> The study of (C,B,A)-permutations is very important and interesting, because it helps to get a better insight into the rearrangements of segments problem and, consequently, into the theory of dynamic systems.<br><br> Methods of studying (C,B,A)-permutations used in the work include: combinatorial, dynamic, algebraic and geometric approaches.<br><br> The most important results obtained in my work are the following.<br><br>1. The criterion of ergodicity for (C,B,A)-permutation was proved.<br><br>2. The Arnold problem of counting the proportion of ergodic (C,B,A)-permutations was solved.<br><br>3. The geometry of Young diagrams for (C,B,A)-permutations was studied and averages of these diagrams were found.<br><br> Besides, I made a comparative analysis of (C,B,A)- and ordinary permutations and found out that their geometric properties are similar, though their inner structures are quite different. Also my work suggests an original method of studying continuous dynamic systems with the help of their discrete analogs. I applied this method to the rearrangements of segments problem, but it could be used for many other dynamic systems, for example, for dynamic systems on torus. 

Awards won at the 2012 ISEF
Third Award of $250 - American Mathematical Society
________________________________________
2013 - MA025 
CLOSED-FORM VOLUMES OF A WIDE FAMILY OF ASTROIDAL ELLIPSOIDS AND THE HYPERBOLIC OCTAHEDRON
Salahaldeen Ibrahim Abu-Alshaikh
Jubilee School, Amman, JORDAN

In this study, closed-form volumes of hyperbolic octahedron and wide families of astroidal ellipsoids were found. Furthermore, the closed form of center of mass and mass moment of inertia for these solids were found. As a special case, the closed-form volumes of unit sphere, ellipsoids, astroidal sphere, hyperbolic octahedron and other astroidal ellipsoids were obtained exactly using a single formula that contains Gamma function. However, it is well known to the applied mathematics field that the exact volume of the hyperbolic octahedron and astroidal ellipsoids are unknown before this study. 

Awards won at the 2013 ISEF
Second Award of $1,500 - Mathematical Sciences
________________________________________
2012 - MA026 
A NOVEL VARIANT OF THE NEWTON-RAPHSON METHOD, A QUADRATIC CONVERGENCE CRITERION, AND COMPUTER GRAPHICS
Mingu Kim
David H. Hickman High School, Columbia, MO

The Newton-Raphson method is an iterative procedure often used to approximate solutions of systems of nonlinear equations. Under well-known hypotheses, iterates of this method are known to converge to solutions quickly; indeed the convergence is at a quadratic rate. The Newton-Raphson method, when applied to polynomials of one complex variable, is famous for the generation of pictures with multiple attracting fixed points with fractal basin boundaries. A weakness of the Newton-Raphson method applied to systems of equations is the computational cost of solving a linear system of equations that is required to complete each iteration. This project proposes a new variant of the Newton-Raphson method that approximates solutions of systems of equations "one variable at a time". This method does not require the solution of linear systems; it is a recursive formula that can be iterated to approximate solutions. An initial guess converges at a quadratic rate to a solution using this method if the criterion discussed in this work is satisfied. Although pictures of basins of attractions for complex polynomials generated by the proposed method differ from those obtained from the original Newton-Raphson method, fractal basin boundary behavior is preserved. Results of numerical experiments show that the recursive formula takes less computing time than the original Newton-Raphson method for some systems of equations. This new algorithm is promising in the development of a more efficient method for approximating solutions for large systems of nonlinear equations that arise from many fundamental problems in a variety of sciences. 

Awards won at the 2012 ISEF
All expense paid trip to tour CERN - European Organization for Nuclear Research-CERN
________________________________________
2013 - MA026 
LOWER CENTRAL SERIES QUOTIENTS OF FINITELY GENERATED ALGEBRAS OVER THE INTEGERS
Katherine Cordwell
Manzano High School, Albuquerque, NM

We study the lower central series for unital associative graded algebras over the integers. Specifically, we consider quotients of lower central series elements, each of which is graded and can be written as the direct sum of graded components. Each component is a finitely generated abelian group and may be further decomposed into a free part and a torsion part. These components depend on the underlying algebra A in subtle ways; using Magma, we gather data, find patterns, prove that certain patterns continue, and formulate some conjectures. We mainly consider free associative algebras on n generators modulo homogeneous relations over the integers. We completely describe the quotient objects for free algebras modulo a relation of the form f = xy - qyx for integer q. We also outline a proof that the graded components of the quotient objects stabilize for the free algebra in two generators modulo the relation x^d (for d a natural number) and present a result concerning a case in which the quotients are finite-dimensional. 

Awards won at the 2013 ISEF
First Award of $3,000 - Mathematical Sciences
________________________________________
2011 - MA026 
NEW TRIANGLE CENTERS ASSOCIATED WITH A TRIAD OF THE SIMULATED CIRCUMCIRCLES
Kang-Ying Liu
Saint Andrew's Priory School, Honolulu, HI

In American Mathematical Monthly, Clark Kimberling and Peter Yff state, “Let A' be the point in which the incircle is tangent to a circle that passes through vertices B and C, and define B' and C' cyclically. The lines AA', BB', CC' concur in X(479).” These unique circles are defined as simulated circumcircles. I first found the points simulated circumcircles passed through, and then obtained the equations of simulated circumcircles based on these points. <br><br> There are seven theorems as following: Theorem 1 restates and proves X(479). Theorem 2 shows triangle ABC and the triangle bounded by three common tangents of incircle and simulated circumcircles are perspective from X(57), which is also the concurrency of the lines determined by excenters and tangencies of incircle and simulated circumcircles. Theorem 3 states points Ta, Tb, and Tc are the intersections of lines pass vertices of triangle ABC and tangencies of incircle and simulated circumcircles. Lines ATa, BTb, and CTc are concurrent at X(55). Theorem 4 discovers a new triangle center that is the Gergonne Point of the triangle bounded by three common tangents of incircle and simulated circumcircles. Theorem 5 discovers a new triangle center that is the perspector of triangle ABC and the triangle formed by the radical axis of excircles and simulated circumcircles. Theorem 6 discovers another triangle center that is the perspector of triangle TaTbTc and the triangle determined by the tangencies of incircle and simulated circumcircles. Theorem 7 states the collinear properties among some well-known triangle centers and new triangle centers.<br><br> After the assessment of Dr. Clark Kimberling, two of the new triangle centers are defined as X(3598) and X(3599), and be named as 1st and 2nd Liu Point in the ETC in March, 2011. 

Awards won at the 2011 ISEF
Fourth Award of $500 - Mathematical Sciences - Presented by Intel
________________________________________
2013 - MA027 
IMPLEMENTING GRAPH THEORY IN A NOVEL PROGRAM FOR PREDICTING CARBON NANOSTRUCTURES
Bailey Liao
Half Hollow Hills High School West, Dix Hills, NY

Graph theory has many practical applications including optimization of transit routes and social networks. This research effectively implemented graph theory towards the synthesis of carbon nanostructures. Currently mass production methods for carbon nanostructures are inefficient due to an inability to predict the results of such chemical synthesis reactions. One proposed solution is the process of annealing which introduces defects into the nanostructures. A novel computer program, the Accelerated Topological Annealing of Carbon (ATAC), seeks to predict the annealing process computationally so time and money are not wasted on trial and error experimentation. The primary limitation with the program is the implementation of a method that accurately adds defects into the system. Utilizing graph theory, each sp2 hybridized carbon atom was represented as a node and each double bond was represented as a specific edge. Thus, a perfectly matched system was created where every node is attached to exactly one specific edge. Using mathematical expressions written into code, ATAC implemented defects more accurately as depicted through a calculated total system energy versus time function. Subsequent iterations of ATAC demonstrated that a carbon nanobud structure emanated from an initial T-Junction structure. Furthermore, a multi-walled carbon nanotube was derived from an initial 6 Ball Peapod nanostructure. Multi-walled carbon nanotubes provide a promising direction for net synthesis considering the inner tube can be extracted to form another nanotube substrate. Similarly, the nanobud structure can be used to advance touch screen technology potentially improving the flexibility of future touch screen applications. 
________________________________________
2012 - MA027 
COMPLEXITY OF INTERLOCKING POLYOMINOES
Sidharth Dhawan
Westview High School, Portland, OR

Polyominoes are a subset of polygons which can be constructed from integer-length squares fused at their edges. A system of polygons P is interlocked if no subset of the polygons in P can be moved arbitrarily far away from the rest. It is already known that polyominoes with four or fewer squares cannot interlock. It is also known that determining the interlockedness of polyominoes with an arbitrary number of squares is PSPACE hard. Here, we prove that a system of polyominoes with five or fewer squares cannot interlock, and that determining interlockedness of a system of polyominoes including hexominoes (polyominoes with six squares) or larger polyominoes is PSPACE hard. 

Awards won at the 2012 ISEF
Third Award of $250 - American Mathematical Society
Second Award of $1,500 - Mathematical Sciences - Presented by Intel
________________________________________
2011 - MA027 
DEVELOPING A CIPHER BASED ON VARIATION BETWEEN NON-BASE 10 NUMBER SYSTEMS
Caroline Knight Snowden
Ponte Vedra High School, Ponte Vedra, FL

PROBLEM: The researcher aspires to develop a secure cipher based on the variation of non-base 10 number systems. HYPOTHESIS: The researcher believes that based on her acquired knowledge of ciphers and the related mathematics, the goal can be feasibly completed. PROCEDURE AND RESULTS: After extensive research, the researcher independently and uniquely developed an algorithm for the conversion of base-10 numbers into a known non-base 10 number system and back again. This algorithm provides the foundation for a cipher based on variation of non-base 10 number systems, thereby achieving the researcher’s proposed goal. CONCLUSION AND DISCUSSION: The cipher developed by the researcher offers a novel and secure method of encipherment. Unlike many of the monoalphabetic and polyalphabetic systems in use today, no method for breaking the number system cipher has been developed aside from trial and error. The number system cipher offers a computationally more manageable alternative to the RSA cipher. While large numbers heighten the security of both ciphers, the number system cipher is expanded only limitedly upon encryption, while the RSA cipher is expanded exponentially. Therefore, the number system cipher offers a secure, convenient method of encipherment to those lacking powerful computational equipment, in either the military or commercial fields. Additionally, the number system cipher can be layered with the RSA cipher (parallel to the layering of an affine cipher) for added security. 

Awards won at the 2011 ISEF
Second Award of $1,500 - Air Force Research Laboratory on behalf of the United States Air Force
________________________________________
2011 - MA028 
POPULATION MODELING OF THE SACRAMENTO SALMON: COMBINING COMPOSITIONAL DATA WITH TRADITIONAL ABUNDANCE ESTIMATES
Damon Chizuru Kawamoto
Pacific Collegiate School, Santa Cruz, CA

Many challenges are faced when forecasting the abundance of salmon for the ocean-fishing harvest. I compared two methods to estimate the abundance of Sacramento River Fall-run Chinook salmon. The standard method, the jack count method, uses the number of jacks (fish that have matured one to two years early) returning to freshwater to predict the Sacramento salmon abundance for the following year. I investigated an alternative method in which the Sacramento abundance is derived using a separate abundance estimate for Klamath Chinook and the relative occurrence (determined by genetic methods) of Sacramento and Klamath fish in the harvest. I first assessed the feasibility of this new approach using historical data from the Pacific Fishery Management Council (PFMC) including the Klamath harvest, Sacramento harvest, Sacramento Index, Klamath abundance and the Sacramento jack count. Second, I applied this method to historical fishery composition data recently obtained using genetic analysis. I used the statistical modeling program R to assess the strength of the relationship between the overall abundance of Klamath and Chinook salmon and their proportional representation in ocean fisheries across different seasons and coastal locations. Analysis of the PFMC data revealed little relationship in the overall abundance and the harvest proportion. Based on an r^2 measure, the jack count method is much more accurate than the alternate methods I investigated. This suggests that fishery composition estimates from genetic data are unlikely to provide substantial improvement in abundance estimates. 
________________________________________
2013 - MA028 
ANOTHER SOLUTION OF THE THUE PROBLEM OF NON-REPEATING WORDS
Boris Zolotov
School #564, Saint-Petersburg, RUSSIA

At the beginning of the XX century a Norwegian mathematician Axel Thue provided an example of some infinitive words over binary and ternary alphabets which don't have factors of exact forms. The construction of these words is simple: they are generated as fixed points of free monoid morphisms.<br><br>An alphabet A is a finite set of elements called symbols or letters. A word over A is a sequence of letters from A. Equipped with the concatenation operation, the set A* of words over A is a free monoid with an empty word as a neutral element and A as a set of generators.<br><br>A morphism f: A*->A* maps A* to A* such that for all words u, v over A f(uv) = f(u)f(v) . A morphism is entirely known by the images of the letters of A.<br><br>We say that the morphism f is a Thue morphism if f is a cube-free and overlap-free morphism and 1 is prefix of f(1) (if it is true, then uk=fk(1) is prefix of uk+1=fk+1(1)). So, if f is a Thue morphism then it has a fixed point u=f(inf)(1).<br><br>Main results<br><br>1. There exist weakly square-free Thue morphisms over the ternary alphabet, namely a 5-uniform morphism f: 1->12321; 2->23132; 3->31213 is a weakly square-free Thue morphism.<br><br>2. There are no weakly square-free Thue k-uniform morphisms for k<5 over a ternary alphabet.<br><br>3. There are exactly 24 square-free 11-uniform morphisms over the ternary alphabet and there are no k-uniform square-free morphisms with a lower k.<br><br>4. The Leech morphism f: 1->1232132312321; 2->2313213123132; 3->3121321231213 is a square-free Thue morphism and there are no k-uniform square-free Thue morphisms with a lower k.<br><br> 
________________________________________
2012 - MA028 
MEL INNOVATION FORMULA
Melanie Rivera-Canals
Ramon Power y Giralt, Las Piedras, PUERTO RICO

In this research, the student researcher investigates whether there is a relationship between The Pascal Triangle, triangular numbers and the Binomial Theorem. After studying closely the Pascal Triangle, combinations, the Binomial Theorem and triangular numbers the research shows that there is a relationship between them. The purpose of this research is to find formulas that find the sum of the first ‘nth terms of the columns of the Pascal Triangle. The problem in this research is: how can we find the sum of the first ‘nth terms of the columns of the Pascal Triangle? Based on this problem the student researcher formulated the following hypothesis: using the Binomial Theorem and triangular numbers, a formula that adds the first ‘nth terms of the columns of the Pascal Triangle can be created. After studying the Pascal Triangle it was changed into the Binomial Theorem form. After studying the sequences, different formulas in which the ‘nth term and the sum of the first ‘nth terms of the columns of the Pascal Triangle were found. These formulas were subjected to several tests using a spreadsheet in which they proved to work. This finding was called Mel Formula, in recognition of the researcher. This formula will have a major impact in both statistics and in algebra. The researcher concluded that, if we use the Mel Formula, we can find the sum of each column of the Pascal Triangle, thus the hypothesis proved to be true. 
________________________________________
2012 - MA029 
GRAPH THEORY TO FIND FIBONACCI IN PASCAL AND EVACUATION ROUTES IN CASE OF A TSUNAMI
Valeria Taiz Rivera Martinez
Escuela Brigida Alvarez Rodriguez, Vega Baja, PUERTO RICO

The research’s main idea is graph theory and matrices. The main questions: Is there a way to find the Fibonacci’s sequence inside the Pascal’s triangle using a graph?; Which are the most effective exit routes in case of a tsunami?; Are the exit routes well designed in case of an emergency on the school? The hypothesis are: If the numbers in the Pascal’s triangle are added forming triangles, the Fibonacci’s sequence can be represented as a graph; If there are five points on the map and trace the possible routes, the most effective route to evacuate the place is by point D to Z; If the exits of the school’s evacuation plan are obstructed, is inefficient and there could be created a graph to help evacuate people. The independent variables are: the values on the Pascal’s triangle, the initial point on the map of the coast and the exit routes. The dependent variables are: the Fibonacci’s sequence, the exit of the coast and the evacuation routes. The graphs were created using software to construct the matrices for the data analysis. In conclusions, the Fibonacci’s sequence was found using a triangular graph; the shortest route for the evacuation is from point D to Z, pointing out that the routes are good in theory not necessarily in practice; the school’s evacuation plan is obsolete and with the help of the graph theory a new one was created. For the future, the investigator wants to make a 3d graph with the Fibonacci’s sequence. 
________________________________________
2013 - MA029 
YONEDA ALGEBRA ON A SPECIAL BISERIAL ALGEBRA
Andrey Semenov
School #564, Saint-Petersburg, RUSSIA

Special symmetric biserial algebras form a simplest class of<br><br>algebras of infinity representation type. As a consequense, this<br><br>class of algebras is of particular interest for research in<br><br>representation theory. <br><br>The main goal of the paper is a description<br><br>of the Yoneda algebras of one family of special simmetric biserial<br><br>algebra defined by a quiver consisting of one vertex and two loops a<br><br>and b with special relations.<br><br>In the work a minimal projective resolution of the only simple module S over algebra A is construcred. Its exactness is proved.<br><br>We collect multiplicative generators of Y(A) and find relations between them. In calculation of the multiplicative structure of Y(A) so called translations of cocycles, multiplicativly generating the Yoneda algebra, to endomorphismes of the resolution were used.<br><br>After that basis cohomology was calculated. We derived the theorem, fully describing the cohomology. 
________________________________________
2012 - MA029 
GRAPH THEORY TO FIND FIBONACCI IN PASCAL AND EVACUATION ROUTES IN CASE OF A TSUNAMI
Valeria Taiz Rivera Martinez
Escuela Brigida Alvarez Rodriguez, Vega Baja, PUERTO RICO

The research’s main idea is graph theory and matrices. The main questions: Is there a way to find the Fibonacci’s sequence inside the Pascal’s triangle using a graph?; Which are the most effective exit routes in case of a tsunami?; Are the exit routes well designed in case of an emergency on the school? The hypothesis are: If the numbers in the Pascal’s triangle are added forming triangles, the Fibonacci’s sequence can be represented as a graph; If there are five points on the map and trace the possible routes, the most effective route to evacuate the place is by point D to Z; If the exits of the school’s evacuation plan are obstructed, is inefficient and there could be created a graph to help evacuate people. The independent variables are: the values on the Pascal’s triangle, the initial point on the map of the coast and the exit routes. The dependent variables are: the Fibonacci’s sequence, the exit of the coast and the evacuation routes. The graphs were created using software to construct the matrices for the data analysis. In conclusions, the Fibonacci’s sequence was found using a triangular graph; the shortest route for the evacuation is from point D to Z, pointing out that the routes are good in theory not necessarily in practice; the school’s evacuation plan is obsolete and with the help of the graph theory a new one was created. For the future, the investigator wants to make a 3d graph with the Fibonacci’s sequence. 
________________________________________
2013 - MA029 
YONEDA ALGEBRA ON A SPECIAL BISERIAL ALGEBRA
Andrey Semenov
School #564, Saint-Petersburg, RUSSIA

Special symmetric biserial algebras form a simplest class of<br><br>algebras of infinity representation type. As a consequense, this<br><br>class of algebras is of particular interest for research in<br><br>representation theory. <br><br>The main goal of the paper is a description<br><br>of the Yoneda algebras of one family of special simmetric biserial<br><br>algebra defined by a quiver consisting of one vertex and two loops a<br><br>and b with special relations.<br><br>In the work a minimal projective resolution of the only simple module S over algebra A is construcred. Its exactness is proved.<br><br>We collect multiplicative generators of Y(A) and find relations between them. In calculation of the multiplicative structure of Y(A) so called translations of cocycles, multiplicativly generating the Yoneda algebra, to endomorphismes of the resolution were used.<br><br>After that basis cohomology was calculated. We derived the theorem, fully describing the cohomology. 
________________________________________
2011 - MA029 
A CREATIVE SOLUTION FOR DIVISION BY ZERO
Markus Robert Woltjer
Wilsonville High School, Wilsonville, OR

The purpose of this project is to invent a creative solution to dividing by zero. It can also be seen as defining the already existing solution that occurs with division by zero. Division by zero has, throughout history, been explored and abandoned by great mathematicians because of the extensive mathematical fallacies associated with the concept. The invention of this solution relies on the assumption of logical axioms and the investigation of their reliability through extending them. These proofs also give an idea of the typical behavior of the solution when applied to various mathematical concepts. The axioms found were (1)”All elements of the Nullinary Loop can result in operations of elements outside of the Nullinary Loop, but the element Xi is closed in the Nullinary Loop.”, (2)”Any fraction with denominator zero is equal to Xi, and the differences in the numerator cause irrelevant divergences of Xi, similar to the concept of infinity-plus-one.”, and (3)”The truth Xi*0=1 does not come algebraically, but in order to uphold principles of inverse operations using Axiom 2.” These were used to prove theorems and eventually implications. According to these results, division by zero can be defined using “The Nullinary Loop”, consisting of zero, one, and the Nullinary Number (Xi). This solution gives rise to many mathematical extensions beyond the limiting “undefined”, especially those concerning projective geometry. The practical importance of this solution are yet to be discovered, but likely existent considering the late application of non-Euclidean geometry to Albert Einstein’s Theory of Relativity. 

Awards won at the 2011 ISEF
Fourth Award of $500 - Mathematical Sciences - Presented by Intel
________________________________________
2012 - MA030 
ANALYSIS BETWEEN THE MACH NUMBER (M) AND THE APEX ANGLE OF THE CONE
Karolyne Li Lopez-Otero
Francisco Morales, Naranjito, PUERTO RICO

The aim of this research was to establish the relation between the flight altitude and the size of (theta) (apex angle of the cone) found through sin[(theta)/2]=1/M, for an airship that surpasses the speed of sound. The question to answer was; Depending on the altitude of flight, which will be the behavior of (theta), found by sin[(theta)/2]=1/M for an airplane whose speed (V-object) surpasses the speed of sound (V-sound)? The hypothesis postulated established that the altitude of flight and (theta) were directly proportional. In order to carry out this investigation, a sample of forty planes (commercial and military) was selected. An exhaustive search was made to determine each ship’s maximum speed, (V1), and (X1), the altitude in which each plane reaches its maximum speed. Later (V2), the speed reduced by 200 km/h, was found, and so was (X1), the altitude lessened by 3,000 meters. Thereupon, the speed of sound was calculated for different altitudes using V-sound=[(gamma)RT/m]^(1/2). Then, each ship was submitted to two tests, where the Mach Number was found, using M=V-object/V-sound, to later find the size of (theta). After all tests were made, it was found that as the flight altitude increases, altogether with the speed, the size of (theta) decreases. On the other hand, as the altitude diminishes, along with the speed, the size of (theta) increases. In conclusion, it could be established that the altitude of flight and the magnitude of (theta), are inversely proportional. 
________________________________________
2013 - MA030 
MATHEMATICALLY MODELING THE EFFICACY OF PERTUZUMAB AND TRASTUZUMAB-DM1 WHEN USED IN CONJUNCTION AGAINST HER2-POSITIVE BREAST CANCER
Rachel Elizabeth Burns
Loudoun County Academy of Science, Sterling, VA

HER2-positive breast cancer is characterized by an overexpression of HER2, a growth receptor protein, on the surface of the cell, which increases the rate of cell growth and replication. Pertuzumab is a chemical which targets and binds to HER2 proteins, resulting in an inhibition of cell signaling, eventually leading to the death of the cell. Trastuzumab emtansine (T-DM1) is an antibody-drug conjugate that not only binds to the HER2 protein and inhibits signaling, but also injects emtansine, a derivative of chemotherapeutic agent maytansine, into the cell following the bonding of the compound to the HER2 protein, thus killing the cell. <br><br>Mathematical modeling of breast cancer tumors in response to various treatment regimens of T-DM1 and pertuzumab can provide a cost effective method of analyzing dosing regimens by decreasing the number of clinical trials necessary. This modeling is achieved through a manipulation of the Koch Tumor Growth model involving a set of dynamic parameters which will allow the effect of different dosing strategies on the tumor to be observed as these parameters are altered. 
________________________________________
2011 - MA030 
MODELING WIND POWER GENERATION USING POLYNOMIAL CHAOS EXPANSION
Ryan Thomas Baker
Hillcrest High School, Midvale, UT

Due to concerns regarding global warming, sustainability, and energy security, researchers are seeking to utilize the potential of renewable energy sources. One of the most promising renewable energy sources is wind power. Wind power generation is dependent on wind speed which is highly uncertain. This presents a challenge in analyzing the effects of a hybrid power system with both gas powered and wind powered generators because the equations used to model power systems require deterministic inputs. Researchers have recently proposed a new method called Polynomial Chaos (PC) for use in propagating uncertainty from model inputs to model outputs. The purpose of this project is to apply PC methods to hybrid power systems to analyze the impacts of using an uncertain energy source in the electric power grid. Applying this method requires the creation of a new set of orthogonal polynomials, constructing a PC approximation for wind power, and then incorporating the wind power PC approximation into the power flow equations which describe the steady-state condition of a power network. When compared to a Monte Carlo method, the new method is just as accurate but significantly faster. The results from this project can be used to better evaluate the reliability, safety, and cost efficiency of incorporating wind power into a power grid and to maximize the efficiency of using wind power as a major power source. 

Awards won at the 2011 ISEF
Certificate of Honorable Mention - American Mathematical Society
Second Award of $1,500 - GE Energy
________________________________________
2012 - MA031 
SENSITIVE DEPENDENCE ON INITIAL DATA IN BACKWARDS-IN-TIME DIFFERENCE EQUATIONS
Steven Alexander Olsen
Baton Rouge Magnet High School, Baton Rouge, LA

In my research I investigate sensitive dependence on initial conditions in so-called "backwards-in-time" second-order diff erence equations and potential methods of inducing regularity into these systems. A second-order diff erence equation is given by the general formula a_(n+2) = A*a_(n+1) + B*a_n, where A and B are real-valued constants and the terms of the sequence a_n are recursively de fined with a_0 and a_1 given. While the nth term of the sequence can be calculated with arbitrary precision in the so-called "forwards-in-time" problem, the "backwards-in-time" situation poses a considerable problem. That is, given two consecutive terms a_(n+1) and a_n in a sequence and the values of A and B, determine the value of a_0 and a_1. While this is feasible if there is no error in the initial values, any amount of truncation in the forwards-in-time sequence has extreme e ffects on the backwards-in-time sequence and causes it to rapidly move away from the original sequence. I investigate methods by which this error-induced divergence can be regularized and potential ways to recover the lost information. My research shows that after repeated averaging of the values of the backwards sequence, the averaged numerical backwards-in-time sequence approximates the correct averaged backwards sequence, and also shows that, under suitable conditions, the information lost due to rounding error can be recovered. 
________________________________________
2013 - MA031 
MATCHING PRECLUSION AND CONDITIONAL MATCHING PRECLUSION FOR DUAL-CUBES
Akhil Nistala
Novi High School, Novi, MI

The matching preclusion number of a graph is the minimum number of edges whose deletion results in a graph that has neither perfect matchings nor almost-perfect matchings. For many interconnection networks, the optimal sets are precisely those induced by a single vertex. Recently, the conditional matching preclusion number of a graph was introduced to look for obstruction sets beyond those incident to a single vertex. It is defined to be the minimum number of edges whose deletion results in a graph with no isolated vertices that has neither perfect matchings nor almost-perfect matchings. In my research, I found the matching preclusion number and the conditional matching preclusion number, and classified all optimal sets with respect to these problems for the dual-cube, a network designed as an improvement of the hypercube. I showed that the dual-cube preserves both the robustness and the efficiency of a hypercube while allowing for more vertices, thus making it vastly superior. This research has practical implications in computer networking as well as in distributed architecture processing. 

Awards won at the 2013 ISEF
Certificate of Honorable Mention - American Mathematical Society
________________________________________
2011 - MA031 
A NOVEL IMPLEMENTATION OF THE ELLIPTIC CURVE METHOD, STAGE 2: USING WEIERSTRASS AND EDWARDS ELLIPTIC CURVES FOR FASTER FACTORIZATION
Aishwarya Ananda Vardhana
Jesuit High School, Portland, OR

The Elliptic Curve Method (ECM) computes a large multiple of a point P on an elliptic curve modulo n to find a prime factor of n. The ECM has been implemented in three forms: Affine Weierstrass (AW): y2 = x3 + ax + b, Projective Weierstrass: y2z = x3 - jxz2- kz3, Projective Jacobian: y2 = x3 – kxz4 – jz6, and newly created Projective Edwards curves (PE): (x2 +y2) (z2) = z4 + kx2y2. The four forms were combined to create Multicurve I algorithm. <br><br>ECM contains two phases, stage 1 and stage 2. Stage 2 factors n if stage 1 fails. Stage 2 focuses strictly on large prime multiples of P within a range [B1, B2] where a large prime multiple of P may yield the point at infinity which in turn yields the smallest prime factor of n. <br><br>2011 work concentrates on the creation of Projective Edwards stage 2 because Edwards curves have thus far not been implemented in Projective space. PE stage 2 has been combined with AW stage 2 to form Multicurve II algorithm. A novel twist to PE stage 2 is using Montgomery Reduction for fast modular arithmetic. The Multicurve II has been implemented in C# language as a parallel-processing and multi-threaded program which runs on multiple cores. The objective was to create an algorithm of 10% faster factorization and 5% higher success rate, which can find factors of 25 digits in length. The Multicurve II algorithm has a success rate 22% higher than AW stage 1 and time efficiency 34% greater than AW stage 1 thereby resulting in a partially deterministic algorithm compared to the probabilistic ECM which drives towards polynomial time. It has also found a 33 digit factor. Currently work is concentrated towards integerating Multicurve II within the General Number Field Sieve for faster factorization. 

Awards won at the 2011 ISEF
All expense paid trip to tour CERN - European Organization for Nuclear Research-CERN
Second Award of $1,500 - Mathematical Sciences - Presented by Intel
________________________________________
2013 - MA031 
MATCHING PRECLUSION AND CONDITIONAL MATCHING PRECLUSION FOR DUAL-CUBES
Akhil Nistala
Novi High School, Novi, MI

The matching preclusion number of a graph is the minimum number of edges whose deletion results in a graph that has neither perfect matchings nor almost-perfect matchings. For many interconnection networks, the optimal sets are precisely those induced by a single vertex. Recently, the conditional matching preclusion number of a graph was introduced to look for obstruction sets beyond those incident to a single vertex. It is defined to be the minimum number of edges whose deletion results in a graph with no isolated vertices that has neither perfect matchings nor almost-perfect matchings. In my research, I found the matching preclusion number and the conditional matching preclusion number, and classified all optimal sets with respect to these problems for the dual-cube, a network designed as an improvement of the hypercube. I showed that the dual-cube preserves both the robustness and the efficiency of a hypercube while allowing for more vertices, thus making it vastly superior. This research has practical implications in computer networking as well as in distributed architecture processing. 

Awards won at the 2013 ISEF
Second Award of $1,500 - Mathematical Sciences
________________________________________
2011 - MA031 
A NOVEL IMPLEMENTATION OF THE ELLIPTIC CURVE METHOD, STAGE 2: USING WEIERSTRASS AND EDWARDS ELLIPTIC CURVES FOR FASTER FACTORIZATION
Aishwarya Ananda Vardhana
Jesuit High School, Portland, OR

The Elliptic Curve Method (ECM) computes a large multiple of a point P on an elliptic curve modulo n to find a prime factor of n. The ECM has been implemented in three forms: Affine Weierstrass (AW): y2 = x3 + ax + b, Projective Weierstrass: y2z = x3 - jxz2- kz3, Projective Jacobian: y2 = x3 – kxz4 – jz6, and newly created Projective Edwards curves (PE): (x2 +y2) (z2) = z4 + kx2y2. The four forms were combined to create Multicurve I algorithm. <br><br>ECM contains two phases, stage 1 and stage 2. Stage 2 factors n if stage 1 fails. Stage 2 focuses strictly on large prime multiples of P within a range [B1, B2] where a large prime multiple of P may yield the point at infinity which in turn yields the smallest prime factor of n. <br><br>2011 work concentrates on the creation of Projective Edwards stage 2 because Edwards curves have thus far not been implemented in Projective space. PE stage 2 has been combined with AW stage 2 to form Multicurve II algorithm. A novel twist to PE stage 2 is using Montgomery Reduction for fast modular arithmetic. The Multicurve II has been implemented in C# language as a parallel-processing and multi-threaded program which runs on multiple cores. The objective was to create an algorithm of 10% faster factorization and 5% higher success rate, which can find factors of 25 digits in length. The Multicurve II algorithm has a success rate 22% higher than AW stage 1 and time efficiency 34% greater than AW stage 1 thereby resulting in a partially deterministic algorithm compared to the probabilistic ECM which drives towards polynomial time. It has also found a 33 digit factor. Currently work is concentrated towards integerating Multicurve II within the General Number Field Sieve for faster factorization. 

Awards won at the 2011 ISEF
$ 3,000.00 will be the total of all awards. First award minimum of $1500.00 - Mu Alpha Theta, National High School and Two-Year College Mathematics Honor Society
________________________________________
2011 - MA032 
METHOD OF OPTIMIZING THE MONTE CARLO STATISTICAL ALGORITHM TO INCREASE COMPUTATIONAL EFFICIENCY IN MULTIDIMENSIONAL INTEGRATION
Pratheek Nagaraj
Marjory Stoneman Douglas High School, Parkland, FL

The Monte Carlo method (MCM) based computational algorithms are extensively used in scientific fields to solve complex problems ranging from climate modeling to financial risk analysis. MCMs are used for modeling phenomena involving a large number of variables with significant uncertainty, where deterministic techniques are impractical to compute an exact result.<br><br>This project focused on implementation, evaluation, accuracy and feasibility of MCM multidimensional integration. The research goal was to identify a more computationally efficient and accurate MCM than the Plain MCM and to further analyze the various components of the optimized algorithm to develop a novel hybrid MCM algorithm. <br><br>This study analyzed algorithms in C++ and Mathematica for Plain, Quasi, Adaptive, MISER, and VEGAS MCMs and tested them through extensive simulations of multidimensional integration scenarios involving oscillatory functions, exponential functions, and localized variance, while spanning various sampling sizes totaling 1200 trials. <br><br>Compared to the Plain MCM, the VEGAS MCM was 89.68% more accurate and the Quasi MCM was 98.50% more accurate with the exclusion of Integral #5 which highly skewed the results; both VEGAS and Quasi MCM were significantly more precise. Overall, the VEGAS MCM outperformed the Plain MCM in accuracy and efficiency, demonstrating the optimal blend of importance sampling with recursion, quasi random numbers, and probability distributions. <br><br>Ultimately, this research serves as the foundation for my development of a novel hybrid MCM, using the VEGAS MCM as the base model, which will dynamically adapt to complex scenarios where the current methods fail to accurately compute a reliable solution. 

Awards won at the 2011 ISEF
All expense paid trip to tour CERN - European Organization for Nuclear Research-CERN
________________________________________
2013 - MA032 
A NOVEL MATHEMATICAL MODEL OF CELLULAR APOPTOSIS UNDER THE INFLUENCE OF HSP70
Ashwin Pavan Ramachandran
Randolph School, Huntsville, AL

Over the past decade, hyperthermia (heat stress) therapy has been garnering renewed attention and scrutiny as an adjuvant therapy to traditional forms of cancer treatment. Since it has never been proven to be effective when used alone, it is often combined and used in combinatorial therapy. Hyperthermia releases several apoptosis inducing factors and is generally effective in the short term. However, as experimental results have shown, it is not as effective in inducing cell apoptosis in the long term. This is because the affected cells tend to invoke a self-defense mechanism and develop thermo-tolerance to the applied heat stress. This is done by the expression of a certain family of proteins, called heat-shock proteins, which exhibit several inhibitory functions during apoptosis. These heat shock proteins, specifically Hsp70, are responsible for the inhibition of hyperthermia induced apoptosis. A novel mathematical model of cell apoptosis under the inhibition of such proteins is presented in this study. Unlike previous models, it is proposed that Hsp70 does not inhibit the release of cytochrome-c, an important apoptotic element downstream of mitochondria, but rather the pro-apoptotic component Bcl-x, otherwise known as Bax. The results from the proposed model are in agreement with observed trends seen in experiments and further experiments are suggested to fine tune the model and for conclusive validation of the hypothesis. Once validated, this model can be used to understand and explain the mechanism of cell apoptosis due to heat shock and to help improve hyperthermia therapy by attenuating some of the perceived shortcomings. It is postulated that once perfected, hyperthermia can be a resourceful combinatorial therapy tool in treating conventional drug resistant maladies. 

Awards won at the 2013 ISEF
All expense paid trip to tour CERN - European Organization for Nuclear Research-CERN
________________________________________
2011 - MA032 
METHOD OF OPTIMIZING THE MONTE CARLO STATISTICAL ALGORITHM TO INCREASE COMPUTATIONAL EFFICIENCY IN MULTIDIMENSIONAL INTEGRATION
Pratheek Nagaraj
Marjory Stoneman Douglas High School, Parkland, FL

The Monte Carlo method (MCM) based computational algorithms are extensively used in scientific fields to solve complex problems ranging from climate modeling to financial risk analysis. MCMs are used for modeling phenomena involving a large number of variables with significant uncertainty, where deterministic techniques are impractical to compute an exact result.<br><br>This project focused on implementation, evaluation, accuracy and feasibility of MCM multidimensional integration. The research goal was to identify a more computationally efficient and accurate MCM than the Plain MCM and to further analyze the various components of the optimized algorithm to develop a novel hybrid MCM algorithm. <br><br>This study analyzed algorithms in C++ and Mathematica for Plain, Quasi, Adaptive, MISER, and VEGAS MCMs and tested them through extensive simulations of multidimensional integration scenarios involving oscillatory functions, exponential functions, and localized variance, while spanning various sampling sizes totaling 1200 trials. <br><br>Compared to the Plain MCM, the VEGAS MCM was 89.68% more accurate and the Quasi MCM was 98.50% more accurate with the exclusion of Integral #5 which highly skewed the results; both VEGAS and Quasi MCM were significantly more precise. Overall, the VEGAS MCM outperformed the Plain MCM in accuracy and efficiency, demonstrating the optimal blend of importance sampling with recursion, quasi random numbers, and probability distributions. <br><br>Ultimately, this research serves as the foundation for my development of a novel hybrid MCM, using the VEGAS MCM as the base model, which will dynamically adapt to complex scenarios where the current methods fail to accurately compute a reliable solution. 

Awards won at the 2011 ISEF
Third Award of $1,000 - Mathematical Sciences - Presented by Intel
________________________________________
2013 - MA032 
A NOVEL MATHEMATICAL MODEL OF CELLULAR APOPTOSIS UNDER THE INFLUENCE OF HSP70
Ashwin Pavan Ramachandran
Randolph School, Huntsville, AL

Over the past decade, hyperthermia (heat stress) therapy has been garnering renewed attention and scrutiny as an adjuvant therapy to traditional forms of cancer treatment. Since it has never been proven to be effective when used alone, it is often combined and used in combinatorial therapy. Hyperthermia releases several apoptosis inducing factors and is generally effective in the short term. However, as experimental results have shown, it is not as effective in inducing cell apoptosis in the long term. This is because the affected cells tend to invoke a self-defense mechanism and develop thermo-tolerance to the applied heat stress. This is done by the expression of a certain family of proteins, called heat-shock proteins, which exhibit several inhibitory functions during apoptosis. These heat shock proteins, specifically Hsp70, are responsible for the inhibition of hyperthermia induced apoptosis. A novel mathematical model of cell apoptosis under the inhibition of such proteins is presented in this study. Unlike previous models, it is proposed that Hsp70 does not inhibit the release of cytochrome-c, an important apoptotic element downstream of mitochondria, but rather the pro-apoptotic component Bcl-x, otherwise known as Bax. The results from the proposed model are in agreement with observed trends seen in experiments and further experiments are suggested to fine tune the model and for conclusive validation of the hypothesis. Once validated, this model can be used to understand and explain the mechanism of cell apoptosis due to heat shock and to help improve hyperthermia therapy by attenuating some of the perceived shortcomings. It is postulated that once perfected, hyperthermia can be a resourceful combinatorial therapy tool in treating conventional drug resistant maladies. 

Awards won at the 2013 ISEF
Third Award of $1,000 - Mathematical Sciences
________________________________________
2012 - MA032 
ON CONVEXITY-PRESERVING SET STRUCTURES
Jeffery Brunson Holste
Mills E. Godwin High School, Henrico, VA

The purpose of this research was to develop further the field of convex analysis. While previous theorems have shown how to identify convex kernels of sets, I proved one of these theorems with a new, innovative method to better illustrate the structural aspects of sets. Then, this theorem was generalized to n-dimensions and a system was developed with which one can identify classes of sets which share common inflection points. Using this newly gained insight, I developed a method by which most 2-dimensional sets which have a differentiable Jordan curve boundary can be converted into a vector which, when unchanged, preserves the convex kernel as long as the intersection of the convex kernel with the boundary of the original set remains the same. This vector serves a very important function. Simultaneously, it reduces the entire structure of a set to a single vector and it preserves useful structural information about the shape of the set. This vector representation of sets allows for more sophisticated, accurate analysis of mathematical and natural phenomena, especially those which are amorphous or difficult to describe.<br><br> Furthermore, these vectors were converted into a vector space and vector addition and scalar multiplication were defined. I proved the essential properties of vector spaces, thus allowing for manipulation of sets and their shapes to determine how they change under certain conditions. Through my proofs of these properties and my development of this new field within convex analysis, several applications were explored, including the analysis of tumor growth and the dispersion of oil spills. Further research should focus on generalizing this vector to n-dimensions and applying this analysis to the morphological study of natural phenomena. 

Awards won at the 2012 ISEF
Fourth Award of $500 - Mathematical Sciences - Presented by Intel
________________________________________
2011 - MA033 
THE COST OF BLACKJACK: A SECOND-YEAR EXPECTED VALUE STUDY
Casey Ann Gibson
Nettleton High School, Jonesboro, AR

The purpose of this project was to calculate probabilities and expected values (EVs- average amount of money lost per dollar bet) for specific hands in the card game “blackjack”, and to use the results to better inform potential blackjack players of all possible outcomes they face. The hypothesis was that potential hands (pairs) of cards dealt to the player and dealer (in addition to another card that may or may not be taken) will have a wide range of probabilities and expected values because of the colossal number of possible outcomes.<br><br>First the probabilities of the player’s possible hands were calculated, followed by the dealer’s possible hands. The product of the two was then either directly inserted into the EV formula ,µ=xP(x), or (if either the player or dealer took another card) the product was then multiplied by the probability of winning, tying, or losing on the second draw. This final product then was used in the EV formula, via an extensive EV table.<br><br>The results proved the hypothesis correct. EVs ranged from -0.000120356 cents per dollar bet to +0.116231108 cents per dollar bet. The data showed that the closer a player’s original sum was to twenty-one, the higher their EV was. When the player was dealt seventeen or more points and the dealer was also dealt seventeen or more points, or less than seventeen points, the player’s EV was positive. However, if the player was dealt less than seventeen points and the dealer was dealt seventeen or more points, their EV was negative. So in conclusion, the possible decisions made by the player in blackjack can make the game profitable. 
________________________________________
2012 - MA033 
A UNITARY GROUP RELAXATION OF THE TRAVELING SALESMAN PROBLEM AND ITS APPLICATIONS
Anirudh Prabhu
West Lafayette Junior/Senior High School, West Lafayette, IN

The famous Traveling Salesman Problem (TSP) is one of the most researched optimization problems. It underlies many practically important problems. Although a considerable body of work has been done on TSP, to date, no computationally efficient algorithm for the problem is known (which is not surprising since the problem is known to be NP-complete). Devising a deterministic worst-case polynomial-time algorithm for this problem, or even showing that one does not exist, would resolve one of the six most difficult unsolved (millennium) problems in mathematics. <br><br> This project focuses on a radically new group-theoretic approach to TSP based on a unitary group relaxation. The TSP can be viewed as optimizing a real function over a discrete group. The discrete group is relaxed to the unitary supergroup. Computational experiments on the resulting relaxation are found to yield good upper bounds for the optimal solution.<br><br> The relaxation is then applied to the problem of arranging micro-array gene expression data in chronological order. <br><br>The performance of the relaxation on time-ordering of data is presented. 

Awards won at the 2012 ISEF
Third Award of $250 - American Mathematical Society
Second Award of $500 U.S. savings bond - Ashtavadhani Vidwan Ambati Subbaraya Chetty Foundation
Third Award of $1,000 - Mathematical Sciences - Presented by Intel
________________________________________
2013 - MA033 
ELECTROMECHANICAL MODELING OF THE HEART IN MOVING DOMAINS USING THE PHASE-FIELD METHOD
Kevin K. Lee
University High School, Irvine, CA

Computational models of the heart have proven essential to the study of cardiac arrhythmias, which are poorly understood yet the leading cause of death in the industrialized world. Models provide an unobstructed view of the heart's electrical behavior on both the surface and interior, but they have been limited in effectively incorporating mechanical effects from contraction due to the difficulty in handling the moving boundaries of the domain of the governing partial differential equation system. Here, we develop a novel method for combining the propagation of electricity with mechanical contraction using the phase-field method. We modify the Fenton-Karma model of cardiac action potentials to incorporate a simple moving domain and validate our algorithm by demonstrating convergence. Then, using Darcy’s Law, we consider the heart as a fluid with an elastic boundary to model the elasticity of the tissue, and we couple local contraction of the domain with the power stroke of the action potential. Our algorithm is independent of the numerical scheme and can easily be applied to 3-D anatomical models of the heart. The theory developed here facilitates more realistic simulations of the heart, thus giving drug developers a more complete tool in designing therapies for heart conditions, perhaps yielding critical insight on the underlying mechanisms of fatal conditions like fibrillation, and enabling dramatic improvements in their treatment and prevention, both of which are very suboptimal at present. 

Awards won at the 2013 ISEF
Third Award of $1,000 - Mathematical Sciences
________________________________________
2012 - MA033 
A UNITARY GROUP RELAXATION OF THE TRAVELING SALESMAN PROBLEM AND ITS APPLICATIONS
Anirudh Prabhu
West Lafayette Junior/Senior High School, West Lafayette, IN

The famous Traveling Salesman Problem (TSP) is one of the most researched optimization problems. It underlies many practically important problems. Although a considerable body of work has been done on TSP, to date, no computationally efficient algorithm for the problem is known (which is not surprising since the problem is known to be NP-complete). Devising a deterministic worst-case polynomial-time algorithm for this problem, or even showing that one does not exist, would resolve one of the six most difficult unsolved (millennium) problems in mathematics. <br><br> This project focuses on a radically new group-theoretic approach to TSP based on a unitary group relaxation. The TSP can be viewed as optimizing a real function over a discrete group. The discrete group is relaxed to the unitary supergroup. Computational experiments on the resulting relaxation are found to yield good upper bounds for the optimal solution.<br><br> The relaxation is then applied to the problem of arranging micro-array gene expression data in chronological order. <br><br>The performance of the relaxation on time-ordering of data is presented. 

Awards won at the 2012 ISEF
Tuition Scholarship Award in the amount of $8,000 - Office of Naval Research on behalf of the United States Navy and Marine Corps
Each winning project will receive $3,000 in shares of UTC common stock. - United Technologies Corporation
________________________________________
2011 - MA034 
RBCAM: A RULE-BASED DISEASE TRENDING TOOL
Anjana Ram
Texas Academy of Mathematics & Science, Denton, TX

Influenza is one of leading causes of hospitalizations and death in the United States. Because of shortages of the vaccine and inefficient administration, a novel vaccination regime is required that can manage epidemic spread. This study establishes the impact of strategic vaccination on the spread of influenza using a rule-based cellular automata model (RBCAM). The simulations establish dependencies on the initial infection spread and vaccination of the population. The model is parameterized with factors of disease spread such as population infection rates (PIR), infection period (PIP), infection (PIT), and re-infection threshold (PRT). <br><br>The compartmentalized model uses a discrete time and space domain cellular automata, where each state is determined by a single or a range of values representing the state. A random walk simulation method is incorporated to include dynamic nature of infection probability among the neighboring cells. MATLAB is used in implementing RBCAM.<br><br>Results indicate that the value of input parameters influence the disease spread significantly. The initial infection distribution affects the spread of influenza, whereby selection of strategic vaccinated areas aided in preventing further spread by identifying specific cells as hot spots for immunization. The peak infection decreased at a reasonably consistent rate, with increasing vaccination showing its favorable influential effects on the rate of disease spread. The data compares well for the years 2006-2010 influenza seasons, and the error is within +/- 21%. The model is effectively used in considering other interrelated parameters such as population distribution, dual viral interactions and inter-cell mixing. 
________________________________________
2013 - MA034 
INTEGRALS OF RATIONAL FUNCTIONS
Magda Lee Hlavacek
Saginaw Arts and Sciences Academy, Saginaw, MI

Some groups of functions are closed under differentiation and integration, and others are not. The ring of polynomials, for example, is closed under differentiation and integration. <br><br> <br><br>Rational functions are functions that can be expressed as a ratio of polynomials. The derivative of a rational function is always a rational function. In some cases, the integral of a rational function is also a rational function. However, in other cases, the integral of a rational function is not a rational function. <br><br>The goal of this project is to develop an effective decision procedure that can determine whether a given rational function integrates to a rational function without factoring the denominator. (Otherwise one could simply integrates the function using partial fraction decomposition.) This goal was achieved by reducing the problem to a system of linear equations that has a solution if and only if the given rational function integrates to a rational function. 
________________________________________
2012 - MA034 
A RELATIONSHIP BETWEEN QUADRATIC CHARACTER AND REPEATING DECIMALS
Junda Huang
James Madison High School, Brooklyn, NY

The theory of quadratic residue is an old and important topic in number theory. Finding an algorithm to determine the quadratic character of a modulo p has been the focus since Gauss's first researches presented in Disquisitiones Arithmeticae. There exists a relationship between the repeating decimals of a/p, where p is an odd prime, and the quadratic residues modulo p. If a and p have the same quadratic characters mod p, then in certain bases, the decimal representation of a/p can be obtained by rotating the decimal representation of b/p. If 2 is a primitive root mod p, then the quadratic character of mod p can be determined by evaluating the fraction a/p and comparing it with 1/p in base 4. This can also be generalized to other bases of perfect squares (9, 16, 25, etc.). 

Awards won at the 2012 ISEF
Third Award of $1,000 - Mathematical Sciences - Presented by Intel
________________________________________
2013 - MA035 
RECOVERING THE EFFECTIVE THERMAL CONDUCTIVITY OF SIMBA SEA ICE WITH A COMPOSITE MATERIAL MODEL
Daniel Wenchu Liu
West High School, Salt Lake City, UT

The global climate changes we observe can be linked to the rapid reduction of sea ice in recent years, and the thermal conductivity of sea ice plays a prominent role in the studies that aim to identify the causes. Sea ice is a complex, composite material consisting of pure ice, brine, and air inclusions, which makes it very difficult to model. This study aims to find a composite material model that best describes the heat transfer within the medium by solving an inverse problem. Sea ice thermal array measurement data from the ``Sea Ice Mass Balance in the Antarctic'' (SIMBA) project is used. The temperature measurements are divided into two sets. One set is used to train the model and the other is used to validate the model. This study first proposes a three-layer model by dividing the sea ice into three different layers. Then it makes a comparison with a four-layer and five-layer model. Finally, this study compares the layered models with a series of polynomial models up to the 7th power. For each of the proposed models, the estimated temperature profile is recovered from the sea ice heat equation using the Runge-Kutta method. The estimated temperature profile is compared with the training data and a root mean squared errors (RMSE) is calculated to measure the discrepancy. The Nelder-Mead method is used to find the optimal model by minimizing the RMSE. The optimal model is verified by calculating its validation RMSE using the validation data. It is found that a five-layer model is the most effective model as it has the lowest RMSE. 
________________________________________
2012 - MA035 
NOVEL OPTIMIZED RUNGE-KUTTA METHODS TO INCREASE COMPUTATIONAL ACCURACY IN NUMERICAL INTEGRATION OF DIFFERENTIAL EQUATIONS
Pratheek Nagaraj
Marjory Stoneman Douglas High School, Parkland, FL

Differential equations (DE) are used in a variety of everyday scenarios to model phenomenon such as population growth and heat diffusion. Often models of such dynamic systems are non-linear and lack analytical solutions, hence numerical approximation techniques like the Runge-Kutta methods are used to solve the differential equations. <br><br><br>In this study, 5 novel approaches are proposed towards optimizing the Runge-Kutta family of methods to solve Initial Value Problems and thereby increase the computational accuracy. The intermediate step values for these proposed novel methods are based on the properties of Standard Normal Gaussian distribution, uniquely determined coefficients of a Pascal Triangle variant, and roots of the Hermite Polynomial. To test the accuracy, both existing and novel methods were implemented in C++ to solve a combination of multiple linear and nonlinear ordinary/partial differential equations with existing analytical solutions that represent real-world problems from physics to biology.<br><br><br>The data of this research conclusively show that the novel methods on are on average 300%-1500% (0.5 to 1.5 orders of magnitude) more accurate than the existing 5th order Runge-Kutta methods thereby indicating superior accuracy and efficiency for similar computational times. The results also indicate that the novel methods have a better distribution of intermediate step values than the existing methods. The proposed novel methods can be adapted to any order Runge-Kutta methods and also be transformed into implicit methods demonstrating their versatility.<br><br><br>Ultimately, this research helps improve the accuracy in mathematical modeling of the world around us saving time, money, and resources. 

Awards won at the 2012 ISEF
Third Award of $1,000 - Mathematical Sciences - Presented by Intel
Second Award of $1,500 - Air Force Research Laboratory on behalf of the United States Air Force
________________________________________
2013 - MA036 
DISCRETIZATION OF INFINITE DIMENSIONAL GEODESICS
Cole Wilkening Hugelmeyer
Boulder High School, Boulder, CO

In infinite dimensions, it is known that the Hopf-Rinow theorem, which establishes the existence of geodesics, fails. Therefore, it is natural to look for a more general criterion for the existence of geodesics that is satisfied in the infinite dimensional case. In this research, the existence and uniqueness problem for distance minimizing geodesics on an embedded Hilbert manifold is studied, and a new mathematical machinery is developed for proving the existence and uniqueness of geodesics in locally noncompact space. We consider the existence of distance minimizing paths in a general metric space, and then apply the results to a general embedded Hilbert manifold with an extrinsic metric. We then consider the application to an ellipsoid in Hilbert space, and derive a result for geodesic completeness on certain hemi-elliptical caps. The Hilbert space ellipse was the first example of a manifold on which geodesics do not necessarily exist between any two given points, and the fact that this method is applicable to such a space suggests that it can be applied to a wide variety of situations that have previously been considered pathological. 
________________________________________
2011 - MA036 
RATIONAL APPROXIMANTS FOR EULER-GOMPERTZ CONSTANT
Vasily Sergeevich Bolbachan
Advanced Science and Education Center - A.N.Kolmogorov School, Moscow, Moscow Region, RUSSIA

We obtain two sequences of rational numbers whose ratio converges to the Euler-Gompertz<br><br>constant.<br><br>Denote <f(x)> by the integral of f(x)exp(-x) from 0 to infinity.<br><br>Recall that the Euler-Gompertz consant is <ln(x+1)>.<br><br>Main idea. Let P_n(x) be a polynimial with integer coefficients. It is<br><br>easy to prove that <P_n(x)ln(x+1)>=a_n+<ln(x+1)>b_n for some integers a_n, b_n. Hence if<br><br><P_n(x)ln(x+1)>/b_n converges to zero, a_n/b_n converges to<br><br>-<ln(x+1)>.<br><br>Main Theorem. Let u be positive real. There exists polynomials P_n(x) (they are explicitly given in<br><br>the paper) such that <P_n(x)ln(xu+1)> tends to u as n tends to infinity.<br><br>Proof of Main Theorem is elementary.<br><br>Also we present some conjectures. 

Awards won at the 2011 ISEF
Third Award of $250 - American Mathematical Society
________________________________________
2012 - MA036 
LORENZ & MODULAR FLOWS ARE KNOT SIMILAR
Anita Kummamuri Rao
Glenda Dawson High School, Pearland, TX

Two different dynamical systems were examined. First are the ordinary non-linear differential equations underlying atmospheric convection introduced by Lorenz as an example of a chaotic dynamical system. Second, in the field of number theory, is the dynamics of the modular group SL(2,Z) on 2-dimensional normalized lattices. The purpose of this project was to graphically investigate the relationship between these two seemingly unrelated fields and to use knot theory to show their underlying similarity. <br><br>The project consisted of looking for periodic solutions to the two different systems. For the Lorenz model, the solutions which traced closed paths around the strange attractor are called as Lorenz Knots. Some of these knots were listed and categorized. In the case of hyperbolic flows on the space of unit lattices, the closed paths corresponding to a few matrices in the modular group were identified, and characterized as modular knots. For one of these matrices, the Eisenstein series terms g2 and g3, the Weirstrass discriminant, and the corresponding Rademacher function were calculated.<br><br>The results show that the class of modular knots coincide with the class of Lorenz knots. 

Awards won at the 2012 ISEF
Certificate of Honorable Mention - American Mathematical Society
________________________________________
2011 - MA036 
RATIONAL APPROXIMANTS FOR EULER-GOMPERTZ CONSTANT
Vasily Sergeevich Bolbachan
Advanced Science and Education Center - A.N.Kolmogorov School, Moscow, Moscow Region, RUSSIA

We obtain two sequences of rational numbers whose ratio converges to the Euler-Gompertz<br><br>constant.<br><br>Denote <f(x)> by the integral of f(x)exp(-x) from 0 to infinity.<br><br>Recall that the Euler-Gompertz consant is <ln(x+1)>.<br><br>Main idea. Let P_n(x) be a polynimial with integer coefficients. It is<br><br>easy to prove that <P_n(x)ln(x+1)>=a_n+<ln(x+1)>b_n for some integers a_n, b_n. Hence if<br><br><P_n(x)ln(x+1)>/b_n converges to zero, a_n/b_n converges to<br><br>-<ln(x+1)>.<br><br>Main Theorem. Let u be positive real. There exists polynomials P_n(x) (they are explicitly given in<br><br>the paper) such that <P_n(x)ln(xu+1)> tends to u as n tends to infinity.<br><br>Proof of Main Theorem is elementary.<br><br>Also we present some conjectures. 

Awards won at the 2011 ISEF
Fourth Award of $500 - Mathematical Sciences - Presented by Intel
________________________________________
2013 - MA037 
ZETA FUNCTION-LIKE SUMS OVER LUCAS NUMBERS
William Kang
Bergen County Academies, Hackensack, NJ

The study of sequences extends in a wide range from the golden ratio to the current financial trading algorithm. Although not realized by many, both the Fibonacci and Lucas sequences are incorporated into the algorithm today. In addition, among the far-reaching applications of these numbers are the Fibonacci search method and the Fibonacci heap data structure in computer science. Hence, mathematicians seek to find more results and further studies into these sequences for the purpose of greater potential benefits. In the area of mathematics, Fibonacci and Lucas numbers are used in connection to efficient primality testing of Mersenne numbers; the method revolves around the fact that if Fn is prime, then n is prime with the exception for F(4)=3. Some of the largest known primes were discovered via this process.<br><br> Yet, despite the extensive literature on Fibonacci and Lucas numbers, there are still many questions, which are still unanswered. <br><br> Recently, several authors considered the problem of estimating expressions of the classical Fibonacci and Lucas sequences given by F0=0, F1=1, Fn= Fn-1 + Fn-2, and L0 =2, L1=1, Ln= Ln-1 + Ln-2, respectively. <br><br> In this paper, we find the exact asymptotic behavior for a class of infinite partial sums whose general term is a negative integer power of L(k). Most of the previous works focused on the relatively simple cases s = 1 and s = 2 where s represents the power. For this paper, we find the general idea in which expressions could be found for cases of s from 1 to 6.<br><br> 
________________________________________
2011 - MA037 
MATH OUT OF THE BOX! CREATIVE METHODOLOGY IN SIMPLIFYING MATH
Eithar Mohammedamin Abdulhalim
49 Secondary, Makkah, Central, SAUDI ARABIA

The nature of math really needs specific mental abilities which makes this subject hard for many students. To create creative method to solve the math operations. a field study was applied where 2 school visits were conducted and created methods were explained for students of 4 th and 5th grade. In comparing the students' results with the traditional ways and the created method, the former one was more accurate. Results: saving time in the created method than the traditional one to its half, the probability of success of this method is 95% while the tradition way is 75%<br><br>Testing again for students according to the creative method as well as calculation of the time required for the solution of operation and the extent to which they comprehend solution according to the newly (created) method and percentage of correctness of answer.<br><br>I have recorded the results obtained by female students and summarized it in the shape of graph accompanied by percentage for students' assimilation.<br><br>During the conduct of the experiment, I have dealt with three variables. <br><br>The amazing results I reached in the understanding of female students for the new (created) method, their assimilation for it and the speed of its application led me to give the following recommendations:<br><br>1- It is possible that the method can be applied for the male and female students of the second and third classes of elementary stage.<br><br>2- Male and female secondary schools students can use this method in the assessment and ability test for the rapid solution of problems.<br><br>3- It is possible to explain this method to low- understanding male and female students as it is easily understood, assimilated and applied. 
________________________________________
2012 - MA037 
NOVEL GRAPH THEORY ALGORITHMS FOR PROTEIN STRUCTURE PREDICTION AND DESIGN
Jonah Milton Kallenbach
Germantown Academy, Fort Washington, PA

This project examined Graph theoretic algorithms for protein structure prediction and design, particularly the ab initio problem, that is, predicting structure from amino acid sequence. The widely used SCWRL4 (Side Chain with Rotamer Library) algorithm, does protein structure prediction using a tree decomposition of the Residue Interaction Graph (RIG) of a protein. Given a (generally) sparse RIG G, a tree decomposition is a pair (T,W) where T is a tree with groups of nodes from G at the vertices, and W is the set of sets of vertices at these nodes, satisfying a particular group of rules. The related concept of a branch decomposition is a pair (T,v) where T is a ternary tree and v is a bijection from the edges of G to the leaves of T. The mathematical structures of these objects allow us to compute the general minimum energy conformation in a very efficient manner. Here three new algorithms are presented for protein structure prediction and design. The first two exploit the closeness of the concepts of the decompositions, using a hybrid graph to combine and optimize them. The third, a novel branch decomposition based algorithm, utilizes the energy minimization heuristic of SCWRL4 on a branch decomposition of the RIG. These algorithms, using O(n), asymptotic runtime notation, are faster than existing algorithms and do not encounter the memory issues of previous branch decomposition algorithms. Also, a powerful modification to the original SCWRL4 algorithm was made, including a new heuristic, treeweight, to bias the algorithm against selecting bags with large numbers of rotamers. Future work will include efficiently implementing these algorithms which will decrease runtimes and assist biomedical researchers with their crucial protein design and structure prediction needs. 

Awards won at the 2012 ISEF
Scholarship Award of $15,000 per year, renewable annually - Florida Institute of Technology
Third Award of $1,000 - Mathematical Sciences - Presented by Intel
________________________________________
2012 - MA038 
PATTERNS AMONG CAROUSEL PRIMES AND CYCLIC INTEGERS IN BASE B
Alan Ziyuan Yang
Upper Dublin High School, Fort Washington, PA

This project examines the interesting numerical and visual properties that exist among carousel primes, also known as full reptend primes, and their closely related cyclic integers in natural base, b. Carousel primes, p, are special primes such that 1/p has a repeating decimal period of length p – 1 in base b. Not every prime is a carousel prime. Moreover, each carousel prime has a corresponding cyclic integer that is comprised of the digits in the decimal period of 1/p. One of the main results of this project is that the ratio of the number of carousel primes less than an arbitrary number N to the total number of primes less than the same number N converges as N approaches infinity in nearly every base, b. In other words, the density of carousel primes among all the primes converges. Also, the decimal period patterns of the reciprocals of carousel primes were analyzed visually using flower graphs. Furthermore, carousel primes can potentially offer valuable information for the long-standing problem of finding an overall pattern among all the primes. As a related topic, research on the properties of carousel primes and cyclic integers can hence be applied to broader problems. In addition, such research can be practically applied to cryptography. 
________________________________________
2013 - MA038 
A NOVEL APPROACH TO THE SPHERICAL CODES PROBLEM
Simanta Gautam
Albemarle High School, Charlottesville, VA

A spherical code is a finite set of points on the surface of a sphere in n dimensional space. The spherical codes problem asks for the maximum number of points in a spherical code where the angle between any two points with respect to the center is at least alpha. A traditional approach to this problem is to define an energy function for a spherical code as the sum of the inverse distance of every pair of points and optimize for minimum energy using gradient descent. However, a method to globally minimize the energy function for any given parameters is unknown, especially for spherical codes in higher dimensions. In our work, we improve this known approach by imposing symmetry groups on the spherical codes, as most configurations are invariant under reflection groups of certain Euclidean lattices. Furthermore, we develop a novel algorithm that individually separates pairs of points by treating them as unit vectors and applying gradient flow on their dot products. Using this approach, we were able to reproduce configurations for many of the best known spherical codes found in literature and find new configurations for dimension 6. 

Awards won at the 2013 ISEF
Third Award of $250 - American Mathematical Society
Second Award of $1,500 - Mathematical Sciences
________________________________________
2011 - MA038 
MATHEMATICAL FLOWERS: PATTERNS IN DOTS GENERATED BY INTERSECTION POINTS
Vahid Fazel-Rezai
Red River High School, Grand Forks, ND

When the diagonals of a convex pentagon are drawn, a new, smaller convex pentagon is formed inside the first pentagon. Such an interesting pattern led to the asking of a more general question, ``Are there more patterns like this with interesting properties?'' A three-step algorithm was used to describe the process: First draw all line segments between any two starting points. Then, take all points lying on at least two line segments as the new set of points. FInally, take out of the new set all points in the starting set. This rule simplifies to the intersection of diagonals for convex polygons. To develop an easier way of exploring these, a MATLAB program was developed to be able to plot the intersection points for larger polygons. These images consisted of patterns of concentric circles and points, which resembled flowers. On the other hand, not all starting sets of points simplify the algorithm to intersection points. Such arrangements include collinear points and continuous curves. Here, the algorithm could be applied multiple times to form a sequence. Some inquiry in this area found some starting sets that produced a infinitely large and infinitely small sequences, along with other intersting shapes. As a result of this project, more questions and possibilities were opened to inquiry. 

Awards won at the 2011 ISEF
Fourth Award of $500 - Mathematical Sciences - Presented by Intel
________________________________________
2013 - MA039 
ESTIMATION OF POPULATION SIZE USING SAMPLING WITH AND WITHOUT REPLACEMENT
Shikha Sreenivasan
Tafuna High School, Pago Pago, AMERICAN SAMOA

Researchers often need to determine exact number of individuals in a population to plan and implement effectively the various policies related to their population. In many situations, counting the exact number of individuals directly is extremely difficult or sometimes impossible. Hence it is important to select samples from the population to estimate their sizes and the use of sampling has become popular for studies in population dynamics and structure. There are numerous sampling methods available to estimate population sizes and each method has its own benefits and limitations depending upon the specific circumstances of the inquiry. Through this experiment, I have tried to analyze how various sample sizes affect the accuracy of estimation of a population using two approaches: sampling with and without replacement. The population size was estimated mathematically by using Bailey’s modification of Lincoln index (capture-recapture method) and Hayne’s method combined with linear regression analysis (removal sampling) which are approaches that are very popular among ecologists and naturalists. The former is a sampling procedure with replacement while the latter is a sampling procedure without replacement. Moreover this experiment helped me to compare the mean estimated population with the actual population and identify the least affected indirect estimation procedure with respect to sample size. 
________________________________________
2011 - MA039 
ENTRIES OF RANDOM MATRICES
Benjamin Jerome Kraft
Liberty High School, Bethlehem, PA

Let U_n be the group of n by n unitary matrices. To select a random unitary matrix, we use the Haar measure. Much study has been devoted to the eigenvalues of random unitary matrices, but little is known about the entries of random unitary matrices and their powers. In this work, we use eigenvalues to understand the entries of random unitary matrices and their powers. We characterize the exact distribution of the top-left entry in the case where the matrix is raised to a power at least n, and give some relationships for smaller powers. These results may have applications in quantum mechanics, telephone encryption, and statistical analysis, in addition to helping illuminate the field of random matrix theory. 

Awards won at the 2011 ISEF
Third Award of $250 - American Mathematical Society
________________________________________
2012 - MA039 
DEVELOPING A NOVEL TEST TO DETECT CANCER GENES FROM MICROARRAY DATA
Shreya Mathur
Oxford High School, Oxford, MS

DNA microarray technology can simultaneously screen thousands of gene expression profiles, transforming how genetics is applied in medicine. However, the lack of normality in microarray data renders common statistical methods ineffective. We propose a novel statistical method, coined the S-test, which does not require stringent assumptions but is still more powerful than some of its competitors. Using both simulation studies and clinical data, we show that our novel method outperforms previous methods. The limiting distribution is obtained under null and alternative hypotheses. The proposed test will help make cancer treatment and gene therapy more successful and may facilitate research regarding cancer vaccinations. It may also help in the development of a prediction model in genetic profiling studies built on a subset of differentially expressed genes and the clinical data to assess the accuracy of the clinical prediction. 

Awards won at the 2012 ISEF
First Award of $1,000 - American Statistical Association
Fourth Award of $500 - Mathematical Sciences - Presented by Intel
________________________________________
2012 - MA040 
PROPERTIES OF PERIODS OF PRIMES
Shubham Goel
Delhi Public School, R.K. Puram, New Delhi, Delhi, INDIA

The period of a prime is defined as the number of digits in the non-terminating repeating decimal expansion of its reciprocal, for example 11 has period 2 (The prime nature guarantees that the representation of the reciprocal does not contain any non-repeating digits). In this project, I prove important properties of periods of primes and setup a factorization experiment to investigate the number of primes with a given period. Theoretically, I establish the following:<br><br>1) For every natural number i, there exists a prime with period i.<br><br>2) I extended this result to satisfy any base system in general using the Zsigmondy’s Theorem. <br><br>3) The number of primes with period i is between 1 and i.<br><br>4) In base 2*i(the quarter imaginary base system), the length of the period of a prime number is either 1,2 or 4 times the length of the same in base 2.<br><br>I further improve Result 3 by relating the problem to counting 'new' factors in the repunit sequence. I use a quadratic sieve, which runs in L_n [1/2,1], to factor the first 600 base 10 repunit numbers. To derive a best fit, taking a logarithmic approach gives us more interesting results and helps to prove that the approximate number of primes with period i is under greatest integer of [log(i/2) + 5](There is no polynomial relationship between the two quantities), drastically less than the theoretical bound. This bound works for all of the tested cases. Result 1 and 2 find applications in primality testing for cryptosystems, by giving way to a ‘continuous prime search’ (by searching with periods) rather than current discrete ones, helping to reduce processing time and memory consumption. 
________________________________________
2011 - MA040 
PROOF OF AN ARCHIMEDEAN PROPOSITION USING EUCLIDEAN GEOMETRY
Alan Ziyuan Yang
Upper Dublin High School, Fort Washington, PA

This study was conducted to prove Archimedes’ fifth proposition in his Book of Lemmas using Euclidean geometry. The hypothesis is: if given proposition five, then this proposition can be proven using Euclidean geometry, algebra, and deduction. In his book, Archimedes defines an arbelos to be the region bounded by three mutually tangent semicircles such that the diameter of one of the semicircles contains the diameters of the other two. The main goal of this proof was to justify that if a segment is constructed within an arbelos such that it contains a point of tangency and is perpendicular to the line containing the centers of a semicircles, then the two circles inscribed within the arbelos and this segment have equal area. Based on the original proposition, a diagram was completed. By using algebraic variables and the Pythagorean Theorem, the radii of the two circles were shown to be equal. Hence, the proposition was proven. The hypothesis was accepted and further studies can be conducted to investigate similar Archimedean propositions. 
________________________________________
2013 - MA040 
FRACTAL DIMENSION AND SPACE FILLING CURVES
Christian Lee Smith
Seneca High School, Seneca, MO

The purpose of my research was to discover if a fractal created by adding continual squares onto one-fourth of each leg of a scalene triangle fills space. My intention was to research the fractal dimension and space-filling curves by learning if a space-filling curve could be produced by simply creating a fractal that would expand both inward and outward. My hypothesis was that the fractal I had created would fill the space between the curve's sides and expand to fill and increasing amount of space, making a more and more intricate and complex shape after every iteration, as well as a space-filling curve. As long as the fractal expanded in both directions after every interation, it would produce a space-filling curve. My results showed that my curve filled space in much of the area, but failed to fill the entire area. There is nearly an infinite amount of space between any two given points of the coordinate plane. My fractal's edges became smaller and smaller with each iteration. Therefor my fractal can never fill the entire space, because it will continually grow larger, but within an infinite area. 
________________________________________
2013 - MA041 
TOOTHPICK PROBABILITY
Cecilia Louise Dean
Tishomingo County High School, Iuka, MS

Probability is used in every area of life. I wanted to conduct an experiment to prove probability is repeatable and predictable over time. I used a 60.96 cm by 101.6 cm sheet of paper with 15 parallel lines 6.6 cm apart. I taped the paper to a level table. After a centered point was found I dropped ten tooth picks one-by-one onto the paper. I recorded the column in which they fell onto a chart. I dropped the ten toothpicks ten times to complete one trial. I conducted ten trials. I also found the standard and mean deviation of each trial. The results concluded my hypothesis to be correct. The standard deviation ranged 2.0757 in each trial. The mean deviation ranged 1.5408 in each trial. Since the range is so small for each deviation, my hypothesis was proven correct. The data was repeatable; therefore, it was also predictable. The data, however, did not produce a perfect bell curve that was equally distributed on either side. A theoretical bell curve was produced to show the distribution of the percentages in each column. 
________________________________________
2012 - MA041 
THE ART OF CRAFTING FUNCTIONS: DIAGRAMMING MATH THROUGH CROCHET
Elise Gabriel Wright
Wasatch High School, Heber City, UT

The purpose of my project was to show how changing a variable in a function changes the function’s physical model, and I used hyperbolic crochet to explore this relationship. I chose to use hyperbolic crochet because, with an increase of one stitch to a set interval, it curls in a distinct way. I created my test function, which is as follows:<br><br> F(X)=((X + 1)/X)^(n – 1))*C<br><br>X is the rate of increase, n is the number of rows, and C is the initial number of chain stitches. I hypothesized that as X decreases, the surface area of the model would increase, as shown by a greater amount of curl. To test my hypothesis, I crocheted four different models, called coral patterns, with different X values. I used the same weight of yarn and hook as constants. I accepted my hypothesis: as X decreased, the surface area and curl of the coral pattern increased. Each of the coral patterns shows similar characteristics but distinctive shapes. I explored different ideas with the properties of crocheting functions, including summing the stitches and alternating the rate of increase between rows to see how that changes the shape. Both these parameters showed distinctive changes to the coral patterns. Using the formula, I can also predict the stitch count in any row. <br><br> Using hyperbolic crochet, we can better understand hyperbolic objects found in nature. We could discover the mathematic formulas for the growth of coral, lettuce, and even our universe. And, with these, we could model them. 
________________________________________
2011 - MA041 
ADDITIVE SEQUENCE
Nikita Maksimovich Nikita
High- School #1, Ekibastuz, Ekibastuz, KAZAKHSTAN

Purpose: Evaluate the difference between the lengths of the product fast additive chains and fast additive chain, if they are end up the same number.<br><br><br>Hypothesis for some integer k, there are fast additive chains a, b and c, such that<br><br>| | a b ||-|| c | | = k and a * b = c<br><br><br>Stages, the procedures of research: 1) the search of literature of the subject;<br><br>2) make plan, questions;<br><br>3) find solutions to questions;<br><br>4) design work;<br><br>Methods: 1) research;<br><br>2) analytical;<br><br>3) search;<br><br>Scientific novelty and degree of independence: found the formula for calculating the length of additive chains ending numbers , , the table of fast chains’ lengths for using numbers of the form , , two theorems and the lemma<br><br><br><br>Results and conclusions: We find the formula for calculating the length of additive chains ending number and the number , a table of fast-chains and their lengths, the numbers of the form and the numbers of the form , two theorems and the lemma are formulated and proved.<br><br><br>The practical use of the results: The results of the work can be used for performing calculations, preparation of algorithms and programs, reducing the number of operations when calculating the values of certain cumbersome formulas and large baselines. 
________________________________________
2013 - MA042 
EXPLORING THE REAL FUNCTIONS USING NEWTON-RAPHSON METHOD
Javier Diaz
University Gardens High School, San Juan, PUERTO RICO

This investigation is about the exploration of real functions with the Newton-Raphson Method. The objective of this research was to explore the real functions with Newton-Raphson method in which these are reflected in analytical solutions, chemical equations in finance and engineering. Since the g (a) is the largest root of the polynomialf(x)=20x^3-40x^2+25x-5+a comparisons between g’, and f'(g(x)) were determined. The polynomial solutions (roots) for values of a in [0,1], were calculated using Newton –Raphson Method. Several graphs were plotted showing the x axis intercepts for the values of a. Next the points of discontinuity that had g (a) in [0, 1] were found. The equation g (a) was determined with the roots previously obtained. Next the differential quotient was applied to approximate g'(x) in [0,1]. It was determined if the limits by the left and right of g ' exist at the point of discontinuity that is between [0, 1]. Finally a table in Excel for values of g ' and f'(g(x)), was developed on which the relationship between these two equations could be determined. Several generalizations were established for g '(a) and f' (g(x)); e.g. this decrease in [0, 0.37], and do not change in [0.38, 1]. The solutions of real functions has applications in science, and mathematics, e.g. in the work of chemical engineering is used to simulate chemical processes in the design of balancing chemical equations. The success of these branches of science is due largely to the Newton-Raphson method. 
________________________________________
2012 - MA042 
SWAP TO SORT: ANALYSIS OF THE WINNABILITY OF A FUNCTION-BASED GAME
Vahid Fazel-Rezai
Red River High School, Grand Forks, ND

First, a single-player sorting game played on a set of numbers was introduced. The game was played by taking turns swapping pairs of elements determined by restricting functions. It was noted that three parameters clearly define every game played with these rules, one of which was kept random from game to game. The other two, the starting set size and the restricting functions, were manipulated and examined. The question to be answered was: for which starting set sizes and restricting functions is it possible to sort the set and win the game? The first approach was based on the observation that the number of restricting functions is finite for a given starting set size. Therefore, by keeping one parameter constant, the winnability of the game could be determined for all resulting games. But the number of cases to check grew extremely quickly with starting set size and this approach was abandoned. The focus was then turned on the functions themselves, mapping them with lines connecting nodes on a graph. This gave a visual representation of how the swaps could occur. Based on this, a necessary and sufficient condition for winning was found: the representative graph must be connected. Furthermore, if one of the restricting functions is bijective, the resulting game is equivalent to a game with simpler restricting functions. This can be extended to find a short checking algorithm for any game with at least one bijective function. This algorithm runs in O(n^3) time, where n is the starting set size. In conclusion, a condition was found to check the winnability of a game through its graph interpretation. For bijective functions, this reduced greatly to a simple checking algorithm that runs in a short amount of time, which also provided a winning strategy. 
________________________________________
2011 - MA042 
TOWARD SOLUTION OF SOIFER-ERDOS PROBLEMS
Georgiy Vladimirovich Kolyshev
Stuyvesant High School, New York, NY

In this paper, we consider the twenty-one year old problems jointly posed by Alexander Soifer and Paul Erdos in a letter that is included in Soifer's _How Does One Cut a Triangle_. This book deals with several different natural generalizations of standard high school problems. In their most general form, these problems are extremely difficult and they do not yet have solutions. Suppose there is a closed, convex figure with points inside it, if we look at the triangles formed by three of these points and choose the one with the smallest area, we can obtain a value alpha by dividing the area of this triangle by the area of the figure. One of the problems posed by Soifer and Erdos asks for the smallest and largest values alpha so that this alpha is obtainable with five points or six points, not obtainable for four points, and always obtainable -- irrespective of the position of the points or the shape of the figure -- for seven points. We obtain the lowest value by applying results from Paolo Gronchi and Marco Longinetti's work and then improving their results (as applied to the problem) by extending methods developed by Dmytro Karabash to solve another one of the problems posed in the previously mentioned book. Additionally, we prove some asymptotic results for sequences related to the problem and provide a conjecture with reasoning for the largest value of alpha. 

Awards won at the 2011 ISEF
Certificate of Honorable Mention - American Mathematical Society
________________________________________
2011 - MA043 
SIZE DOES MATTER, BUT DOES SHAPE?
Tyler Gordon Toepke-Floyd
Wishek Public School, Wishek, ND

The purpose of my project was to see if the shape of a parachute affected the speed of its descent. To do this I kept the surface area of each parachute the same. The shapes of the parachutes were an equilateral triangle, a square, a rectangle, a pentagon, a hexagon, and a circle. The surface area of each of the parachutes was 144 square inches. <br><br> My procedure was as follows: 1. I used a plastic material and drew my parachutes according to mathematical formulas. 2. I cut them out and attached one twelve inch piece of string to each vertex. I attached eight strings for the circle in a double crossing pattern. 3. I attached one washer to each parachute. 4. I recorded each fall from a height of 138 inches repeated 10 times.<br><br> The results of my project were that the triangle fell the fastest followed by the rectangle. Then came the square, followed by the hexagon. The second slowest was the pentagon and the slowest was the circle. <br><br> In conclusion, I found that my hypothesis was partly correct. My hypothesis was that the parachutes would fall at the same rates. They didn’t all fall, however, at the same rates and there was about a difference of a second in their average times between the fastest and slowest parachutes. Overall the parachutes were fairly close in there speeds and times, but not as close as I thought they would be. Differences in shape do matter. 
________________________________________
2013 - MA043 
TRINOMIAL POWERS AND THE NUMBER OF THEIR TERMS
Jose Miguel Rodriguez-Lozano
Centro Recidencial de Oportunidades Educativas de Mayaguez, Mayaguez, PUERTO RICO

This research suggests a permanent arrangement for the expanded trinomials elevated to the n power. Trinomial powers’ terms were organized in a logical order based on the binomial theorem. A method to determine its coefficient pattern (as Pascal’s triangle) and organize the terms is treating the trinomial (a+b+c)^n, as a binomial (a+(b+c))^n. <br><br>A three step method is presented to find the number of terms of an expanded trinomial. This is an easier way to find the desired term without multiplying term by term of many trinomials. The coefficients and variables will be known in any selected term. After knowing the number of terms that (a+b+c)^n will have, the desired term will be defined as “t”.<br><br> By using this method, another process can be applied to find which one is the expanded term. The number of the expanded term will be known according to the correct order of the trinomial expansion. The main contribution of the research is the order of the expanded trinomial and the method to find the desired term inside it. Further investigations will be to work with quadrinomials. 
________________________________________
2012 - MA043 
A SYMBOLIC CALCULATOR IN THE FINNISH MATRICULATION EXAMINATION IN MATHEMATICS
Meri Vainio
Paivolan Kansanopisto/Valkeakosken Tietotien lukio, Tarttilla, FINLAND

Year 2012 symbolic calculators were allowed in the Finnish matriculation examinations, which is the final exam in the Finnish high school. The test is organised in all Finnish schools simultaneously and test situation is supposed to be the same for all. The national authority will give ratings to each student. <br><br>The symbolic calculators on the market this year have different features. I used three different calculators.<br><br>To find what is the benefit of the symbolic calculators, I solved both of year 2011 matriculation examinations of advanced level mathematics with 'old style' graphic calculator ant with the symbolic calculator. <br><br>The solutions of the questions were cut in to the steps. For each step I evaluated wether or not the step was done by the CAS. Every part between equal signs was detected as a step. This kind of clear calculation problems were in spring 2011 five test questions and in fall seven. In other solutions one step can include some subsequent part of solution.<br><br>A symbolic calculator can solve almost all steps, which were needed to create a solution to the first five questions. All tested symbolic calculators could solve derivatives, integrals and equalitions.<br><br>There were minor differences between the CAS systems. When 10 most suitable qoestions were selected, symbolic calculator solved most of the steps, in spring 2011 93,2% in autumn 77,8%. As a result, symbolic calculator seem to give clear benefit to those students, who can use the CAS system. 
________________________________________
2013 - MA044 
RELATIONSHIP BETWEEN HAMILTONIAN CYCLES AND DETERMINANTS OF TESSELLATIONS AND PLATONIC SOLIDS' GRAPHS
Edsel Gabriel Torres Morales
Brigida Alvarez Rodriguez, Vega Baja, PUERTO RICO

Demonstrate if there is a relationship between Hamiltonian cycles and/or paths on graphs of both tessellations and the Platonic solids, with determinants of the adjacency and incidence matrices was the main idea for this research. Other idea was to find any difference between relations in the tessellating graphs compared with that of Platonic solids. The goal of this research was to prove that there is a specific relationship between Hamiltonian cycles and/or paths and their determinants in the tessellations’ and platonic solids’ graphs.<br><br> <br><br>For every tessellation, the researcher verifies if there is a Hamiltonian path. Then he proceeds to find the adjacency and incidence matrices’ determinants of these graphs of tessellation. Then, these steps were repeated with graphs of the Platonic solids and evaluated the relation of the Hamiltonian cycles with determinants. Finally, the researcher evaluated both relationships to find their differences.<br><br>The results showed that, if the determinant of the adjacency and incidence matrix is zero, a Hamiltonian cycle and/or path are present on the tessellation. In addition, all of the Platonic solids have a Hamiltonian cycle but there is no relationship between the determinant of the matrix and the Hamiltonian cycle since all determinants were numbers at random. For the future the researcher will focus on creation of a formula in which, with data supplied from graphs (number of vertex and edges), a person can determine whether the graph have a Hamiltonian cycle and/or path. 
________________________________________
2011 - MA044 
EXCURSIONS WITH EXTENSION FIELDS
Katherine Leigh Cordwell
Manzano High School, Albuquerque, NM

Field theory is the study of fields, which are basic mathematical structures that possess certain properties. Using the ideas of extension fields and automorphisms, we explore three different applications of field theory. <br><br>One of these applications is rationalizing complicated denominators, such as 1/[2^(1/2) + 5^(1/3) + 7^(1/5)] or 1/(2z^2 + z^5 + 7), where z is a root to an equation such as x^9 + 5x^8 + 21x^5 + 49x + 368. A straightforward way uses the Extended Euclidean Algorithm for polynomials. We discuss a significantly simpler method that works in many cases. This method uses concepts of consecutive extension fields.<br><br>Factoring integers is a second application. This is of great importance to public key cryptography. Integers factor differently in extension fields than they do in Z, and we can utilize the extension field factors to find integer divisors.<br><br>Another application is that of constructing regular polygons. We consider n-gons, where n is a prime of the form 2^(2^k) + 1. Using automorphisms of extension fields, we explicitly find formulas for cos(2*pi/5), cos(2*pi/17), and cos(2*pi/257), which account for three out of the five known polygons of this form. 
________________________________________
2013 - MA045 
DOTS AND LINES: A COMBINATORIAL INTERPRETATION OF THE HOMOTOPY GROUPS OF FINITE TOPOLOGIES
Colin Campbell Aitken
Leland High School, San Jose, CA

The homotopy groups of a topological space are commonly studied as a topological invariant that gives information about the space's holes, homotopy type, and connectedness. In the case of finite topologies, finding applications is difficult because there is no published method of interpreting homotopy theory in terms of the combinatorial characteristics for which these topologies are usually studied. An attempt to construct an analogue of homotopy groups for graphs was examined by Atkin and Smith, but the groups they associate to a graph G do not in general correspond to the homotopy groups of a finite topology whose associated graph is G when such a topology exists. <br><br><br>My project presents a new definition of the homotopy groups of a graph G, in the spirit of Atkin by using only combinatorial methods, and shows that under this new definition the homotopy groups of G are indeed the same as any topology whose associated graph is G. In doing so, this demonstrates that a 1966 construction of Stong is sufficient to construct all paths in a finite topology, which was previously unproven. In addition, this allows for the new construction of a graph G^k (for any positive integer k), such that the nth homotopy group of G^k is the (n+k)th homotopy group of G. This allows tools developed to study the first homotopy group to be used to study higher homotopy groups, and has no known analogue without the use of graphs, in the literature or by inspection.<br><br> <br><br>This new definition allows for the construction of a (possibly infinitely-generated) recursive presentation for any homotopy group of any finite topology, graph, and/or simplicial complex. In addition, it provides a test for graph isomorphism which in most cases can tell when two graphs are not isomorphic in O(v^3) time. 

Awards won at the 2013 ISEF
First Award of $1,000 - American Mathematical Society
________________________________________
2011 - MA045 
EFFECTS OF CELL COMPRESSIBILITY, MOTILITY, AND CONTACT INHIBITION ON THE GROWTH OF TUMOR CELL CLUSTERS
Jonathan F Li
St. Margaret's Episcopal School, San Juan Capistrano, CA

I analyze the effects of cell migration, compression, and contact inhibition on tumor growth using the Cellular Potts Model (CPM). Cell proliferation, motility, cell-to-cell adhesion, contact inhibition, and cell compressibility are incorporated in the model. I find that increased motility has a direct effect on the growth rate of a tumor. Cell lines with greater motility overcome the attractive forces of cell-to-cell adhesion and have more space to proliferate. I analyze the interplay between cell motility and compressibility within the CPM, and find that more motile cells are generally smaller than their more sedentary counterparts. I obtain an explicit inverse-relationship between the cell compressibility and motility parameters. In addition, I find contact inhibition in the CPM penalizes clumped cells by halting their growth, giving motile cells a greater advantage. The model also shows that cells that are less responsive to contact inhibition are more invasive. Together, these results raise questions on the effectiveness of some chemotherapy treatments, which, by targeting cells that undergo mitosis, may actually select for these equally or more invasive motile cells. I have begun testing my model with in vitro data obtained from a lab source and my model is reflective of the data. 

Awards won at the 2011 ISEF
Third Award of $250 - American Mathematical Society
Second Award of $500 U.S. savings bond - AVASC-Ashtavadhani Vidwan Ambati Subbaraya Chetty Foundation
Award of $3,000 - China Association for Science and Technology (CAST)
________________________________________
2013 - MA045 
DOTS AND LINES: A COMBINATORIAL INTERPRETATION OF THE HOMOTOPY GROUPS OF FINITE TOPOLOGIES
Colin Campbell Aitken
Leland High School, San Jose, CA

The homotopy groups of a topological space are commonly studied as a topological invariant that gives information about the space's holes, homotopy type, and connectedness. In the case of finite topologies, finding applications is difficult because there is no published method of interpreting homotopy theory in terms of the combinatorial characteristics for which these topologies are usually studied. An attempt to construct an analogue of homotopy groups for graphs was examined by Atkin and Smith, but the groups they associate to a graph G do not in general correspond to the homotopy groups of a finite topology whose associated graph is G when such a topology exists. <br><br><br>My project presents a new definition of the homotopy groups of a graph G, in the spirit of Atkin by using only combinatorial methods, and shows that under this new definition the homotopy groups of G are indeed the same as any topology whose associated graph is G. In doing so, this demonstrates that a 1966 construction of Stong is sufficient to construct all paths in a finite topology, which was previously unproven. In addition, this allows for the new construction of a graph G^k (for any positive integer k), such that the nth homotopy group of G^k is the (n+k)th homotopy group of G. This allows tools developed to study the first homotopy group to be used to study higher homotopy groups, and has no known analogue without the use of graphs, in the literature or by inspection.<br><br> <br><br>This new definition allows for the construction of a (possibly infinitely-generated) recursive presentation for any homotopy group of any finite topology, graph, and/or simplicial complex. In addition, it provides a test for graph isomorphism which in most cases can tell when two graphs are not isomorphic in O(v^3) time. 

Awards won at the 2013 ISEF
All expense paid trip to tour CERN - European Organization for Nuclear Research-CERN
Fourth Award of $500 - Mathematical Sciences
All expense paid four week trip and scholarship to the Bessie Lawrence International Summer Science Institute. - American Committee for the Weizmann Institute of Science
________________________________________
2013 - MA046 
MYSTERY PATTERNS BEHIND THE EUCLIDEAN ALGORITHM
David Chang Luo
Baton Rouge Magnet High School, Baton Rouge, LA

In this science fair project, I will be answering the question, what are some patterns regarding the number of steps it takes to find the gcd(a,b) using the Euclidean Algorithm? Are there any patterns in the number of steps it takes to find the greatest common divisor of two numbers using the Euclidean Algorithm? I will use the Euclidean Algorithm in finding the gcd of two numbers (a,b). The algorithm is written in the form, a=bq+r, where q stands for the quotient and r stands for the remainder, and (a,b) are positive integers. For my project, I will be finding the patterns in the steps, you do the Euclidean Algorithm and see how many steps it takes for the remainder r, to be equal to zero. I will design and make the Euclidean Algorithm step chart by using Microsoft Publisher on the computer, then I will begin computing the gcd(a,b) of numbers that line up and complete the chart, I will begin to look for some patterns in the steps. I will then record my data and begin proving my theories and expanding the chart to see if I can find something new. One goal of this scientific project is to find a pattern in the steps it takes to find the gcd(a,b) using the Euclidean Algorithm so that in the future, we may be able to compute the gcd(a,b) quicker and more efficient than it already is, I want to introduce a new theory and pattern to the world of mathematics. 
________________________________________
2011 - MA046 
THE VARIETIES GENERATED BY A LEFT ZERO SEMIGROUP OF DEGREE K AND THEIR SUBVARIETY LATTICES
Ilias Valer'evich Iliasov
Lyceum #572, Center of Mathematical Education, Saint-Petersburg, Saint-Petersburg, RUSSIA

In this paper we investigate some properties of the subvariety lattices for varieties generated by left zero semigroups of some positive degree. It is well known that such semigroups generate a small permutational variety which can be embedded in a natural way to the universal small permutational combinatorial semigroup variety. Our results give a way to investigate small varieties in general. Also, we obtained a complete description of the subvariety interval between the varieties generated by the left zero semigroup of degree k and k+1 respectively. One of modern mathematics proved that there exist a limit combinatorial finitely generated variety. According to this result our description has an application in theory of infinitely based varieties. 
________________________________________
2011 - MA047 
DENNIS-VASERSTEIN TYPE DECOMPOSITIONS FOR SOME FAMILIES OF CHEVALLEY GROUPS
Constantine Borisovich Tsvetkov
Lyceum #572 Center of Mathematical Education, Saint-Peterburg, Saint-Peterburg, RUSSIA

Let R be a commutative ring with unit, and let F be a reduced irreducible root system, let P be a subset of simple roots of F. Let a and b be a pair of simple roots. One says that the elementary subgroup E(F, R) of the Chevalley group G(F,R) admits the parabolic (or Dennis-Vaserstein type) factorization for the roots a and b if any element from the group E(F, R) can be written as a product puq, where the elements p and q lie in the corresponding parabolic subgroups EP(a) and EP(b), and the element u lies in the intersection of the opposite unipotent radicals. For classical root systems Fn and a, b being the opposite endnodes of the Dynkin diagram of Fn the existence of the factorization has been found in the late 1970’s under some assumptions on the stable rank of R. This particular case of factorization implies injective stability for the sequences of K1 functors modeled on classical groups.<br><br>It seems natural to ask what the condition one needs to require from R in the general situation. It turns out that the case of a classical root system can be handled by similar methods and the result can be formulated in terms of the stable rank of R and the distance between roots on the Dynkin diagram. The purpose of my project is to investigate the case of an exceptional group for some specific choices of the pair a, b. I find sufficient conditions on the ring R which ensure parabolic factorizations for these cases. 
________________________________________
2011 - MA048 
INTEGER PARTITIONS AND SEQUENCES
Manosij G Dastidar
South Point High School, Kolkata, INDIA

This project presents an extension of Stanley's theorem related to partitions of positive integers. Stanley’s theorem states that the total number of distinct members in the partitions of a positive integer n is equal to the total number of 1's that occur in the partitions of n. <br><br>My generalization states a similar relation between the total number of distinct members in the partitions of n and the total number of 2's or 3's or any general integer k that occur in the partitions of n and the subsequent integers. More precisely, the total number of distinct members in the partitions of a positive integer n is equal to the total number of k's that occur in the partitions of n, n+1, . . . , n + k -1. <br><br>To the best of my knowledge, this result is a new extension of Stanley's theorem. This is applied to obtain an array of interesting corollaries, including alternate proofs and analogues of some well-known results in the theory of partitions. I extend Ramanujan's results on congruence behavior of the `number of partition' function p(n) to get analogous results for the `number of occurrences of an element k in partitions of n'. Further, I provide an alternate proof of Ramanujan's congruence identities related to integer partitions. <br><br>Finally, I have proposed a general structure of adding points to existing partitions in the Ferrer's diagram and have formulated and proved a combinatorial result for adding k points to partitions of n and count the number of resultant partitions. 

Awards won at the 2011 ISEF
First Award of $1,000 - American Mathematical Society
________________________________________
2013 - MA048 
IDENTIFICATION OF BIOMARKERS FOR THE EARLY DETECTION OF LUNG CANCER USING A NOVEL STATISTICAL TEST
Shreya Mathur
Oxford High School, Oxford, MS

Lung cancer is the most common cause of cancer-related death, and the low 5-year survival rate, ranging from 10 to 14%, reflects the problem of late diagnosis of the disease. Biomarkers can easily be used in clinical testing to detect cancer in its earlier stages and increase chances of survival. Though global proteome profiling holds promise for finding biomarkers, proteomic analysis of plasma has been hampered by methodological issues and so far has failed to discover reliable and clinically useful markers for the early detection of lung cancer. For one, multiple variables affect a disease condition. As a result, univariate statistical tests are not generally suitable for finding biomarkers for a disease condition. Moreover, distribution assumption justifying parametric testing is often untenable in proteomic studies. In response to this daunting problem, we propose a novel nonparametric bivariate statistical test, coined the S-test, which does not require stringent assumptions and outperforms previous methods in both simulation and clinical studies. The proposed test has a high power and a low type-I error rate, and is able to accurately detect biomarkers in a given data set. Asymptotic distribution is found for the S-test statistic. Moreover, as this test finds biomarker combinations, its results are highly applicable in a clinical setting, allowing the early detection and diagnosis of many life-threatening diseases. The proposed test will also allow researches to create more accurate cancer drugs to inhibit the specific proteins that cause the disease condition. Therefore, the S-test has the potential to advance both cancer detection and treatment. 

Awards won at the 2013 ISEF
Second Award of $500 - American Statistical Association
________________________________________
2011 - MA048 
INTEGER PARTITIONS AND SEQUENCES
Manosij G Dastidar
South Point High School, Kolkata, INDIA

This project presents an extension of Stanley's theorem related to partitions of positive integers. Stanley’s theorem states that the total number of distinct members in the partitions of a positive integer n is equal to the total number of 1's that occur in the partitions of n. <br><br>My generalization states a similar relation between the total number of distinct members in the partitions of n and the total number of 2's or 3's or any general integer k that occur in the partitions of n and the subsequent integers. More precisely, the total number of distinct members in the partitions of a positive integer n is equal to the total number of k's that occur in the partitions of n, n+1, . . . , n + k -1. <br><br>To the best of my knowledge, this result is a new extension of Stanley's theorem. This is applied to obtain an array of interesting corollaries, including alternate proofs and analogues of some well-known results in the theory of partitions. I extend Ramanujan's results on congruence behavior of the `number of partition' function p(n) to get analogous results for the `number of occurrences of an element k in partitions of n'. Further, I provide an alternate proof of Ramanujan's congruence identities related to integer partitions. <br><br>Finally, I have proposed a general structure of adding points to existing partitions in the Ferrer's diagram and have formulated and proved a combinatorial result for adding k points to partitions of n and count the number of resultant partitions. 

Awards won at the 2011 ISEF
Second Award of $1,500 - Mathematical Sciences - Presented by Intel
________________________________________
2011 - MA049 
THE FRACTAL GEOMETRY OF THE MANDELBROT AND JULIA SETS: A SIMPLE RELATIONSHIP IN THE COMPLEX PLANE
Aishwarya Saluja
Hedgesville High School, Hedgesville, WV

Purpose: The Mandelbrot set is considered the most fascinating mathematical image. It is a map of the complex plane that shows information about the Julia sets for each complex number “c”. The mathematical basis for the Mandelbrot set is f (z) = z^2 +c. The project explores various Julia sets generated by altering the value of “z” with c held constant.<br><br>Methods: The mathematical basis of the Mandelbrot was first studied. Approximately 200 variables were altered in its unique quadratic equation and the results were computed both manually and utilizing computer software. The different variables were manipulated using generic fractal generator software from the NLVM website. The generated fractal images represent various Julia sets.<br><br>Analysis: A mathematical basis of the Mandelbrot set is described. For each c of f(z) = z^2 + c there exists a particular Julia set Jc. A total of approximately 200 Julia set images were created using the fractal generator, some of which are displayed. Since the mathematical basis of each image was also manually generated, the results are tabulated illustrating the number of iterations and the coordinates in the complex plane.<br><br>Conclusions: The Julia sets are represented by Fc(z) = z^2 + c where c is constant and z is variable. The Julia sets can be classified into two distinct types- connected and totally disconnected (Cantor dust). The fusion of all possible connected Julia sets will form the composite image of the Mandelbrot set. 
________________________________________
2013 - MA049 
THE EXISTENCE OF COMPUTABLE SEQUENCE THAT CANNOT BE DESCRIBED BY FINITE AUTOMATA
Raushan Serikzhan Serikzhanovna
Republican Specialized Physics and Mathematics School after Zhautykov, Almaty, KAZAKHSTAN

The goal of the project is to construct an infinite sequence that cannot be generated by any simple automatic device, and to estimate its complexity. The conjecture on the existence of such a sequence is based on the idea of superiority of Turing machines over finite automata.<br><br>In order to reach the goal of the project one had to study Turing machines, finite automata, computable martingales, and the diagonalization method.<br><br>In the project, a new notion of automaton martingale is introduced, and the existence of an infinite binary random sequence that cannot be generated by a finite automaton is proved. 
________________________________________
2013 - MA050 
SPIDRONS
Yun Ha Seo
Tinian High School, Tinian, NORTHERN MARIANA ISLANDS

Geometry has been close partners with art, architecture, and engineering when it comes to patterns, tiling, to even simple shapes themselves. Spidrons, a tiled plane that uses a series of two types of alternation triangles, is one of these special geometric patterns. The purpose of this project was to explore the world of spidrons, create several examples, and show their art value, practicality, and how they are used in everyday life. The math, art, and engineering connection were centered throughout the project. 
________________________________________
2011 - MA050 
LOWER BOUNDS FOR ODD PERFECT NUMBERS
Anirudh Prabhu
West Lafayette Junior-Senior High School, West Lafayette, IN

A number that is the sum of its proper divisors is called a perfect number. The study of perfect numbers began more than 2300 years ago with Euclid’s work (circa 300 B.C.). To date, 47 perfect numbers have been discovered. All the known perfect numbers are even. It is not known if any odd perfect numbers exist. <br><br><br> Many analytic upper bounds for odd perfect numbers have been published in the literature. However, no nontrivial analytic lower bound has been reported to date. The hypothesis of this project was that there exists a nontrivial analytic lower bound for odd perfect numbers. The project sought to prove the hypothesis by deriving such a bound.<br><br><br> First, a lower bound for the product of the distinct prime factors of an odd perfect number was derived using Euler's Theorem, certain previous results of Suryanarayana and Hagis, the binomial theorem and the QM-AM-GM-HM inequalities. Using the above bound, Euler's Theorem, Rosser's Theorem, and Dusart's Theorem the first nontrivial analytic lower bound for odd perfect numbers was established. Some improvements of the new lower bound are also discussed. <br><br><br> The analytic lower bound established in this project complements known analytic upper bounds for odd perfect numbers and suggests a promising approach to this old problem. If an odd perfect number does not exist, then one way to prove its nonexistence is to derive a lower bound that exceeds a known upper bound. The work reported in this project represents a first step in that direction. 

Awards won at the 2011 ISEF
Third Award of $250 - American Mathematical Society
Third Award of $1,000 - Mathematical Sciences - Presented by Intel
________________________________________
2013 - MA051 
A COUNTEREXAMPLE TO LANDO'S CONJECTURE: ON INTERSECTION OF SPHERES IN 3-SPACE
Vladislav Alexandrovich Belousov
Advanced Educational Scientific Center of Lomonosov Moscow State University A.N.Kolmogorov School, Moscow, RUSSIA

If we look at the intersection of 2 polyhedrons homeomorphic spheres in 3-space, we can notice that their intersection is a union of curves. Now lets look how the curves are situated on the first sphere and on the second. Lets name this situating of curves on spheres possible. What kind can they be?<br><br>In this text a circle is a closed broken line without self-intersections in 3-space. Let A and B be a union of disjoint circles on sphere.<br><br>Lando's Conjecture. Pair A and B is possible if and only if the number of circles in them is the same. <br><br>Suppose that M is a union of disjoint circles in sphere S2. Define (‘dual to M’) graph G = G(S2,M) as follows. The vertices are the connected components of S2 −M. Two vertices are connected by an edge if the corresponding connected components are neighbors. <br><br>Theorem.Let M1 , . . . , Mm be unions of disjoint circles in sphere S2 ⊂ R3 . Then there exist disjoint spheres with holes P1,...,Pm whose interiors are inside S2 and such that ∂Pi = Mi for each i = 1,...,m if and only if M1,...,Mm are pairwise unlinked. <br><br>Avvakumov theorem.Let A and B be a union of disjoint circles on sphere.Using the Embedding Extension Theorem we can formulate the necessary and sufficient property of the pair of graphs corresponding to A and B such that A and B are possible.<br><br>Graph G has vertices A,C3,C1,C2,A',C3',C1',C2'and edges BB', ACi, A'Ci'. Graph H has vertices B,D,P1,P2,P3,Q1,Q2,Q3 and edges BD,BPi,PiQi. The pair of sets of disjoint spheres corresponding to graphs G and H is the minimal counterexample to Lando's conjecture. <br><br>The result of my work is a new combinatoric prove of the above proposition. 
________________________________________
2011 - MA051 
ON THE PATTERNS EXISTING AMONG CAROUSEL PRIMES IN BASE N
Simanta Gautam
Albemarle High School, Charlottesville, VA

The initial purpose of this project was to understand the structure of Carousel Primes in various bases. As I found new conjectures and proofs, I tried to tackle an unsolved problem in this topic: how many Carousel Primes exist?<br><br><br>To create data, I generated a couple of these primes by hand but realized that it would be too time consuming. Therefore, I wrote two computer programs in java providing me with more than enough data. The first program finds the first 50 Carousel Primes in bases 2 to 1000. The <br><br>second program finds the first 200 Carousel Primes in a specific base.<br><br><br>Through all the data generated by these programs, I was able to search for patterns and understand the structure of Carousel Primes not just in base 10, but in hundreds of bases. Some of the conjectures I created were very surprising and I went on to prove/disprove those using my background in number theory. One of the proofs I wrote solved a small case of the unsolved problem mentioned earlier. I was able to prove (using Fermat's Little Theorem) that any base that’s a square number does not have any nontrivial Carousel Primes.<br><br><br>This research is new because I didn't limit myself to base 10 when searching for Carousel Primes. A great application for Carousel Primes could potentially be in cryptography. Through understanding the patterns of these primes in various bases, it is possible to create a new cipher which could improve modern cryptography. 

Awards won at the 2011 ISEF
Second Award of $1,500 - Mathematical Sciences - Presented by Intel
________________________________________
2013 - MA052 
CONFIGURATION SPACES OF 4-BAR LINKAGES
Ilya Viktorovich Kirillov
Lyceum 1303, Moscow, RUSSIA

Topology of configuration spaces of 4-bar linkages is considered. A<br><br>4-bar linkage is a planar mechanism with four rigid bars consecutively<br><br>connected to form a polygonal line. The endpoints of the polygonal<br><br>line are presumed to be fixed on the plane. All vertices of the<br><br>polygonal line are hinges, and the bars are allowed to rotate freely<br><br>around the hinges.<br><br>All possible 4-bar linkages are described by the lengths of the bars<br><br>and the distance between the fixed hinges. So, a 4-bar linkage is<br><br>given by five parameters. <br><br>The configuration space of a 4-bar linkage is the space of all its possible positions. This space can be naturally identified with a two-dimensional surface with possible singularities. All possible configuration spaces of 4-bar linkages are classified depending on the parameters. 

Awards won at the 2013 ISEF
Third Award of $250 - American Mathematical Society
________________________________________
2011 - MA052 
BRAID GROUP REPRESENTATIONS AND BRAIDING QUANTUM GATES
Rebecca Chen
Park Tudor School, Indianapolis, IN

Unitary representations of the braid group are sought for their potential usefulness in topological quantum computation, in which two-dimensional quasi-particles called anyons are braided to process quantum information. Useful braid group representations are difficult to find because current methods yield representations that either are not unitary or have the wrong dimensions. My project explores a new method for finding braid group representations: solving a generalized version of the famous Yang-Baxter equation.<br><br>The equation I worked with is the (2, 3, 1)-generalized Yang-Baxter equation (gYBE) for an 8x8 matrix R. After making several assumptions about the form of R to ensure unitary solutions, I systematically solved the (2, 3, 1)-gYBE and classified all solutions of the form R = X direct sum Y, where the 4x4 matrices X, Y, are (2x2)-diagonally unitary, into three families, each described by a single parameter.<br><br>My project goes on to show that my solutions to the gYBE lead to unitary braid group representations, thus generating braiding quantum gates. The realization of such braiding quantum gates in physical systems would lead to useful topological quantum computers and thereby bring benefits to our society. Another significance of my work lies in my promising new method, which may be repeated by other researchers to find more braid group representations that generate braiding quantum gates. 

Awards won at the 2011 ISEF
Certificate of Honorable Mention - American Mathematical Society
Fourth Award of $500 - Mathematical Sciences - Presented by Intel
________________________________________
2013 - MA053 
ANALYSIS OF NOVEL CLUSTERING ALGORITHMS FOR GENE EXPRESSION PATTERNS
Shashwat Kishore
Unionville High School, Kennett Square, PA

One of the central problems in modern computational biology is the identification and classification of groups of genes with similar expression profiles. This problem holds the key to future advances in disease treatment and prevention. Clustering is the process by which these groups are identified. The first step in clustering is the compilation of a similarity matrix. A similarity matrix on n genes is an n by n matrix that has a 1 in the entry at the intersection of the i-th row and j-th column if gene i and gene j are similar and a 0 if they are not similar. Clustering algorithms are then applied to the similarity matrix to identify clusters of genes with similar expression profiles. Typically, experimentally determined similarity matrices have large amounts of error. In order to be practically useful, clustering algorithms need to produce accurate outputs in fast runtime.<br><br> <br><br>In this project, a novel graph theory algorithm for clustering is presented. The algorithm is shown to have runtime complexity O(n) on an n-gene data set, making it among the fastest algorithms used today. The algorithm is proven to produce an output that can be made as accurate as necessary. Finally, it is run on large test data sets with promising results. Even more promising is the new technique that is the basis of the algorithm. This technique quickly and accurately tests whether two genes are similar, and has a wide array of possible applications. 

Awards won at the 2013 ISEF
Certificate of Honorable Mention - American Statistical Association
Fourth Award of $500 - Mathematical Sciences
________________________________________
2013 - MA054 
A MAXIMUM PRINCIPLE AND ITS APPLICATIONS
David L Pan
Canterbury School, Fort Wayne, IN

A function is said to satisfy the maximum principle on an interval if its maximum occurs at one of the endpoints of the interval. It is well known that a nonnegative first or second derivative of a function implies the maximum principle. It is also well known that a nonnegative third or higher order derivative of a function fails to imply the maximum principle.<br><br> The purpose of this project is to investigate under what conditions the maximum principle remains true for nonnegative high order derivatives. The main result is a theorem that states a nonnegative high order derivative can still imply the maximum principle if certain geometric endpoint conditions are satisfied. Applying this theorem to polynomials results in a maximum principle for polynomials of any degree under linear coefficient conditions. Also, computer programs are created to compute the probability of maximum principle validity for arbitrary coefficients.<br><br> More important, this project leads to a question of finding sharp maximum principle conditions for high degree polynomials. The sharp conditions for cubic polynomials are found. However, theoretical difficulties are present for higher degree ones due to Galois theory. 

Awards won at the 2013 ISEF
Certificate of Honorable Mention - American Mathematical Society
________________________________________
2013 - MA055 
Q-DISTRIBUTION NORMALITY ON BOUNDED BASES
Dylan Richard Airey
Texas Academy of Mathematics and Science, Denton, TX

A number x is normal in base b if each block of k digits occurs with frequency b^-k in the b-ary expansion of x. B-ary expansions can be generalized. Given a sequence Q = (q_n) of integers greater than 1, any number x can be represented by a Q-Cantor series expansion. Normality for Cantor-series expansions has many pathological properties and many of the theorems for normality in b-ary expansions no longer hold. This leads to two different types of normality, Q-normality and Q-distribution normality. The case where Q is infinite in limit has been well studied, and these two notions of normality are not equivalent. Normality for bounded basic sequences has not been researched as extensively, and there are many open problems. The purpose of this study was to show the equivalence of Q-normality and Q-distribution normality for classes of bounded bases. Results show that Q-normality in periodic basic sequences is equivalent to normality in b-ary expansions, and for any sequence (a_n), there is a bounded basic sequence so that the number formed by concatenating the digits of a_n is not Q-normal. In addition, a construction was given to produce a number that is Q-distribution normal when sampled along certain polynomials. It was also shown that Q-normality and Q-distribution normality do not imply nx is Q-normal for all natural numbers n. In conclusion, these results lead to a better classification of normality for bounded basic sequences. This research could help link Cantor-series expansions to ergodic theory and dynamical systems. 
________________________________________
2013 - MA056 
NUMERICAL STUDIES OF EXISTENCE FOR A BOUNDARY VALUE PROBLEM OF ORDINARY DIFFERENTIAL EQUATIONS
Jefferson Zhiping Zou
Alabama School of Fine Arts, Birmingham, AL

Numerical computing, or numerical analysis, is a process of approximating or computing numerical values and has been extensively used to solve mathematical problems rather than using general symbolic manipulation. Thanks to more and more powerful and accessible computers, this technique is seeing increased usage. Nowadays numerical computations are powerful and necessary for computing and/or approximating actual values; moreover, they provide a useful tool to aid classical mathematical research. Numerical computing is incredibly powerful for studying problems where symbolic manipulation is impossible or can only yield partial results, i.e. differential equations, polynomial root finding, linear algebra, statistics, mathematical finance and optimization where enormous data are involved. This research in particular used numerical computations to study the existence and non-existence of positive solutions to a boundary value problem of nonlinear second order ordinary differential equations. This boundary value problem arises from studying various phenomena in biology, physics, and chemistry. In this research, numerical solutions were computed in order to find a set of specific conditions on the parameters n, m, and λ for existence and non-existence of a solution to the boundary value problem; MATLAB was used for all numerical computations. The significance of this research is two-fold: first, the results will complement recent mathematical research on this topic and help to yield a complete answer to this problem and second, the research may provide new proof and support for this relatively new approach – using numerical computing to aid professional mathematical research. 
________________________________________
2013 - MA057 
SUPERADDITIVITY AND SUBADDITIVITY IN FAIR DIVISION
Rishi Suvir Mirchandani
Fox Chapel Area High School, Pittsburgh, PA

I examine the classic problem of fair division of a piecewise homogeneous cake. The “cake” can be representative of any divisible desideratum, leading to such practical applications as the distribution of natural resources, the splitting of inheritances, and the resolution of international disputes.<br><br> The concept of “fairness” may be defined by notions such as “proportionality,” “envy-freeness,” “equitability,” and “Pareto-optimality,” or any combination of the four. Previous works developed division algorithms to yield fair allocations. However, the previous research assumed that all utility functions are “additive,” i.e., that utility varies directly with quantity of allotment. My main contention is that additive functions accurately model utility only in certain situations; for many others, a superadditive or subadditive function may be more appropriate.<br><br> Recognizing the incompatibility of previous algorithms with nonadditive utility functions, I propose and test a new division protocol that utilizes nonlinear programming. In this protocol, fairness notions are formulated into constraints on the feasible region, and the utilities of each player are defined as functions of the decision variables. The optimal solution yielded is an effective allocation of the good in question.<br><br> Finally, I prove theoretical results about the properties of nonadditive utility functions. The most critical of these results addresses the orthogonal issue of the “price of fairness.” The price of fairness quantifies the relative amount of aggregate social utility that is lost in the interest of satisfying a particular fairness notion. I prove that, with nonadditive utility, the prices of proportionality, envy-freeness, and equitability can be unbounded. 

Awards won at the 2013 ISEF
Fourth Award of $500 - Mathematical Sciences
Third Award of $1,000 - Mu Alpha Theta, National High School and Two-Year College Mathematics Honor Society
________________________________________
2013 - MA058 
MIXING VARIABLES IN EXTREMUM PROBLEMS
Muhammed Zahid Ozturk
Private Istanbul Anafen Science High School, Istanbul, TURKEY

Extremum problems are used in many areas such as engineering and mathematics. The main purpose of this project is to suggest elementary ways for solving those problems; to make this way applicable in different areas like biology; and to show that solution to the problem, the extremum value, is obtained in case of equality of the variables, especially on symmetrical expressions.<br><br>In general, we use some inequality theorems or some calculus methods when we need to calculate extremum values of a function. Sometimes it becomes hard to determine which theorem will be used, and gets complicated to find the solution of the extremum problem with many variables. However finding a solution is like a solving a puzzle by using our method, but with a small difference: just put the first piece correctly and then problem is solved with induction.<br><br>The main principle of our method is decreasing the length of variables interval using such a transformation in each step: change two of the variables in an appropriate way and leave the others unchanged. Then it becomes very easy so that computations can be done by four basic operations, instead of using calculus methods of taking derivatives or doing computations for many variables.<br><br>The project suggests solutions for different types of problems; contains proofs of some inequality theorems; and explains how to apply the method in different conditions. Moreover, some new types of problems are generated by using reverse application of the method. 
________________________________________
2013 - MA059 
PROOFS FOR A CLASS OF ORIGINAL GRID-BASED PATTERN REALIZATION PROBLEMS
Alan C. Xu
Ann Arbor Huron High School, Ann Arbor, MI

This project provides proofs and results for a class of original pattern realization (partition) problems in grid-based diagrams that, with modifications and restrictions, are related to Fermion statistics, scheduling, logistics, power grid optimization, etc. These problems revolve around selecting combinations of numbers and adding them together to achieve all sums, “X”, from 1, to the sum of all elements in the grid, “S”. The main restriction is that the number of elements selected in the row with the most elements selected can differ by no more than one from the number of elements selected in the row with the least elements selected. <br><br>The problems selected with solutions obtained are a stepping stone to real-life applications in various fields mentioned above. The problems can also vary from simple vertically- and horizontally-ordered rectangular grids to more generally arranged grids with other variants and restraints such as different element ordering. Plans for future work related to randomly ordered and more generic grid-based diagrams that are more mathematically challenging also with broader potential real-life applications are briefly discussed as well. 
________________________________________
2013 - MA060 
A MATHEMATICAL ANALYSIS OF SET VARIANTS
Evan Zheran Liu
Albuquerque Academy, Albuquerque, NM

This project investigates the size of maximal extensions of Super Set in Set and Projective Set as well as maximal caps in Set and Projective Set as functions of the number of features or dimensionality. An [n, n-r, d]-code exists if and only if there exists an (n, d-1)-set in an n-dimensional vector space formed by a finite field of order q. Since linear dependence of a set of vectors relates to collinearity and is actually equivalent to collinearity over an n-dimensional vector space formed by a finite field of order 2, the lower bound may be strengthened by analyzing [n, n-r, d]-codes, and the maximal cap size in n-featured Projective Set may be exactly determined to be 2^(n-1). Using the size of the maximal caps in Projective Set, we prove that the maximal extension of Super Set in n-featured Projective Set is 2^(n-2). No clear relationship exists between the maximal extension of Super Set in n-featured Set and in (n+1)-featured Set, so we use a Mathematica program to investigate the size of the maximal extension of Super Set in Set numerically. The program confirms known bounds of lower dimensional maximal caps in Set, gives new bounds on higher dimensional maximal caps in Set, and suggests a method to tighten all bounds. Coding Theory further allows tightening of the bounds. 

Awards won at the 2013 ISEF
Fourth Award of $500 - Mathematical Sciences
________________________________________
2013 - MA061 
DETERMINING COMBINATORIAL SUMS USING ROOTS OF UNITY
Liang Zhou
John W. North High School, Riverside, CA

Combinations play an important role in our everyday lives, with applications ranging from map routes to password encryption. Roots of unity, in another realm of mathematics, are complex numbers w such that w^n = 1 for some positive integer n.<br><br> This project seeks to utilize the cyclic nature of roots of unity working in tandem with a clever application of the Binomial Theorem to find and prove an algorithmically efficient closed-form expression for the combinatorial sum nC0 + nCc + nC(2c) + nC(3c) + ... for positive integers n and c such that c < n.<br><br> Subsequently, we will discuss the internal mechanics for our proof and why it works through a graphical representation of the roots of unity. It is shown that the formula (with appropriate substitutions) acts like a "filter" by cancelling out unwanted terms and leaving desired ones.<br><br> We will then generalize our main results to produce a corollary and various other extensions. Finally, through complexity analysis of a Java program, it is proven that the derived closed-form expression is computationally more efficient than direct calculation of the original sum. 

Awards won at the 2013 ISEF
New American University Provost Scholarship - Arizona State University
________________________________________
2013 - MA062 
MATHEMATICAL MODELING OF LANGUAGE'S AFFECT ON POPULATION FLOW AND COMMUNITY STRUCTURE
Gabriel Alexander Ferns
Nicolet High School, Glendale, WI

Many areas of applied mathematics, such as Sociology, Economics, and Epidemiology, are interested in the prediction community structure and intercommunity movements. Knowledge about these factors allow scientists to better predict governmental districting, epidemics, and immigration patterns (Thiemann, 2010). This paper studied how language determines community structure and intercommunity movement. In the first part, I looked at intercommunity movement by adapting the recently proposed Radiation Model (Simini, 2012). The model was altered to incorporate language differences, and this new model was used to predict the movement of people between counties in Texas. The standard Radiation Model performed slightly better than the language based Radiation Model in these tests. In the next part, I predicted community structure using the maximization of the modularity function (Newman, 2006) with the language based Radiation Model as the null model. The same calculations were run with the standard Radiation Model as the null model. Differences between the results of these two calculations were shown to be almost nonexistent. 
________________________________________
2013 - MA063 
EARLY DETECTION OF DISEASE THROUGH FRACTAL DIMENSION ANALYSIS
Madhurima Das
Plymouth High School, Canton, MI

In this project, a mathematical method is being explored as an innovative tool for early detection of diseases. Researchers have noted that shapes of healthy human organs such as the lungs and the brain are irregular; time dependent responses of many healthy organs such as the human heart are irregular as well. The degree of these irregularities can be quantified through fractal dimension analysis and the fractal dimension is affected by disease. A technique to determine fractal dimension for image data is the box-count method, and a technique to determine fractal dimension for time-series data is through use of the Hurst exponent. <br><br> The fractal dimension approach is used here to differentiate between healthy and diseased states for a few different types of diseases such as eye diseases (glaucoma and diabetic retinopathy), ovarian cancer, and heart disease (arrhythmia). The box-count method is used on images of blood vessels from the retina of human eyes and images of ovarian cancer cells; the Hurst exponent is used on heartbeat rate time-series data. The goal is to demonstrate that there are quantifiable differences in the fractal dimensions of healthy and diseased organs and between healthy and diseased heart rate data. These differences are verified through statistical analyses. These techniques, if implemented, could assist in earlier detection of different diseases and allow patients to get the right level of attention leading to faster treatment. All data used in this project were from online databases managed by research institutions made available to the public for research. 
________________________________________
2011 - MA301 
THE SCIENCE OF A MATH GAME
Vivian H. Nguyen, Mynah Holloway, 
Starr's Mill High School, Fayetteville, GA

The human mind constantly pursues enigmas to solve, yet the most challenging ones become stale over time because of the persistent development of the human race’s intellectual ability. As a result, Sudoku and crossword puzzles became too effortless to solve, so our hypothesis is that if we, the experimenters, observed different magic squares and theories, then we would be able to develop basic algorithms that can facilitate the creation of magic squares of all sizes. This project explored the fundamentals of a magic square and invented the basic rules for the production of magic squares. First, we obtained the method for creating a simple four-by-four square and generally accepted theories for the construction of magic squares with odd dimensions and squares with dimensions that are multiples of four. Next, based on observations from these steps, we pondered upon the creation of a previously undeveloped algorithm for magic squares with even dimensions that were not multiples of four. From the final three compiled algorithms, we composed magic squares consecutively to a square with fifty-three numbers in each row and column, and each completed magic square went into one of the three groups. Our results yielded squares that are all magic squares because each is an arrangement of numbers from one to n^2 when n is the number of elements in each row, column, or main diagonal that each add to n(n^2+1)/2. In conclusion, our experiment resulted in the creation of algorithms for all magic squares, and now, dexterous people will be able to observe a new, challenging puzzle. 
________________________________________
2012 - MA301 
BLOODSPATTER... JUST A STAIN, OR CRIME SOLVING EVIDENCE?
Jonathan Lee Ciecka, Benjamin Edward Naber, 
McIntosh High School, Peachtree City, GA

Bloodspatter analysis is one of the newest forms of forensic science. Our experiment tests the validity of its results, by calculating the point of origin using two mathematical equations. For consistancy of the fake blood, 15ml of red paint and 10ml of water were used in each trial. To create a medium velocity spatter pattern we modified a rat trap and placed a sponge (saturated with fake blood) on the base of the trap before arming and releasing the spring. Once the spatter was created for each test, five dried drops were selected at random from each board. Each drop was then measured by width and elongation, its directionality was strung, and the distance to the area of convergence was measured. The measurements achieved were plugged into two trigonomic equations, and then standard deviation was calculated in order to see how far off our results were from where the actual point of origin was. After analyzing our data, we found that the calculated point of origin was on average less than two inches away from the actual point of origin. Before our experiment began, we determined that if the average calculated point of origin was less than four inches away from the actual point of origin, bloodspatter could be considered a valid science. Our results were much closer than we had expected, and from the data collected we have concluded that bloodspatter analysis is a valid forensic science. 
________________________________________
2011 - MA301 
THE SCIENCE OF A MATH GAME
Vivian H. Nguyen, Mynah Holloway, 
Starr's Mill High School, Fayetteville, GA

The human mind constantly pursues enigmas to solve, yet the most challenging ones become stale over time because of the persistent development of the human race’s intellectual ability. As a result, Sudoku and crossword puzzles became too effortless to solve, so our hypothesis is that if we, the experimenters, observed different magic squares and theories, then we would be able to develop basic algorithms that can facilitate the creation of magic squares of all sizes. This project explored the fundamentals of a magic square and invented the basic rules for the production of magic squares. First, we obtained the method for creating a simple four-by-four square and generally accepted theories for the construction of magic squares with odd dimensions and squares with dimensions that are multiples of four. Next, based on observations from these steps, we pondered upon the creation of a previously undeveloped algorithm for magic squares with even dimensions that were not multiples of four. From the final three compiled algorithms, we composed magic squares consecutively to a square with fifty-three numbers in each row and column, and each completed magic square went into one of the three groups. Our results yielded squares that are all magic squares because each is an arrangement of numbers from one to n^2 when n is the number of elements in each row, column, or main diagonal that each add to n(n^2+1)/2. In conclusion, our experiment resulted in the creation of algorithms for all magic squares, and now, dexterous people will be able to observe a new, challenging puzzle. 
________________________________________
2012 - MA301 
BLOODSPATTER... JUST A STAIN, OR CRIME SOLVING EVIDENCE?
Jonathan Lee Ciecka, Benjamin Edward Naber, 
McIntosh High School, Peachtree City, GA

Bloodspatter analysis is one of the newest forms of forensic science. Our experiment tests the validity of its results, by calculating the point of origin using two mathematical equations. For consistancy of the fake blood, 15ml of red paint and 10ml of water were used in each trial. To create a medium velocity spatter pattern we modified a rat trap and placed a sponge (saturated with fake blood) on the base of the trap before arming and releasing the spring. Once the spatter was created for each test, five dried drops were selected at random from each board. Each drop was then measured by width and elongation, its directionality was strung, and the distance to the area of convergence was measured. The measurements achieved were plugged into two trigonomic equations, and then standard deviation was calculated in order to see how far off our results were from where the actual point of origin was. After analyzing our data, we found that the calculated point of origin was on average less than two inches away from the actual point of origin. Before our experiment began, we determined that if the average calculated point of origin was less than four inches away from the actual point of origin, bloodspatter could be considered a valid science. Our results were much closer than we had expected, and from the data collected we have concluded that bloodspatter analysis is a valid forensic science. 
________________________________________
2011 - MA301 
THE SCIENCE OF A MATH GAME
Vivian H. Nguyen, Mynah Holloway, 
Starr's Mill High School, Fayetteville, GA

The human mind constantly pursues enigmas to solve, yet the most challenging ones become stale over time because of the persistent development of the human race’s intellectual ability. As a result, Sudoku and crossword puzzles became too effortless to solve, so our hypothesis is that if we, the experimenters, observed different magic squares and theories, then we would be able to develop basic algorithms that can facilitate the creation of magic squares of all sizes. This project explored the fundamentals of a magic square and invented the basic rules for the production of magic squares. First, we obtained the method for creating a simple four-by-four square and generally accepted theories for the construction of magic squares with odd dimensions and squares with dimensions that are multiples of four. Next, based on observations from these steps, we pondered upon the creation of a previously undeveloped algorithm for magic squares with even dimensions that were not multiples of four. From the final three compiled algorithms, we composed magic squares consecutively to a square with fifty-three numbers in each row and column, and each completed magic square went into one of the three groups. Our results yielded squares that are all magic squares because each is an arrangement of numbers from one to n^2 when n is the number of elements in each row, column, or main diagonal that each add to n(n^2+1)/2. In conclusion, our experiment resulted in the creation of algorithms for all magic squares, and now, dexterous people will be able to observe a new, challenging puzzle. 
________________________________________
2012 - MA301 
BLOODSPATTER... JUST A STAIN, OR CRIME SOLVING EVIDENCE?
Jonathan Lee Ciecka, Benjamin Edward Naber, 
McIntosh High School, Peachtree City, GA

Bloodspatter analysis is one of the newest forms of forensic science. Our experiment tests the validity of its results, by calculating the point of origin using two mathematical equations. For consistancy of the fake blood, 15ml of red paint and 10ml of water were used in each trial. To create a medium velocity spatter pattern we modified a rat trap and placed a sponge (saturated with fake blood) on the base of the trap before arming and releasing the spring. Once the spatter was created for each test, five dried drops were selected at random from each board. Each drop was then measured by width and elongation, its directionality was strung, and the distance to the area of convergence was measured. The measurements achieved were plugged into two trigonomic equations, and then standard deviation was calculated in order to see how far off our results were from where the actual point of origin was. After analyzing our data, we found that the calculated point of origin was on average less than two inches away from the actual point of origin. Before our experiment began, we determined that if the average calculated point of origin was less than four inches away from the actual point of origin, bloodspatter could be considered a valid science. Our results were much closer than we had expected, and from the data collected we have concluded that bloodspatter analysis is a valid forensic science. 
________________________________________
2011 - MA301 
THE SCIENCE OF A MATH GAME
Vivian H. Nguyen, Mynah Holloway, 
Starr's Mill High School, Fayetteville, GA

The human mind constantly pursues enigmas to solve, yet the most challenging ones become stale over time because of the persistent development of the human race’s intellectual ability. As a result, Sudoku and crossword puzzles became too effortless to solve, so our hypothesis is that if we, the experimenters, observed different magic squares and theories, then we would be able to develop basic algorithms that can facilitate the creation of magic squares of all sizes. This project explored the fundamentals of a magic square and invented the basic rules for the production of magic squares. First, we obtained the method for creating a simple four-by-four square and generally accepted theories for the construction of magic squares with odd dimensions and squares with dimensions that are multiples of four. Next, based on observations from these steps, we pondered upon the creation of a previously undeveloped algorithm for magic squares with even dimensions that were not multiples of four. From the final three compiled algorithms, we composed magic squares consecutively to a square with fifty-three numbers in each row and column, and each completed magic square went into one of the three groups. Our results yielded squares that are all magic squares because each is an arrangement of numbers from one to n^2 when n is the number of elements in each row, column, or main diagonal that each add to n(n^2+1)/2. In conclusion, our experiment resulted in the creation of algorithms for all magic squares, and now, dexterous people will be able to observe a new, challenging puzzle. 
________________________________________
2012 - MA301 
BLOODSPATTER... JUST A STAIN, OR CRIME SOLVING EVIDENCE?
Jonathan Lee Ciecka, Benjamin Edward Naber, 
McIntosh High School, Peachtree City, GA

Bloodspatter analysis is one of the newest forms of forensic science. Our experiment tests the validity of its results, by calculating the point of origin using two mathematical equations. For consistancy of the fake blood, 15ml of red paint and 10ml of water were used in each trial. To create a medium velocity spatter pattern we modified a rat trap and placed a sponge (saturated with fake blood) on the base of the trap before arming and releasing the spring. Once the spatter was created for each test, five dried drops were selected at random from each board. Each drop was then measured by width and elongation, its directionality was strung, and the distance to the area of convergence was measured. The measurements achieved were plugged into two trigonomic equations, and then standard deviation was calculated in order to see how far off our results were from where the actual point of origin was. After analyzing our data, we found that the calculated point of origin was on average less than two inches away from the actual point of origin. Before our experiment began, we determined that if the average calculated point of origin was less than four inches away from the actual point of origin, bloodspatter could be considered a valid science. Our results were much closer than we had expected, and from the data collected we have concluded that bloodspatter analysis is a valid forensic science. 
________________________________________
2013 - MA301 
TUG-OF-WAR: PHYSICAL-MATHEMATICAL ANALYSIS OF THE FORCES INVOLVED IN TUG-OF-WAR
Claudio Freda, Andrei Militaru, Francesco Bray
Liceo Scientifico Statale Paolo Frisi, Monza, ITALY

This project's goal is to create a physical model of tug-of-war. <br><br>The study examines indoor rather than outdoor competitions, due to the possibility of players to plant the heel of their shoes into the ground when outdoors. This limitation gave us the opportunity to carry out an experiment to calculate the value of the coefficient of friction between the sole of the shoe and the platform on which matches are played. The first part of the study is focused on determining the physical elements affecting the conditions of equilibrium during the pull.<br><br>By using Newton's laws we have determined how to relate the force exerted by the players and the tilt angle between the players and the soil. We then studied the obtained function with the open source program GeoGebra; after finding a solution to that equation we estimated its value using the bisection method. We concluded the first part creating a computer simulation, employing the graphic engine LÖVE, displaying the players, their movements, the estimated value of the tilt angle, and any correlated physical data. <br><br>In the second part of the work we expanded the model to cover situations where equilibrium is not granted, or rather the case in which one of the two teams is prevailing over the other. We employed rational mechanics to analyse this dynamic situation. By analogy with what was done in the first part, we studied the occurrence from a physical angle and obtained, by the use of calculus, an equation closely modeling the phenomenon. 
________________________________________
2011 - MA302 
COMBINATORIAL ANALOGUE OF A TOMOGRAPHY PROBLEM
Aliaksandr Nekrashevich, Mikhail Khursevich, 
Middle School 41, Minsk, BELARUS

History of the problem. At the beginning this problem was some analogue of a tomography question, to be more detailed it was a problem about the minimal number of lines with projections such that it was definitely available to restore source points. The solution was found, and it was absolutely combinatorial. As a result there appeared a combinatorial question about reconstruction using (k;d) - pencils. <br><br> Importance of this project. This project is connected to computerized tomography and to the compressing of vector graphics’ information. On the other hand, our solution is connected to the graph theory, particularly to the hypergraphs, coverings and matroid theory. 
________________________________________
2012 - MA302 
DEVELOPING AN APPLICATION OF THE GAUSS-OSTROGRADSKY THEOREM AND FLUX OPERATORS TO INVESTIGATE AN ARCHITECTURAL SITUATION OF FLUID DYNAMICS
Vaibhav Venkat Penukonda, Neil J.G. Singh, 
Canterbury School, Fort Myers, FL

Some of today's most notable buildings are recognized not for their sound foundations or efficient structures but rather for their shapes. And Londons 30 St. Mary Axe, a building shaped like a bullet that transcended the capabilities of the architectural world when built, is no exception. The researchers, thus, worked to develop a mathematical representation of the infamous building and investigate just how structurally significant and relevant its design truly is. The purpose of this research is to model the work of architectural mathematics in a vector field and to calculate its respective flux operators to measure its effectiveness in minimizing downward wind flow, a major component of fluid dynamics represented by volumetric flux -- the rate of volume flow across a unit area (m^3/(s m^2)). After applying the Gauss-Ostrogradsky (divergence) theorem to the shape regression and its respective vectorfield, the resulting flux is calculated using triple integration and LaPlacian operators to be 5pi (m^3/(s m^2)). This "flux operator" is per unit area, so the approximation yields a flux of 15.71 (m^3/(s m^2)) for the entire building. This approximation of a flux operator for the 30 St. Mary Axe, when compared to previous civil engineering studies, represents a 27 to 46% decrease in flux, and thus lesser downward wind flow compared to the average for other buildings. This calculation represents one of the many applications of flux operators that exist and perhaps validates this buildings renown. 
________________________________________
2013 - MA302 
SIMULATION OF PROTEIN FOLDING USING MONTE CARLO METHODS IN A TRIANGULAR LATTICE
Niranjan Balachandar, Nirali Thakor
Shepton High School, Plano, TX

The purpose of this experiment was to simulate the folding pattern of htau40, a 441-amino acid residue isoform of the tau protein, whose abnormal folding has been linked to Alzheimer’s disease. Proteins fold because of the “hydrophobic effect” so each amino acid was converted to hydrophobic (H) or polar (P). Two and three dimensional triangular lattices were used to map out the amino acid sequence in order to avoid parity problems. The Monte Carlo random walk approach was used to simulate protein folding. The simulation code was written using Java and Python languages. The free energies were calculated for different folds, a lower free energy indicating greater stability. It was concluded that in the folds for which the hydrophobic residues were more clustered around their centers, fewer hydrophobic residues would react with external factors, thus giving them lower free energy and greater stability. Hence the natural folding pattern of the tau protein must be concentrated and densely distributed about its center. This is important, as understanding the normal folding patterns of tau is a step closer to understanding the abnormal folding that leads to Alzheimer’s. Further research of this project may include addition of various biological parameters to the simulation like bond angles, electric charges, levels of polarity, etc. for greater accuracy in modeling. It may also be useful to study abnormalities in protein folding as seen in certain human diseases, and specifically examine clinical data of patients with Alzheimer’s disease. 

Awards won at the 2013 ISEF
New American University Provost Scholarship - Arizona State University
Second Award of $1,500 - Mathematical Sciences
Third Award of $1000 - National Aeronautics and Space Administration
First Physical Science Award of $1,500 - Sigma Xi, The Scientific Research Society
________________________________________
2012 - MA303 
A SECRET LOCK FOR ANY THREE PEOPLE
Chih Wei Tan, Hok Wai CHANG, 
Pui Ching Middle School, Macau

In this project, a smart card lock that allows any three members of a group to unlock a lock by using their smart cards is designed. By utilizing an elementary property of quadratic functions, i.e., if two quadratic functions agree with each other at three distinct points, then they are equal and a coding method for the smart card is able to be designed. In addition, Lagrange polynomials are applied to recover the code generating quadratic function from three different smart cards without solving any linear system.<br><br>To implement the idea, a program is written for generating the codes and writing the codes to smart cards. Another program is written for reading smart cards and checking if the code combination is correct or not.<br><br>A magnetic lock which is connected to and controlled by an USB interface is also built. The hardware and the software have been combined to create a working prototype of our invention.<br><br>Finally, the design is original, and illustrates an application of Mathematics is simple, but interesting and tricky. It is likely that other useful applications using the same principles will be found. 
________________________________________
2013 - MA303 
SCALE BALANCED POLYNOMIAL
Yi-Hao Zhu, Che-Yu Huang
National Feng-Shan Senior High School, Kaohsiung, CHINESE TAIPEI

A Balanced Polynomial is an integral polynomial having a zero at some primitive root of unity. A Scale Balanced Polynomial (SBP) of degree n-1 is a Balanced Polynomial having all of 1,2,...,n as coefficients with constant term 1. These properties of SBP's are explored:<br><br>1. Algebraic Properties<br><br>a. SBP of degree n-1 exists precisely for all integers n except powers of primes.<br><br>b. Due to the Chinese Remainder Theorem, we give a method to construct SBP's of degree n-1 whenever n is a product of two relatively prime integers.<br><br>c. Every integral polynomial divisible by a cyclotomic polynomial is balanced. Conversely, each SBP of degree n-1 has the nth cyclotomic polynomial as a factor.<br><br>d. If p and q are distinct primes, then there are at least 2*(p-1)!*(q-1)! SBP's of degree pq-1.<br><br>2. Linking between Algebra and Physics<br><br>Through the eyes of Physics, a Balanced Polynomial represents a system of n point particles being placed on entire roots of unity, each with weight given by the coefficient and having 0 as center of gravity.<br><br>3. Linking between Physics and Geometry<br><br>Since the torque is the vector product of the lever-arm distance and the force, it follows that each Balanced Polynomial is associated with an equiangular n-sided polygon with vertices formed by the partial sums of the torques. In case of SBP's, the equiangular polygon has edges of the length 1,2,…,n in some order.<br><br>4. Linking between Equiangular Polygons<br><br>If n has m distinct prime factors, then to every equiangular polygon P defined by SBP of degree n-1, there is a finite sequence of n-sided equiangular polygons with edge lengths k-th powers of those of P for any positive integer k less than m. 
________________________________________
2011 - MA303 
PARALLEL AND CONCURRENT EULER LINES
Ufuk Kanat, Furkan Samil Basaran, 
Private Samanyolu Science High School, Ankara, TURKEY

We started our research project with a question : if we draw a line passing through a vertex of a triangle and intersects a side of that triangle or if we draw the diagonals of a quadrilateral when the Euler lines of new triangles will be parallel or concurrent.From this question we found nice results.<br><br> We denoted the tangent of the angle within the Euler line of a triangle and a side of that triangle with trigonometric variables.<br><br> We found that the Euler lines of new triangles are parallel if and only if the angle within the side of triangle and the line is equal to 60 or 120 degrees.<br><br> We studied on the concurrence of the Euler lines of new triangles and first triangle.<br><br> We found that the Euler lines of the four triangles which are formed by diagonals and sides of quadrilateral are parallel if and only if the angle between diagonals is equal to 60 or 120 degrees.Let the centers of the parallelograms with vertices are orthocenters, circumcenters and center of gravities of that four triangles be H,O and G respectively. Then H,G and O are collinear and the distance between H and G is twice as the distance between O and G like an Euler line of a triangle.Let the angle between the diagonals be 60 or 120 degrees.Then this line is parallel to the Euler lines of that four triangles.<br><br> Finally , we found that the Euler lines of the four triangles which are formed by diagonals and sides of a cyclic quadrilateral are parallel if and only if the angle between diagonals is equal to 60 or 120 degrees otherwise that four Euler lines are concurrent. 
________________________________________
2013 - MA303 
SCALE BALANCED POLYNOMIAL
Yi-Hao Zhu, Che-Yu Huang
National Feng-Shan Senior High School, Kaohsiung, CHINESE TAIPEI

A Balanced Polynomial is an integral polynomial having a zero at some primitive root of unity. A Scale Balanced Polynomial (SBP) of degree n-1 is a Balanced Polynomial having all of 1,2,...,n as coefficients with constant term 1. These properties of SBP's are explored:<br><br>1. Algebraic Properties<br><br>a. SBP of degree n-1 exists precisely for all integers n except powers of primes.<br><br>b. Due to the Chinese Remainder Theorem, we give a method to construct SBP's of degree n-1 whenever n is a product of two relatively prime integers.<br><br>c. Every integral polynomial divisible by a cyclotomic polynomial is balanced. Conversely, each SBP of degree n-1 has the nth cyclotomic polynomial as a factor.<br><br>d. If p and q are distinct primes, then there are at least 2*(p-1)!*(q-1)! SBP's of degree pq-1.<br><br>2. Linking between Algebra and Physics<br><br>Through the eyes of Physics, a Balanced Polynomial represents a system of n point particles being placed on entire roots of unity, each with weight given by the coefficient and having 0 as center of gravity.<br><br>3. Linking between Physics and Geometry<br><br>Since the torque is the vector product of the lever-arm distance and the force, it follows that each Balanced Polynomial is associated with an equiangular n-sided polygon with vertices formed by the partial sums of the torques. In case of SBP's, the equiangular polygon has edges of the length 1,2,…,n in some order.<br><br>4. Linking between Equiangular Polygons<br><br>If n has m distinct prime factors, then to every equiangular polygon P defined by SBP of degree n-1, there is a finite sequence of n-sided equiangular polygons with edge lengths k-th powers of those of P for any positive integer k less than m. 
________________________________________
2011 - MA304 
FRIENDLY NUMBERS IN ESTABLISHED INTERVALS
Raul Eduardo Marquez-Nunez, Giovanni Santiago Feliciano, 
Centro Residencial de Oportunidades Educativas de Mayaguez, Mayaguez, PUERTO RICO

It has been established in number theory, which studies the properties of numbers, that natural numbers larger than five (5) may have one or more friendly numbers. It has been speculated that numbers have an infinite amount of friendly numbers, however, has not yet been proven. A way to prove or disprove this would be to observe if there is a pattern of repeating friendly numbers in given intervals. If such a pattern does exist, it would seem to indicate, that as long as another interval of an established length can continue to exist, there would be another set amount for friendly numbers. To prove this we will create a program that finds friendly numbers and we will verify if such a pattern exists. <br><br> Upon the finalization of the tests, no such pattern was found for any of the test numbers. However, it was found that the maximum abundancy a number can have increases linearly as the number increases. However, due to the fact that a limited amount of test numbers were used, it may be possible that, a number, unbeknownst to us at the moment, may develop such a pattern, and indicate that indeed has infinite friendly numbers. For this, further research would be needed in the matter, by others who are better equipped and experienced to conduct a more exhaustive research. 
________________________________________
2012 - MA304 
THE LEE MODEL OF "COMPLETENESS" POLYHEDRONS WITH MATCHING COLOR AND A NEW WAY OF MAKING POLYHEDRONS MODELS
Wai Chon Lio, Leong Man Chong, Bernardino Judas Cordova Wong
Macao Pooi To Middle School, Macao, MACAU

In this study, we have analyzed how to match the colors of Platonic solids and Archimedean solids.<br><br>We used the in-house developed software – plane geometry laboratory “PG_Lab” and solid geometry laboratory “SG_Lab” to produce the topological graphs and solid graphs of 18 polyhedrons. Through the usage of this custom build software we are able to thoroughly analyze the complex polyhedrons from various perspectives.<br><br>We began our study by analyzing how to match the colors of the polyhedrons’ faces. According to “four color theorem”, which requires that the color of individual faces must be different from adjacent faces. It seems that the theorem cannot demonstrate “the beauty of Mathematics”. Consequently, along with our mentor, we decided to change colors of the faces to a pattern that is more inline with proportionality and concordance. Our study eventually let us to propose “Completeness” of the faces’ colors. Also, we match the colors on the topological graphs and design the colorful development drawings.<br><br>We even extended the theorem to the edges’ colors. Finally, we successfully produced 18 models which are according to “Completeness” or “Half of Completeness”.<br><br>In addition, we used a creative method—by using unifilar fish line to pass through beans to produce polyhedrons. Also, we use the inference of Euler’s enicursal theorem to prove the feasibility of the method.<br><br>Furthermore, we discover the marvelous connection between using topological graph to simulate sticking beans, labyrinth and Hamiltonian path. 
________________________________________
2013 - MA304 
ON THE STABILITY OF LUNG PARENCHYMAL LESIONS WITH APPLICATIONS TO EARLY PNEMUOTHORAX DIAGNOSIS
Archis Ramkrishna Bhandarkar, Rohan Banerjee
Thomas Jefferson High School for Science and Technology, Alexandria, VA

Spontaneous pneumothorax, a prevalent medical challenge in most trauma cases, is a form of sudden lung collapse closely associated with risk factors such as lung cancer and emphysema. Our work seeks to explore and quantify the currently unknown pathological factors underlying lesion rupture in pneumothorax through biomechanical modeling. We hypothesized that lesion instability is closely associated with elastodynamic strain of the pleural membrane from pulsatile air flow and collagen-elastin dynamics. Based on the principles of continuum mechanics and fluid-structure interaction, our proposed model coupled isotropic tissue deformation with pressure from pulsatile air motion and the pleural fluid. Next, we derived mathematical instability criteria for our ordinary differential equation system and then translated these mathematical instabilities to physically relevant structural instabilities via the incorporation of a finite energy limiter. The introduction of novel biomechanical descriptions for collagen-elastin dynamics allowed us to demonstrate that changes in the protein structure can lead to a transition from stable to unstable domains in the material parameter space for a general lesion. This result allowed us to create a novel streamlined algorithm for detecting material instabilities in transient lung CT scan data via analyzing deformations in a local tissue boundary. 

Awards won at the 2013 ISEF
Fourth Award of $500 - Mathematical Sciences
________________________________________
2011 - MA305 
YOUNGER HOCHSCHILD COHOMOLOGIES OF SOME CLASSES OF ALGEBRAS
Fedor Ivanovich Uvarychev, Anatoliy Alexandrovich Zaikovskiy, 
Lyceum #572, Center of Mathematical Education, Saint-Petersburg, RUSSIA

For finite algebras, as well as for algebras in general, the primary cohomology theory is the theory of Hochschild cohomology. A direct sum of groups of Hochschild cohomology is usually considered together with action, called the cap-product, which defines the structure of a graded algebra on this direct sum - the Hochschild cohomology algebra. The Hochschild cohomology algebra is computed for many specific examples of algebras, and the technology of these calculations is being gradually developed. <br><br>In this paper we compute the zeros and the first Hochschild cohomologies of some classes of algebras, the zeros are calculated together with the structure of a commutative algebra, and the first with the structure of the Lie algebra. Also a new technique to calculate the center and Lie algebra of derivations for the path algebras of a quiver with relations was developed. After that, this technique has been applied to certain classes of algebras, namely the path algebras of a quiver without relations, symmetric Nakayama algebras and symmetric special biserial algebras (SSB-algebras).<br><br>SSB-algebras were actively investigated for the derived equivalence for the last few years. One of the modern mathematicians found a lot of necessary and sufficient conditions on the fact that the two SSB-algebras are derived-equivalent, but still the problem is not solved. Since Hochschild cohomology algebra, regarded as Gerstenhaber algebra, is an invariant with respect to the derived equivalence, realized computing can be useful for the complete answer to the question of the derived equivalence of the SSB-algebras. 
________________________________________
2013 - MA305 
PERCOLATION GAMES ON CAYLEY GRAPHS OF GROUPS
Maksim Lvovich Bezrukov, Aliaksandr Stadolnik
Minsk Gymnasium #41 named after Serebriany V. H., Minsk, BELARUS

The aim of our research was to build the mathematical model of the following processes with the given conditions: percolation (movement and filtering of fluids through porous materials), the spread of bacteria in different surroundings (like in Petri dishes), the spread of computer viruses in global nets, etc.<br><br>We took into consideration the following deterministic game suggested by Itai Benjamini in 2002 when two species compete on a graph - the red and the blue. Starting with one red vertex and one blue, each second the blue blues all its neighbors which are not yet colored. The red is lazy and grows only at even times. If both species target the same vertex, it becomes blue. The question arises: will the blue surround the red after some ﬁnite period of time? If the red is able to grow inﬁnitely many times, we may say that the blue will lose, otherwise if the red surrounded by the blue stop growing, the blue wins. We investigated this problem for Cayley graphs of groups. <br><br>In our research, we developed a novel technique of studying geodesics in graphs by means of generalized Young diagrams. As the main result, we completely solved the problem for Cayley graphs of ﬁnitely generated abelian groups and for Cayley graphs of the integer Heisenberg group. In case of abelian groups, we showcased that the blue loses only on graphs of the groups ZxG, where G is finite abelian. In case of the Heisenberg group, the blue always wins. <br><br>We believe the results of our research may be applied in Biology, Healthcare, Computer Science, Medicine and water purification industry, etc. 

Awards won at the 2013 ISEF
Third Award of $250 - American Mathematical Society
________________________________________
2012 - MA305 
THE S(N) BASED ONE-WAY CRIPTING ALGORITHM
Gergo Kopenczei, Peter Enekes, 
Bonyhadi Petofi Sandor Evangelical High School and Boarding School, Bonyhad, HUNGARY

Our objective is to introduce a one-way cripting algorithm serving IT security at much higher level than those currently in use. The strength of a hash function such as MD5 and SHA1 has been called into question as a result of recent discoveries and accelerated progression in computer technology. It is clear that we need some replacements in the not-too-distant future. <br><br>Our approach based on the s(n) divisor function seems to be suitable for being used as such an algorithm. In the first instance our solution is useful in digital signatures and creating password hashes. Our method offers more safety than most methods which are in use currently. <br><br>To make a successful rainbow table attack, the attacker needs a hash dictionary. The time needed for that is significant, and this is related to the current calculation capacity of the computers. In our approach, by changing one parameter we correlate the safety with computation capacity of the computers. This way the accelerating speed of the computers is irrelevant from security point of view. <br><br> Any parameter in use requires a new dicitonary completely different from any others. Even if the attacker finds identic hash, it is not sure that this is the right one, only if he/she knows our parameter. <br><br>The biggest advantage of our approach is the stronger resistance against rainbow table attack, and in the meantime by changing only one parameter we can keep the safety level even if the computing speed is increasing. 

Awards won at the 2012 ISEF
Fourth Award of $500 - Mathematical Sciences - Presented by Intel
________________________________________
2011 - MA306 
THE WORLD OF IRRATIONALS
Jose Daniel Sales, Perez Leandro Matias, 
Escuela Provincial de Comercio No 7, San Pedro de Jujuy, Jujuy, ARGENTINA

There exist numbers with infinite non periodic decimal digits that are found in the set of real numbers but are incapable of being expressed as fractions: the irrational numbers. This work is aimed at finding a new way of interpreting those irrational numbers through an inductive method in order to provide an answer related to the usage of this numerical set in real life. Such method has involved applying geometrical concepts so as to get back to the historical roots of that numerical set. In that way, geometrical representations were made to visualize the length that algebraic irrational numbers represent. A ruler was created with the representations made. The diagonals of some rectangles can be measured using our measurement instruments. For instance squares, and rectangles whose width is half the length of those shapes. Conclusively, this project has let us give a meaning to algebraic irrational numbers and show their practical usage thanks to the interpretation of their operability through the measurements done with our ruler and thus, proving the viability of our research onto this field. 
________________________________________
2012 - MA306 
MUSIC IN TOWN
Elalim Zen Vukovic, Jasmine Zen Vukovic, 
Scientific High School "Lorenzo Mossa", Olbia (OT), Sardinia, ITALY

The project describes a new way of proposing tourism. We planned a trip round the streets of our city, Olbia, and we chose the ones that bore the names of the Italian unification’s protagonists, since last year was the sesquicentennial anniversary of that so important event. <br><br>We solved the problem of repeating the same way making an Eulerian graph, where the number of edges joining in the same vertex is even. <br><br>The tour involves four “music points” where it could be transmitted through touch screen computers. We searched for the Italian romantic composers as Verdi, Donizetti, Rossini and Puccini who lived in that period and whose music inspired both politics and culture to unify Italy. <br><br>In order to make more original our work we mathematically analyzed the classical compositions of these musicians. Then we produced our own variations basing on the analyzed works with the help of four types of symmetry: vertical translation, horizontal reflection, vertical reflection and the synthesis of the latter two.<br><br>We produced four pieces of classic music that could be played together with the originals in the four “music points”. Other information would be displayed on the computers when listening to music. We have placed the “music points” in large places to avoid the obstruction of the passage of people.<br><br>Our project wants to help improving tourism in Olbia but it is also applicable in any other city or town. It joins science with history and so it is useful but also interesting and instructive. 
________________________________________
2013 - MA306 
DISSECTION OF SQUARE INTO 'N' CONGRUENT SQUARES
Nayana Ravindranath Koravatti, Aishwarya Ashok
Amrita Vidyalayam, Davangere, INDIA

A square can be dissected into 4(=2²), 9, 16, or, in general, any square number of congruent squares quite easily. But it requires some effort to divide a square into a non-square number, i.e., 2, 3, 5, 6, 7, 8, 10, 11... of congruent squares. This is a well-documented problem; for instance, the cases of dissecting of a square into 3 / 5 / 10 congruent squares occur as puzzles in ancient literature and many solutions are available for these specific cases. <br><br>This work presents a novel method of dissection of a square into N congruent squares where N is any integer including square number, thereby giving a uniform general construction. <br><br>An important point about this algorithm is that, the dissection can be effected by ruler and compasses alone.<br><br> 

Awards won at the 2013 ISEF
Fourth Award of $500 - Mathematical Sciences
________________________________________
2011 - MA307 
MOBIUS: ART, RHYTHM AND MOVEMENT THROUGH MATH
Cintia Paola Branca, Yanina Gisela Mansilla, 
Escuela de Ensenanza Media N0 318 "Antartida Argentina", Diaz, Santa Fe, ARGENTINA

The local cultural youth is the one that fosters the answer to this work. When they listen to the song Simetría de Moebius (Catupecu Machu, 2009), they feel the need to know what "Möbius" refers to. This gives them the opportunity to deal with geometry topics in depth and to enter in the morphology field. The basic elements of Topology are introduced in the math classes, mainly the model known as "¨Möbius Strip" (Möbius 1790 – 1868). It is there where the questions arise: Is it possible to find some morphological patterns in works of art? Is there any relationship between the Möbius Strip and some aesthetic shapes? It is known that the natural shapes have functions that can be interpreted as an answer to nature requirements. For instance, there is a logic of growth, development and evolution that results in coherent shapes appearing in nature. So, certain geometry patterns, apart from the aesthetic function, can be found in some aspects of art that lead them. In the project, it is characterized the Möbius model, and a comparative analysis of its properties in relation to the different expressive productions is carried out in order to investigate the possible existence of an analog topological pattern in some art demonstration. With the analysis of different works of art (from different branches) and the collected data, the students can identify that there are certain morphological aspects similar to the ones of the Möbius model used as the starting point of the work. 
________________________________________
2012 - MA307 
STUDY OF ARCHIMEDES' SPIRAL POLAR EQUATION
Irma Liz Alicea, Darlene Nieves Cruz, 
Francisco Morales, Naranjito, PUERTO RICO

This investigation is based in the study of the polar equation of Archimedes’ spiral. The purpose was to explore systematically the nature of the changes in the graph of the spiral when the constants a and b are changed. To begin the study, it was questioned what would happen to the graph of the equation r(theta)=a+b*(theta), when the value of a and b is replaced by positive values 0<a<11 y 0<b<11. In order to answer it, the variables where evaluated each at a time. Then, was analyzed the variation of the graph by combining different values for a and b. After graphing and analyzing the results, they showed that the constants a and b determine the starting point of the spiral from the pole and the distance between successive turns, respectively. Moreover, whenever a is not equal to 0, the ratio between the area of the first round of the spiral and the area of the circle which circumscribes the first round will tend to 1 when the constant a tends to infinite, and to 3 when the constant b tends to infinite. Otherwise, whenever a=0, the ratio will be 1/3 for all possible values of b. These results validate the hypothesis. In the future, it will be considered further study of the area measurements of each turn extending the possible value of (theta)*n*2*(pi) laps to compare the results with this study and determine any patterns in the area measurements growth regarding the circles circumscribing every turn. 
________________________________________
2013 - MA307 
ANALYSIS OF THE ERDÖS DISTANCE PROBLEM IN MODULAR ELLIPTIC SPACE
Richard Rene Sala, Markus Woltjer
Wilsonville High School, Wilsonville, OR

The original Erdös Distance Problem proposes a Euclidean plane, where “Between N distinct points on a plane there are at least N^(1 – o(1)) distinct distances.” As N increases, the number of unique distances and occurrences of each distance becomes difficult to compute. Since the innovation of exhaustive computer searching this century, researches have utilized various methods of combinatorics and incidence geometry to estimate functions of distances and N. The decision made for this project was to move to modular elliptic space, similar to the finite fields typically experimented in estimation research. This allows the field to be defined as an equation of ellipses. This method allows for faster calculation and generation of points because of a unique property of ellipses: any point on the curve can be raised to an integer power to generate more points on the curve. A program was designed to generate a field in elliptic space, specified by varying coefficients on simple elliptic relations. A prime modulus "p" or "z_p" is applied to the entire relation rather than the evaluation of the relation. Points on the curve are generated using a small integer grid of typically one million or less total points. Points on the curve are raised to higher powers sufficient data for analysis are determined, and before the identity of the curve breaks pseudo-random generation by repeating points. Each elliptic relation and prime modulus gives a sample of data for histogram analysis, searching for patterns that would give critical understanding of the Erdös Distance Problem. 
________________________________________
2013 - MA308 
COMPUTERIZED GAME THEORY ANALYSIS OF THE POPULAR WORD GAME, GHOST
Stefan Luka Colton, Axel Feldmann
Hunter College High School, New York, NY

This is a game theory analysis of the game GHOST . Since 1987, there has existed a solution for the 2-player version of the game, which combinatorial in nature, yet the 3-player variant has remained unsolved. This experiment developed a program to solve this problem using computerized solving algorithms and game theory concepts. The dictionary used was a standard SIL Language English dictionary, however, other dictionaries may be utilized for future study. The principal concepts used in this experiment were recursion, iteration, and simulated game trees. The 2-player solution was confirmed to be a victory for the first player, as was hypothesized. Using this result, individual 3-player rounds were calculated and the solutions were used to create an overarching solver for the entire 3-player game. It was hypothesized that for 3-player ghost, the 2nd player could force a win. The result of the game is in fact surprising; player 1 cannot win, yet he determines which of the other 2 players is victorious. This result for a 3-player game makes it very interesting. An analysis of a second ruleset was also done, and found the same result for 2-player ghost. 
________________________________________
2011 - MA308 
IT IS NOT JUST OF DEGREE ONE..! A THEORY TO FIND THE REMINDER IN POLYNOMIALS DIVISIONS
Jawaher Saleh A. Enani, Hend Khalid I Bimuhaya, 
Riyadh Najd, Riyadh, Central, SAUDI ARABIA

Introduction: The difficulty of finding the remainder in a division of polynomials of the second degree by long division is a problem we decided to efficiently solve. <br><br>Purpose : To prove a theory which can make the method of finding the remainder easier and time saving for students. <br><br>Procedures: Investigating why cannot a person find the remainder of a second degree polynomial without long division. For that, we had some trails of making f(x), g(x) controlled followed by the value of r(x).<br><br>Results: Throughout this theory we were able to find the remainder of a second degree polynomial without using long division. <br><br>Conclusion: The study found that out hypothesis is positive. And that students can apply it easily, fast, and a very efficient way. 
________________________________________
2012 - MA308 
ON RANKS OF VARIETIES GENERATED BY FINITE SEMIGROUPS
Vladimir Alexandrovich Mazin, Grigory Gerasev, 
The Laboratory of Continious Mathematical Education, Saint-Petersburg, Saint-Petersburg, RUSSIA

The variety is a class of semigroups with a fixed finite set of identities which are valid in each semigroup from the class. The relationship between algebraic properties of a semigroup and its identities is very complicated. So the study of variety may pose a challenge even in case of the simplest varieties.<br><br>A recent breakthrough in the semigroup theory is related to concept of rank. The main feature of the rank is that the Rees-Sushkevich varieties could be determined by their rank uniquely up to permutation identities. Formally, the rank of the variety is defined as a vector, which describes the belonging of series of special finite combinatorial semigroups to the variety. So we can identify even very complex Rees-Sushkevich variety even if we know some special semigroups which belong to it.<br><br>The ranks proves to often determine a variety in general case. The rank is defined for an arbitrary variety, but until now there was no research studying its behavior in general case.<br><br>In the present paper we observe the rank with examples of varieties of the most famous and most complicated semigroups and discover some general patterns in the evaluation of the rank. Finally we formulate widely-usable theorems regarding the ranks. 
________________________________________
2012 - MA309 
COUNTING ZEROS OF RATIONAL HARMONIC FUNCTIONS: PARAMETER SPACES
Youkow Homma, Lyndon Ji, 
Carmel High School, Carmel, IN

We take a parameter space approach to studying the number of solutions to p(z) = bar(z) and r(z) = bar(z), where p(z) and r(z) are holomorphic polynomials and rational functions, respectively. Upper bounds on the number of zeros were previously obtained by Khavinson-Swiatek and Khavinson-Neumann while lower bounds are easily obtained by the Argument Principle. Between these two bounds, we show that any number of simple zeros allowed by the Argument Principle occurs and that the complement of these sets of simple zeros is contained in a proper real algebraic subset. We present analogous results in the case that solutions to r(z) = bar(z) correspond to images produced by an n-point gravitational lens. 

Awards won at the 2012 ISEF
Certificate of Honorable Mention - American Mathematical Society
________________________________________
2013 - MA309 
STUDY OF INTEGRALS OF PARAMETRIC FUNCTIONS FOR FERMAT’S CURVE OF THIRD DEGREE
Andres Josue Arroyo Colon, Edwin Torres-Cuevas
Centro Residencial de Oportunidades Educativas de Mayaguez, Mayaguez, PUERTO RICO

Though the integral of a semicircle curve and many other containing square roots of sums and differences of squares can be solved by trigonometric substitution, this is not the case for similar functions, but of third degree. However, this method only works by using the fact that the sum of the squares of sine and cosine equals one ( sin^2 x+cos^2 x=1 ). In this investigation, a method is presented for integrating powers of sums and differences of cubes, using parametric functions of Fermat’s curve of third degree, Fermat’s sine and Fermat’s cosine, which have the property that the sum of the cubes of these functions equal 1 (sinf^3 x+ cosf^3 x=1), that provide a method analogous to the already known method of trigonometric substitution. Also, a formal power series representation is developed, and some possible future applications of these functions in physics and engineering are stated. The study of these functions revealed a number of similarities between the trigonometric functions, and the “Fermat’s trigonometric” functions considered in the investigation, such as integration and differentiation patterns, reduction formulas, and development of their power series. These properties hint at a more complicated structure, in which the trigonometric functions and the “Fermat’s trigonometric” functions are just specific cases. 

Awards won at the 2013 ISEF
Third Award of $1,000 - Mathematical Sciences
________________________________________
2011 - MA309 
FROM THE AROUND THE WORLD SAILING RECORD TO BLACK HOLES: STRANGE GEOMETRIES
Clement Martinez, Arnaud Vespa, Marine Auriol
College Albert Camus, Miramas, FRANCE

Basing our work on Graph theory and Modern (non-Euclidean) geometry, we contemplated real world applications. After considering other possibilities, among which applications to neuroscience, we decided to focus on two topics :<br><br>1. Studying optimization problems in maritime transport as required by a local shipping company, taking into account the relevancy of these tools in order to calculate and optimize connection schedules for ships. <br><br>2. Creating the necessary tools to allow us to beat the Around the world sailing record on a genuine ocean race simulator challenge. The choice of the optimal path in this race resembles the choice of the shortest route in a network. <br><br>In order to remain relevant and to understand the subject in all its dimensions:<br><br>- we combined mathematical theories (like spherical, tropical and discrete geometries), <br><br>- we worked, as much as possible, with specialists from the different areas : research, business and sailing (skipper, navigator, naval architect, weather specialist), <br><br>- we sailed on a racing multihull to measure the best adjustment in order to go fast.<br><br>For the optimization maritim routes problem, we created a software enabling us to find the shortest route within a network of routes. <br><br>For the sailing problem, we first studied databases of the performance of the trimaran for each wind condition, meteorological statistics and gathered information during an outing on a genuine racing trimaran. Using these data and mathematics, we devised a method, allowing us to beat the record in 48 days and 44 minutes. 

Awards won at the 2011 ISEF
Fourth Award of $500 - Mathematical Sciences - Presented by Intel
________________________________________
2011 - MA310 
MINIMAL LIE RINGS
Tole Ernstuly Tolegenov, Adepkhan Yeren, 
High School in Physics and Math, Almaty, KAZAKHSTAN

Abstract: In the project minimal rings are studied.Some classical results are presented .It is proved that any minimal Lie ring contains an infinite locally finite subring. 
________________________________________
2012 - MA310 
NEIGHBORS WITH PRESCRIBED PRIME FACTORS
Mark Alan Holmstrom, Theresa McLaughlin, 
Live Oak High School, MORGAN HILL, CA

The ABC Conjecture is a deep conjecture about numbers A, B, and C such that A+B=C. It is a vital part of the research in mathematics today. It has been slow work attempting to prove this far-reaching conjecture, and it still remains incomplete. One of the mathematicians whose work is relevant to this conjecture is D. H. Lehmer who studied the ABC equation when A=1. We used his method of using a narrow set of numbers as a basis for our research. His methods were thorough, but exhaustive. Our project's goal is to formulate a new method that is more efficient than that of Lehmer and still creates a relatively thorough solution set.<br><br> We go about creating this solution set in a different way. An initial set is augmented with new solutions found by a simple procedure. We then repeat this procedure to further augment the set until no new solutions arise. The process ends when no more solutions are found through combination of any two numbers in the set. <br><br> We performed our process with a maximal prime factor of 163 where as Lehmer's method only reached a prime factor of 41. We found 115,207 solutions to our equation. The largest solution we found was 19,316,158,377,073,923,834,000, though it may be possible to find a larger solution if we used a higher maximal prime.<br><br> Our new method creates solutions to Lehmer's equation that he was unable to find through his exhaustive methods. Work on the ABC Conjecture relates back to many other conjectures and theorems, such as Fermat's Last Theorem, Roth's Theorem, and the Mordell Conjecture. These findings are specific to set of numbers that fit the limitations of the ABC Conjecture and Lehmer's equation. 

Awards won at the 2012 ISEF
Certificate of Honorable Mention - American Mathematical Society
________________________________________
2012 - MA311 
(ALMOST) UNIT-DISTANCE POINTS IN THE POLYCHROMATIC PLANE: COLORINGS OF THE N-DIMENSIONAL SPACE
Fabian Henneke, Danial Sanusi, Xianghui Zhong
Kippenberg-Gymnasium, Bremen, GERMANY

In this paper, we shall concentrate on colorings of the n-dimensional space: we assign one of a given number of colors to each point. Although it seems as if those colorings can be chosen ‘chaotically’, they do exhibit certain regularities. In this context, in 1950, E. Nelson sought to discover the minimum number of colors necessary to color the plane in such a way that there are no unit-distance points of the same color. With a small number of colors relative to the dimension, it is impossible to avoid those unit-distance points.<br><br>In the course of our work, we shall address various kinds of colorings and their inherent structure, often giving natural generalisations to higher dimensions. Firstly, we shall introduce the field of mathematical colorings by stating and proving elementary results relating to the above problem. In order to prove advanced results, we deduce a topological fact on covers related to the dimension from the Brouwer fixed point theorem. This makes it possible to utilise the concept of<br><br>epsilon-neighborhoods in the context of colorings: we restate the initial problem for points of distance ‘almost’ 1, giving better, quadratic bounds on the number of colors for up to five dimensions. After this, we shall address a more discrete type of colorings using cubic tesselations of cuboids and the whole n-dimensional space. A similar approach can also be used to prove that the game Hex always ends with a winner. 

Awards won at the 2012 ISEF
First Award of $1,000 - American Mathematical Society
First Award of $3,000 - Mathematical Sciences - Presented by Intel
________________________________________
2012 - MA312 
OPTIMAL ALLOCATION OF GLOBAL CONSTRAINED RESOURCES USING THE HYPERBOLIC VORONOI DIAGRAM
Shanthi Shanmugam, Caroline Shouraboura, 
Forest Ridge School of the Sacred Heart, Bellevue, WA

Our world relies on important resources supplied to a scattered population from specific points on the earth’s surface. Trauma centers provide emergency care for everyone within their region of service. Food relief supplies are staged, warehoused and distributed from specific locations to the hungry families who depend on them. Frequently, we need to minimize the total distance to the population needing the resource. In the case of Trauma centers, transportation time can mean life or death. The transportation cost of moving food supplies or other goods is substantial and is born in both real costs and costs to the environment.<br><br>Voronoi Diagrams provide a mathematically elegant method for answering distance-related questions. The important characteristic of a Voronoi cell is that all points within a given cell are closer to the vertex of that cell than to any other vertex. However, we were challenged by the need to account for capacity constraints at specific points of supply. What if a particular trauma center had an unusual rush of patients or a shipment of supplies failed to reach a particular food depot? How should the population assignment change? To retain the algorithmically efficient properties of a Voronoi Diagram we developed a way to warp the edges of Voronoi cells to accommodate capacity constraints. Our breakthrough was a theorem and proof claiming Voronoi edges as hyperbolas and an algorithmic method of calculating the optimal assignment of resources given constrained sources of capacity. We include a simulation of our algorithm. 

Awards won at the 2012 ISEF
Fourth Award of $500 - Mathematical Sciences - Presented by Intel
Mu Alpha Theta will award up to three awards for a total of up to $4500. First award minimum of $1500.00 - Mu Alpha Theta, National High School and Two-Year College Mathematics Honor Society
First Award of $2,500 - Sigma Xi, The Scientific Research Society
________________________________________
2012 - MA313 
THE EFFECTS OF HUMAN DYNAMICS ON THE PROLIFERATION OF THE EBOLAVIRUS
Rajan Upendra Sheth, Alfred Xue, 
Hamilton High School, Chandler, AZ

Ebola is a biosafety level 4 virus with mortality rates greater than 80%. With the globalization of the world, international cities have become important to the trade and transportation of the global market; however, these cities are extremely susceptible to bioterrorist attacks and can spread diseases to large populations quickly. Thus, the model is designed to test the effectiveness of quarantines and the effects of fear on the proliferation of Ebolavirus. With little real-world data to use, modeling Ebola using mathematical programs is important. This model includes quarantines in which travel between cities is significantly reduced and a variation that accounts for fear during an epidemic. The experiment generated a formula that approximates the relationship between the intervention threshold and the number of deaths caused by Ebola. This model also demonstrates that as the threshold level of the quarantine increases, there is a decreasing effect on the number of people who die per increase in threshold level to the quarantine, which implies that as the intervention threshold decreases, there is an increasing effect of the people saved per decrease in threshold level. The fear accounted for in this model did not have a significant effect on the proliferation of Ebolavirus. However, quarantines decrease the impact of an Ebola epidemic by increasing the time before the virus infects a city. This model could also be used to determine where the virus would strike after its initial city and enact treatments that could reduce the viability of the virus itself. 
________________________________________
2012 - MA314 
QUIVERS OF SELF-INJECTIVE ALGEBRAS
Anatoly Zaikovsky, Artemy Bezguzikov, 
Center of Mathematical Education, St. Petersburg, RUSSIA

Modern algebra studies various abstract objects such as groups, rings, fields. Our work is devoted to one of these objects - to algebras. There is a standard procedure which associates any finite-dimensional algebra with some directed graph (that is, with some drawing made of points connected with arrows). This graph is called Gabriel quiver of this algebra. A quiver shows many properties of the initial algebra in a simple visual form. The primary goal of this work was to find visual properties of quivers, corresponding to finite-dimensional algebras with some particular properties. <br><br> <br><br>The class of self-injective algebras is very interesting for study, since it includes many useful types of algebras such as a very important class of group algebras. The main result of our paper is that quiver is a quiver of some self-injective algebra if and only if each its arrow can be completed to a directed cycle. On one hand, this result allows to check self-injectiveness of algebras in a visual form. On another hand, it shows how the class of self-injective algebras is reach, since for any quiver covered by cycles we can find a corresponding algebra.<br><br>In order to prove our results, we developed a special mathematical language, which can be effectively used in representation theory of finite-dimensonal algebras. We made an abstract property of self-injectiveness more visual, which greatly simplifies work with this class of algebras. 
________________________________________
2012 - MA315 
UNTANGLING THETA GRAPHS WITH MODIFIED GAUSS CODES
Karn Imwattana, Varot Pakavatsoontorn, 
Triam Udom Suksa School, Bangkok, THAILAND

Knots usually occur in a Y-shaped earphone. When tying its three ends together, it becomes what mathematicians call “theta graphs”, not mathematical knots. Many invariants, properties that are unchanged for the same type of knots, have been used to classify only mathematical knots, not theta graphs. We want to develop an algorithm to classify theta graphs into knotted (non-trivial) and unknotted (trivial) graphs. <br><br>A theta graph is described by a modified Gauss code, which is a sequence of labels for the crossings in order to indicate each path along the theta graph diagram. Therefore, deformations on the theta graph diagram can be virtually executed by changing the modified Gauss code. The algorithm is developed to simplify the theta graph by continuously rearranging or reducing the modified Gauss code so that the theta graph has the least number of crossings. Unknotted graphs will have no remaining crossing. This algorithm is developed and implemented by a computer program. Many theta graphs can be completely simplified by our algorithm, except for a few theta graphs. Our future goal is to extend the algorithm to cover all types of theta graphs. 
________________________________________
2012 - MA316 
SOME USEFUL EQUATIONS ON P-ADIC ORDER AND THEIR APPLICATIONS
Cuneyd Ozturk, Kursat Rasim Mestav, 
Ankara Science Lycee, Ankara, TURKEY

This project provides a different point of view for elementary number theory problems using an auxiliary function called p-adic order. Relating to this function we set up some preliminary equations which will be our primary tool in the whole project. This gives us a technique which eases to understand the essence of the problem.<br><br>We used the technique mentioned to prove famous theorems including the Zsigmondy's Theorem, a special case of Preda-Mihalescu Theorem, Euler Theorem, a series of theorems relating to primitive roots and some other theorems. Moreover, we solved many math olympiad problems with using this technique.<br><br>The proof we developed for Zsigmondy’s theorem revealed that a little condition saying that the power of the first term should be less than the power of the second term, mentioned in the statement of the theorem was in fact unnecessary. We modified the theorem by putting out that condition and prove this more aesthetic type of Zsigmondy’s theorem by of course using our primary tool p-adic order. <br><br>The various types of problems solved in this project shows that p-adic order function gives us a useful perspective in elementary number theory. As frequently in mathematics we see the primitive ideas can reveal some unexpected results which are to some extent the case for this project. 
________________________________________